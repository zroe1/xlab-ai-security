{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "389f7864",
   "metadata": {},
   "source": [
    "# Running GPT-2 Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8eb06579",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f714b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ff577dc2164d838373d710297b57f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe1c0882b474595873c77125a16f687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab61e2cdf3204463992fbaf7d0e7a667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f0197ace4f49a481901e58a37a37b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38cc3b6885e493fab390572fd3ee930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c8d16ebb2a4eeba5eb19ef408b10e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d1e7d79e6a40fc862c69a88cf4be4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b45c81b",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe36035",
   "metadata": {},
   "source": [
    "We have some text and we would like to predict the next few characters of text. We cannot simply input our text into the model because our model doesn't speak in language; it speaks in numbers!\n",
    "\n",
    "The first step to almost any NLP task is to turn our language into some sort of series vectors or number. In our setting, we apply a \"tokenizer\" to split up the text into chunks, where each chunch is assigned a number. From here we can print the numbers out to inspect them. These will be fed to the model, which will turn each of these numbers into a high-dimensional vector, for our tranformer to perform computations on and learn relationship in the text useful for predicting the next token.\n",
    "\n",
    "Take a look at the example below for how to tokenize the string: \"The great scientist Albert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "2f838019",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  1049, 11444,  9966]])\n"
     ]
    }
   ],
   "source": [
    "prefix = \"The great scientist Albert\"\n",
    "encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "print(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe48bc3",
   "metadata": {},
   "source": [
    "Our tokenizer can also decode these ids back into the original text. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d0cd5747",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The great scientist Albert'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84ca0a8",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "For a given input of text, return a list of tokens in plain text. For example for the input \"Hello there\", the function should re\n",
    "\n",
    "Hint: You only need to call tokenizer and tokenizer.decode to complete this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "aa32938d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', ' great', ' scientist', ' Albert']"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plain_text_tokens(prefix):\n",
    "    '''\n",
    "    '''\n",
    "    rv = []\n",
    "    \n",
    "    encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "    for i in encoded_input['input_ids'][0]:\n",
    "        rv.append(tokenizer.decode([i]))\n",
    "        \n",
    "    return rv\n",
    " \n",
    "plain_text_tokens(prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e616ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "09446bbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c2bb257b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "09a9b697",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "    token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "    generated_text = tokenizer.decode([token_id.item()])  \n",
    "    text+=generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "4f8ede67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The great scientist Albert Einstein, who was a great scientist, was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a3e7d6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f826d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2bc14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
