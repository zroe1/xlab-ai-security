{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd83fc31",
   "metadata": {},
   "source": [
    "# Running GPT-2 Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "08cb456d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ecded170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62ff577dc2164d838373d710297b57f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebe1c0882b474595873c77125a16f687",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab61e2cdf3204463992fbaf7d0e7a667",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97f0197ace4f49a481901e58a37a37b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c38cc3b6885e493fab390572fd3ee930",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3c8d16ebb2a4eeba5eb19ef408b10e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38d1e7d79e6a40fc862c69a88cf4be4f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "348c240e",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a10d1b",
   "metadata": {},
   "source": [
    "We have some text and we would like to predict the next few characters of text. We cannot simply input our text into the model because our model doesn't speak in language; it speaks in numbers!\n",
    "\n",
    "The first step to almost any NLP task is to turn our language into some sort of series vectors or number. In our setting, we apply a \"tokenizer\" to split up the text into chunks, where each chunch is assigned a number. From here we can print the numbers out to inspect them. These will be fed to the model, which will turn each of these numbers into a high-dimensional vector, for our tranformer to perform computations on and learn relationship in the text useful for predicting the next token.\n",
    "\n",
    "Take a look at the example below for how to tokenize the string: \"The great scientist Albert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "4293a707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  464,  1049, 11444,  9966]])\n"
     ]
    }
   ],
   "source": [
    "prefix = \"The great scientist Albert\"\n",
    "encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "print(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69102826",
   "metadata": {},
   "source": [
    "Our tokenizer can also decode these ids back into the original text. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "1cf2a0c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The great scientist Albert'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88336ad5",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "For a given input of text, return a list of tokens in plain text. For example for the input \"Hello there\", the function should return ['Hello', ' there', ' g', 'pt', '-', '2']. Note that this is very different than just splitting up the text into random chunks or where there are spaces! Tokenizers are designed to create groupings of characters that are often found together or that are significant in the structure of language. Tokenizers make it easier to for the model to learn by providing an algorithm to group language together in a way that helpf the model learn. \n",
    "\n",
    "Hint: You only need to call tokenizer and tokenizer.decode to complete this task!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4e0c5f07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello', ' there', ' g', 'pt', '-', '2']"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def plain_text_tokens(prefix):\n",
    "    '''\n",
    "    Tokenizes prefix and list of it's tokens in plain text\n",
    "    '''\n",
    "    rv = []\n",
    "    encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "    for i in encoded_input['input_ids'][0]:\n",
    "        rv.append(tokenizer.decode([i]))\n",
    "    return rv\n",
    " \n",
    "plain_text_tokens(\"Hello there gpt-2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "997abdb3",
   "metadata": {},
   "source": [
    "Back to our model. We will tokenize our text into numbers to feed into the model. When the model is done predicting text, we can untokenize the results to see if what the model is saying makes sense.\n",
    "\n",
    "Below is some code for the one full pass through the model with the prefix \"The great scientist Albert Einstein\"\n",
    "\n",
    "Take a look at the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "be943853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 50257])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"The great scientist Albert\"\n",
    "encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6b7881f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  464,  1049, 11444,  9966]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2763a39",
   "metadata": {},
   "source": [
    "What is going on here?\n",
    "\n",
    "`torch.Size([1, 4, 50257])` tells us that logits is a 3 dimensional array (i.e., it is 1*4*50257). The first dimension represents the batch size and because there is only one batch, we can ignore it for now. The next dimension is the sequence length. Note that:\n",
    "\n",
    "```python\n",
    ">>> encoded_input['input_ids']\n",
    "tensor([[  464,  1049, 11444,  9966]])\n",
    "```\n",
    "\n",
    "There are four tokens when we tokenize \"The great scientist Albert\" so there will therefore be a sequence length of 4. For the final dimension, we have 50257 which represents the model's vocab size. Why do we have so much data? Don't we only want the next predicted piece of text? \n",
    "\n",
    "This is actually very necessary and it gives us a lot of control in analysis of the model. In total, we have 4 vectors of length 50257. Each vector represents a probability distribution for each token in the vocabulary. For example, if the value at 2048 is higher, that means that the model is assigning a higher probability to the token at position 2048 to be the next in the sequence of text.\n",
    "\n",
    "This makes sense for our purposes: if we have a probability distribution for the next token in the text, we can sample from it to predict the next token! But why do we have four probability distributions in the output. In other words why do we need a probability distribution for each token in the input?\n",
    "\n",
    "The answer to this question is oddly, that this make it easier to train our model! If we have a piece of text that we are training on from the internet, we can train multiple examples in paralell. For example, if we have the text \"The great scientist Albert\" here are a few different examples we could choose to train on. \n",
    "\n",
    "\n",
    "1. Prefix=\"The\" and label=\" great\"\n",
    "2. Prefix=\"The great\" and label=\" scientist\"\n",
    "3. Prefix=\"he great scientist\" and label=\" Albert\"\n",
    "\n",
    "\n",
    "The first three vectors in the logits in the code above correspond to the model's predictions for the first three prefixes above. We only care about the model's label for the input \"The great scientist Albert\" so therefore, we only need to extract the final vector from the probability distribution. This makes sense because for our purposes, we aren't training the model. We are only interested in seeing what the model predicts for the next token.\n",
    "\n",
    "We can extract this last vector by taking `logits[0][-1]`. Lets see what the model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "787d331a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Einstein\n"
     ]
    }
   ],
   "source": [
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "generated_text = tokenizer.decode([token_id.item()]) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe5c317",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "abb4c23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "    token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "    generated_text = tokenizer.decode([token_id.item()])  \n",
    "    text+=generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "fef1da5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The great scientist Albert Einstein, who was a great scientist, was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist. He was a great scientist'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f3010f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619a4a96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c28709",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
