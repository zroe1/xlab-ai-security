---
title: "FGSM & PGD"
description: "Background information on FGSM, BIM, and PGD attacks"
---

## Creating Adversarial Images

Adversarial images are slightly perturbed images which can be misclassified by
a network trained for computer vision. These perturbations could be as small as
changing the value of a few pixels, leading to dramatic reductions in accuracy,
depending on the robustness of the network. While methods such as adversarial
training [@goodfellow2015explainingharnessingadversarialexamples], defensive distilation
[@papernot2016distillationdefenseadversarialperturbations], and countless other techniques
aim to solve this issue, guaranteeing adversarial robustness for computer vision is
still an open research problem.

Adversarial attacks could be a security concern when computer vision models are
deployed in high stakes settings. Imagine a stop sign is physically altered,
such that self-driving car classifies it as a cake. This could cause a car to continue
moving when others expect it to stop, potentially causing a crash.

<p align="center">
  <img src="/images/traffic.png" alt="A descriptive alt text" />
  <br />
  <b>Fig. 1</b>
  <br />
  <em>Source: [@pavlitska2023adversarialattackstrafficsign]</em>
</p>

This segment of the course focuses on generating adversarial samples to fool a
convolutional neural network trained on the CIFAR-10 dataset.
We will cover the Fast Gradient Sign Method (FGSM), its iterative variant (BIM),
and Projected Gradient Descent (PGD). The Carlini-Wagner attack [@carlini2017evaluatingrobustnessneuralnetworks] and black
box methods such as the Square Attack [@andriushchenko2020squareattackqueryefficientblackbox] will be covered in later pages as well.

## Distances

There are a few metrics for calculating the 'distance' between the original image and the perturbed one, the most notable being $L_0$, $L_2$, and $L_{\infty}$ [@carlini2017evaluatingrobustnessneuralnetworks].
$L_0$ is equivalent to the number of non-matching pixels, and is easy to calculate.
$L_2$ is the typical 'norm' used in linear algebra, and refers to the vector distance between the two images.
$L_{\infty}$ calculates the maximum perturbation to any of the pixels in the original image.

For our attacks we will use, $L_{\infty}$ because it is simple, cheap to calcuate, and historically
conventional for the kinds of attacks we are performing. It is also intuitive: a single dramatically
changed pixel (for example, green to pink), would be easy to spot.
Minimizing an $L_\infty$ metric, for example, to keep all changes within a $8/255$,
is typically enough to prevent our adversarial images from becoming suspicious.

<Dropdown title="Understanding L-norms in More Detail">

The choice of distance metric significantly impacts both the attack strategy and the resulting adversarial examples. Here's a deeper dive into each norm:

**L₀ Norm (Sparsity)**

- Counts the number of pixels that have been changed
- Useful when you want to minimize the number of altered pixels
- Can result in dramatic changes to individual pixels
- Often used in patch-based attacks

**L₂ Norm (Euclidean Distance)**

- Measures the standard geometric distance between original and perturbed images
- Tends to spread perturbations across many pixels with smaller individual changes
- More mathematically convenient for optimization
- Often results in smoother-looking perturbations

**L∞ Norm (Maximum Change)**

- Constrains the maximum change to any single pixel
- Ensures no pixel is changed by more than ε
- Practical for ensuring visual imperceptibility
- Standard choice for many attack papers due to its intuitive interpretation

The choice between these norms represents different threat models and practical constraints in real-world scenarios.

</Dropdown>

## FGSM

FGSM is an approach used to simply generate adversarial samples. These are typically not the smallest perturbations required to induce a network to misclassify the input image, but are generated quickly. An adversarial image generated by FGSM may remain indistinguishable from the original one to the human eye, but the distance between it and the real image, calculated using an L-norm, would be higher than a more intensive method like PGD. In this approach, the signs of the gradient of the loss function with respect to the input image are used to perturb the original image to produce adversarial output.

$$
x' = x + \epsilon \cdot sign(\nabla loss_{F,t}(x))
$$

To create an adversarial image using FGSM, there are only a few steps. Using the same loss function used to train the model, generate the loss with respect to the input image. Then, calculate the gradient of the loss function with regard to the input image data, figure out the sign of each pixel, and adjust the original image accordingly. Intuitively, the direction required to reduce loss is reversed, to make the model less accurate.

## BIM

This Iterative Method involves the same approach of calculating the signs of inputs, but instead, a few iterations of this is done with a smaller multiplicative parameter.

```math
x'_i = x'_{i-1} + clip_\epsilon(\alpha \cdot sign(\nabla loss_{F,t}(x'_{i-1})))
```

## PGD

PGD is very similar to iterative FGSM, only differing by initializing random noise instead of starting with no perturbation. It is a standard approach which continues to be used to generate adversarial input nowadays. PGD is relatively easy to implement, and also lacks computational complexity, leading to a useful benchmark adopted by many researchers to test model robustness. This will be covered in greater detail in Section 2.4.

<NextPageButton />

# Citations
