---
title: "FGSM & PGD"
description: "Background information on FGSM, Iterative FGSM, and PGD attacks"
---

## Creating Adversarial Images

Adversarial images are slightly perturbed images which can be misclassified by a network trained for computer vision. These perturbations could be as small as changing the value of a few pixels, leading to dramatic reductions in accuracy, depending on the robustness of the network. Adversarial images are typically indistinguishable to the human. Adversarial training is the process of adapting a model to become more resilient towards these inputs, and can involve further training or processing. A lack of adversarial training has a dearth of possible negative real-world consequences. For one, a road sign could be slightly physically altered, leading to misclassification by a self-driving car, and a collision.

More on adversarial training will be covered later in this course. This segment of the course focuses on generating adversarial samples to fool a Convolutional Neural Network trained on the CIFAR-10 dataset, using a few methods. We will cover the Fast Gradient Sign Method (FGSM), its iterative variant (IGSM), and Projected Gradient Descent (PGD). The Carlini-Wagner approach (CW) and black box methods such as the Square Attack will be covered in later pages, too. 

## Distances 

There are a few metrics for calculating the 'distance' between the original image and the perturbed one.
```math
\|v\|_p = \left( \sum_{i=1}^{n} |v_i|^p \right)^{\frac{1}{p}}.
```



## FGSM

FGSM is an approach used to simply generate adversarial samples. These are typically not the smallest perturbations required to induce a network to misclassify the input image, but are generated quickly. An adversarial image generated by FGSM may remain indistinguishable from the original one to the human eye, but the distance between it and the real image, calculated using an L-norm, would be higher than a more intensive method like PGD. In this approach, the signs of the gradient of the loss function with respect to the input image are used to perturb the original image to produce adversarial output. 

$$ x' = x - \epsilon \cdot sign(\nabla loss_{F,t}(x))
$$

To create an adversarial image using FGSM, there are only a few steps. Using the same loss function used to train the model, generate the loss with respect to the input image. Then, calculate the gradient of the loss function with regard to the input image data, figure out the sign of each pixel, and adjust the original image accordingly. Intuitively, the direction required to reduce loss is reversed, to make the model less accurate.

## Iterative FGSM

Iterative FGSM involves the same approach of calculating the signs of inputs, but instead, a few iterations of this is done with a smaller multiplicative parameter.

```math
x'_i = x'_{i-1} - clip_\epsilon(\alpha \cdot sign(\nabla loss_{F,t}(x'_{i-1})))
```

## PGD

PGD is very similar to iterative FGSM, only differing by initializing random noise instead of starting with no perturbation. It is a standard approach which continues to be used to generate adversarial input nowadays.




Sources:

https://arxiv.org/pdf/1608.04644
