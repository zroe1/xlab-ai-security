---
title: "FGSM & PGD"
description: "Background information on FGSM, BIM, and PGD attacks"
---

This section has a series of coding problems using PyTorch. _As always, we highly recommend you read all the content on this page before starting the coding exercises._

<ExerciseButtons
  githubUrl="https://github.com/zroe1/xlab-ai-security/blob/main/working/FGSM_BIM_PGD.ipynb"
  colabUrl="https://colab.research.google.com/github/zroe1/xlab-ai-security/blob/main/working/FGSM_BIM_PGD.ipynb"
/>

## Creating Adversarial Images

This segment of the course focuses on generating adversarial samples to fool a convolutional neural network trained on the CIFAR-10 dataset. We will cover the Fast Gradient Sign Method (FGSM), its iterative variant (BIM), and Projected Gradient Descent (PGD). The Carlini-Wagner attack [@carlini2017evaluatingrobustnessneuralnetworks] and black box methods such as the Square Attack [@andriushchenko2020squareattackqueryefficientblackbox] will be covered in later pages as well.

## Distances

There are a few metrics for calculating the 'distance' between the original image and the perturbed one, the most notable being $L_0$, $L_2$, and $L_{\infty}$. $L_0$ is equivalent to the number of non-matching pixels, and is easy to calculate. $L_2$ is the typical norm used in linear algebra, and refers to the vector distance between the two images. $L_{\infty}$ calculates the maximum perturbation to any of the pixels in the original image.

For our attacks we will use $L_{\infty}$ because it is simple, cheap to calculate, and historically conventional for the kinds of attacks we are performing. It is also intuitive: a single dramatically changed pixel (for example, green to pink), would be easy to spot. Minimizing an $L_\infty$ metric, for example, to keep all changes within a $8/255$, is typically enough to prevent our adversarial images from becoming suspicious.

<Dropdown title="Understanding L-norms in More Detail">

The choice of distance metric significantly impacts both the attack strategy and the resulting adversarial examples. You may find some level of inconsistency across the literature describing the different distance metrics. For our course, we use the definition of an $L_p$ norm outlined in [@carlini2017evaluatingrobustnessneuralnetworks] where $v = x_\mathrm{original} - x_\mathrm{adversarial}$

$$
\|v\|_p = \left(\sum_{i=1}^{n} |v_i|^p\right)^{\frac{1}{p}}
$$

If you are interested, you should be able to find the limit of the $L_p$ norm as $p \rightarrow \infty$ is equivalent to the maximum difference between any pixel value in $x_\mathrm{original}$ and $x_\mathrm{adversarial}$. Likewise, you should find that when $p = 0$ the $L_p$ norm is equivalent to the number of pixels changed.

</Dropdown>

## FGSM

Fast gradient sign method (FGSM) [@goodfellow2015explainingharnessingadversarialexamples] is a simple approach used to generate adversarial samples quickly. While the approach is efficient, it has the downside of having a lower chance of being effective.

To create an adversarial image using FGSM, there are only a few steps. Using the same loss function used to train the model, generate the loss with respect to the input image. Then, calculate the gradient of the loss function with respect to the input image data. Finally, adjust the original image based on the sign of its gradient.

$$
x' = x + \epsilon \cdot \mathrm{sign}(\nabla \mathrm{loss}_{F,t}(x))
$$

Intuitively, you are moving the image in a direction which increases the loss, making the model less accurate.

## BIM

The Basic Iterative Method (BIM) [@kurakin2017adversarialmachinelearningscale] involves the same approach of calculating the signs of inputs, but instead, a few iterations of this is done with a smaller multiplicative parameter.

```math
x'_i = \mathrm{clip}_\epsilon(x'_{i-1}  + \alpha \cdot \mathrm{sign}(\nabla \mathrm{loss}_{F,t}(x'_{i-1})))
```

This should look similar to equations you have seen before for [gradient descent](https://www.khanacademy.org/math/multivariable-calculus/applications-of-multivariable-derivatives/optimizing-multivariable-functions/a/what-is-gradient-descent), but instead of optimizing the weights of the model we are training, we are optimizing the input.

## PGD

PGD is very similar to iterative FGSM, only differing by initializing random noise instead of starting with no perturbation. PGD continues to be used as a standard approach in research today. PGD is relatively easy to implement and efficient, making it a useful benchmark adopted by many researchers to test model robustness.

<NextPageButton />

## References
