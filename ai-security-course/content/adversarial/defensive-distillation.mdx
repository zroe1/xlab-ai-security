---
title: "Defensive Distilation"
description: "Background information and mathematical intuition behind defensive distilation."
---

This section has a series of coding problems using PyTorch. _As always, we highly recommend you read
all the content on this page before starting the coding exercises._

<ExerciseButtons
  githubUrl="https://github.com/zroe1/xlab-ai-security/blob/main/working/working/defensive_distillation.ipynb"
  colabUrl="https://colab.research.google.com/github/zroe1/xlab-ai-security/blob/main/working/working/defensive_distillation.ipynb"
/>

In this section, you will learn about "defensive distilation," a technique to defend
against adversarial attacks in computer vision. Although this defense has be broken [@carlini2017evaluatingrobustnessneuralnetworks],
similar techniques with analogous motivations have pushed the state of the art forward [@bartoldson2024adversarialrobustnesslimitsscalinglaw].
In this document, we will explain distilation, then defensive distilation and the intuition behind the defense.

## Distilation

Distilation, proposed by [@hinton2015distillingknowledgeneuralnetwork], is a technique
to leverage a more complex and capible model to produce a more compute-efficent model
that preserves the performace. The author's accomplish this by training the smaller model
on the outputs of the capible model.

As a review, when we train an image classifer we usually use what are called "hard labels".
For an image of a 2 in the MNIST dataset, the hard label would assign 100% probability to the
"2" class and 0% probability to every other class. In distilation however, we use "soft labels"
from a trained model which assign various positive probabilites to every class.

Why train on these soft labels? Aren't the hard labels a more accurate measure of ground truth?
The original paper claims that these soft labels are useful because they give context to the underlying
structure of the dataset. One example they give is for an image of a 2, a model
may give $10^{-6}$ probability of the image being a 3 and a $10^{-9}$ probability of it being a 7.
In another example, of a 2 you may find the reverse. The authors claim that this extracts
more detailed infromation about the data. Rather than training a model to learn what is a
2 and what is not a 2, we can use these soft labels to also teach the model which 2s look more like
3s than they do 7s.

### Adding Temperature

A traditional softmax is calculated via the following equation. When there are $K$ classes,
and $z$ is the pre-softmax output, the equation below gives the probability for class $i$:

$$
q_i = \frac{e^{z_i}}{\sum_{j=0}^{K-1}e^{z_j}}
$$

If we want the output of the softmax to be smoother, we can add a constant $T$ which
forces the distibution to be more uniform. Note that are traditional softmax is the equivalent
to the equation below when $T = 1$.

$$
q_i = \frac{e^{z_i / T}}{\sum_{j=0}^{K-1}e^{z_j / T }}
$$

Why add temperature? Smoother labels make distinctions between small probabilites
more salient. Returning to our example of an MNIST image of a 2, you may get
softmax probabilies as low as $10^{-6}$ or $10^{-9}$. The difference between these values
encodes real information, but if you train on these outputs, they are so close to zero
that they barely influence the loss.

<Dropdown title="Soft labels don't replace hard labels">

For the reasons mentioned above, training on soft labels is helpful for distilling
a larger model into a lighter weight alternative. In practice, it should be often
useful to train on some combination of the soft labels
and the hard labels. The authors note:

> Typically, the small model cannot exactly match the soft targets and erring in the direction of the
> correct answer turns out to be helpful.

In other words, you should be aware that soft labels encode useful information but shouldn't
be treated as a complete replacement of hard labels.

</Dropdown>

## Defensive Distilation

In AI security we are less concerned with efficiency and more concerned with robustness.
Therefore, unlike the variation proposed by [@hinton2015distillingknowledgeneuralnetwork],
defensive distilation, uses the same model archieture for both the original and
distilled models.

The motivation behind using distilation as a defense is it produces a smoother model
which generalizes better to inputs an epsilon distance away from clean images. In other words,
gradients of the loss with respect to indiviudal pixel values should be small if
a model is distilled properly. The authors say:

> If adversarial gradients are high, crafting adversarial samples becomes easier
> because small perturbations will induce high DNN output
> variations. To defend against such perturbations, one must
> therefore reduce variations around the input, and consequently
> the amplitude of adversarial gradients

You can think about navigating up a mountain where climbing up in a certain direction
increases the loss. When you are optimizing an adversarial example, you are trying to
climb up as fast as possible. If a model is properly distilled however, it will be
difficult to climb up because the landscape is mostly flat.

<p align="center">
  <ThemeImage
    lightSrc="/images/distilation_light.png"
    darkSrc="/images/distilation_dark.png"
    alt="Swiss cheese security model"
    style={{ align: "center", width: "80%", display: "block", margin: "0 auto" }}
  />
</p>

### Implmenting defensive distilation
