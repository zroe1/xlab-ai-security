---
title: "Ensemble Attacks"
description: "Background information and mathematical intuition behind ensemble attacks on image classifiers."
---

One interesting feature of adversarial images is that they often "transfer" to
other models. By transfer, we mean that we can optimize an adversarial example on
one model, and use it to successfully attack an entirely different model. We can
see an analogous quality for the adversarial suffix jailbreaks on language models
which you will explore in the [GCG section](https://xlabaisecurity.com/jailbreaking/gcg/) later in this course.

## Why Care About Transferability

Transferible adversarial images and jailbreaks are interesting, but it may not be
entirely obvious why they are important for security. One reason is that in many cases,
an attacker may want to attack a model that is sucured behind an API. In other words,
the attacker doesn't have white-box access and therefore cannot run an algorithm like
PGD or CW. They may also not be able to query the model repeatidly without arousing
suspicion or encurring high API costs. One solution to this issue is to attack a white
box model for free using as many iterations as the attacker would like, and then hope
that the attack transfers. When a white box model is used in this way, you may hear
it refered to as a "surrogate model." Because there are plenty of open source models available for
download, there are plenty of surrogate models to choose from, making these transfer attacks practical to execute.

## Improving Transferability

One problem with attacks that rely on transferibility is that they aren't reliable.
Sometimes the transfer works, but other times it doesn't and in practice, it is difficult
to know which model is a good choice to use as a sorrogate. One solution to this
problem is to use a diverse selection of sorrogate models rather
than to choose one and hope for this best. These kinds of attacks are called ensamble
attacks [@liu2017delvingtransferableadversarialexamples].

In an ensamble attack, you choose $k$ models and assign each a weight $\alpha$ for how much that model should
influence the attack loss. Traditionally $\sum_{i=1}^k \alpha_i = 1$, so you can think about
the total loss as being shared between each model. If $x$ is our clean image and $\delta$ is our adversarial
perturbation, let $\ell_i(x + \delta)$ be the attack loss for a specific model $i$.
You can think of $\ell_k(x + \delta)$ as being similar to the $f$ function from
the Carlini-Wagner attack in a previous section. Let $D(\delta)$ be some distance
metric such as an $L_p$ norm. Then we try to find $\delta$ such
that it minimizes the following equation.

$$
\argmin_\delta D(\delta) + \sum_{i=1}^k \alpha_i \cdot \ell_i(x + \delta)
$$

In the original paper, the authors use cross entropy loss for $\ell$ but other attacks
that are conceptually similar can use other losses.

<Dropdown title="Comments on notation">

The original authors of the [Delving into Transferable Adversarial Examples and Black-box Attacks](https://arxiv.org/pdf/1611.02770)
paper use a very different notation than the one we use above. We changed the notionation
for clarity and to be more consistent with the other notebooks and eaiser to understand,
but it is a good excercise to get used to parsing less-than-ideal notion. For reference,
here is the orginal formulation for optimizing ensemble attacks. $y^*$ is undersood to be
a one hot vector and $J_i$ gives the softmax probabilities for model $i$. For more details
you can reference the paper itself.

$$
\argmin_{x^*} - \log \left( \left( \sum_{i=1}^k \alpha_i J_i(x^*) \right) \cdot \mathbf{1}_{y^*} \right) + \lambda d(x, x^*)
$$

</Dropdown>

## Citations
