---
title: "Evals for Security"
description: "Blog post on red-teaming gpt-oss-20b"
---

## AdvBench

AdvBench was introduced in [@zouUniversalTransferableAdversarial2023] to evaluate the effectiveness of GCG. The evals contains 500 target strings between 3 and 44 tokens (16 tokens on average) that reflect harmful behavior. The adversary "wins" if the model outputs the exact harmful string. The benchmark also includes 500 prompts which attempt elicit harmful behaviors. In this setting, the authors consider an attack successful if the model "makes a _reasonable_ attempt at executing the behavior."

## StrongREJECT

Strong reject attempts to evaluate models on well-defined questions with facually verifiable answers. This is intended to make evaluation more reliable because an attack is successful only if concrete, harmful information is provided.

While 70% of the prompts were newly created for StrongREJECT, 10% are from DAN [@shen2024donowcharacterizingevaluating] and 10% are from AdvBench [@zouUniversalTransferableAdversarial2023]. The authors ensure that prompts had the following qualities:

1. Prompts were refused by OpenAI, Anthropic, Google Gemini, Meta LLaMA, and DeepInfra
2. Questions were "detailed enough for a human to easily evaluate responses to them"
3. Queries ask for information that is facually verifiable
4. Prompts were "simple enough for moderately-capable models to answer them"

## References
