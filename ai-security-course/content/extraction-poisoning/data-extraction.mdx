---
title: "Extracting Training Data from LLMs"
description: ""
---

## Motivation
In this section, we'll focus on the extraction of training data from LLMs. This also often falls under the category of "memorization," although extracting training data—the focus of this write-up—generally implies the exist of an adversary seeking to retrieve such data. 

Due to the risk of exposing personally identifiable information (PII), most methods of extracting training data at quickly fixed in the frontier models (although may remain a problem in open-weight model). 

## Base Case: GPT-2
@carliniExtractingTrainingData2021 propose one of the first methods for extracting LLM training data, specifically from GPT-2 (and in case you're wondering: no, Carlini isn't the only researcher to have ever published in this field, but yes, he is an author on the majority of papers cited in Section 5). They're primarily interested in a concept they dub $k$-eidetic memorization: a string $s$ is $k$-eidetic memorized by an LLM if $s$ is extractable and appears in at most $k$ training examples.

<Dropdown title="What does it mean for a string to be extractable?">

The original defines a string $s$ as extractable from a model $f_\theta$ if there exists a prefix $c$ such that
$$
s \gets \underset{s': |s'| = N}{\arg \max} f_\theta(s' | c),
$$
i.e., a string $s$ is considered extractable if there exists a prefix $c$ such that when given to the LLM, the most likely continuation is $s$. In practice, the $\arg \max$ computation is intractable and we can use greedy decoding as a substitute.

The authors note, however, that there are a number of corner cases for which this definition breaks down. The example thy give is a prompt similar to "Repeat this sentence: _____". This trivially allows the model to repeat *any* string, technically classifying it as "memorized." The authors mention that they avoid such corner cases by only prompting the LLMs with short prefixes. We note, though, that even short prefixes may not completely solve this problem, as it may be feasible to use, e.g., GCG to optimize just a few tokens to elicit a model response. All such corner cases are against the "spirit" of memorization, however, and we hope it is clear that they can (and should) be safely ignored.

</Dropdown>

Clearly, small $k$-eidetic memorization is the most harmful, as it means that if a language model sees an example *only a few times,* it can memorize it. We don't want GPT-2 spitting out peoples' names and addresses just because they showed up in its training set once! 

### An Initial Attack
The authors' first attack involves simply prompting GPT-2 with the start-of-sequence token, then letting the model autoregress using top-$k$ sampling [@fan2018hierarchical], collecting 200,000 samples in total. They then find the lowest-perplexity samples, as the model being very "unsurprised" by a given piece of text likely means it has memorized it. Unfortunately, this scheme is rather bad. It mostly finds either false positives (e.g., strings repeated over and over) or content that *is* memorized, but is done so expectedly (e.g., the MIT public license). 

### A Better Attack
The problem with the previous setup was that even with the top-$k$ sampling, the samples had to be likely from start to finish to be generated by the model; the model was essentially stuck in a "local minima" of high-probability token generation. How can we combat this? You might remember the concept of *temperature* applied to the softmax function way back from the [defensive distillation writeup](https://xlabaisecurity.com/adversarial/defensive-distillation/). 

<Dropdown title="I don't remember.">

No worries! We apply a temperature $T$ to the softmax by dividing all the input logits by $T$:
$$
\frac{e^{z_i / T}}{\sum_{j = 1}^n e^{z_j / T}}
$$
If we sample with a temperature $T > 1$, the output softmax distribution gets flattened, as dividing by a larger number squishes all the logit values $z_j$ closer together. Thus, we make the model less "certain" about the next token to generate (increasing the distribution's entropy!). As a result, it becomes more likely that we see "unexpected" or "interesting" tokens. As you might expect, using a $T < 1$ has the exact opposite effect, and $T = 1$ gives the normal softmax distribution. For a good visual representation of temperature, see [3Blue1Brown's short](https://youtube.com/shorts/XsLK3tPy9SI?si=kx-m0BXqKUKY4HWJ).

</Dropdown>

To make our model more creative, we can use a higher temperature during generation. There's just one problem with this simple approach: a high temperature makes the model likely to step off its generative path, which we don't want if it's generating a memorized example. Thus, the authors sample starting with $T = 10$ and decaying down to $T = 1$ over the first 20 generated tokens (about 10% of the length of the full sequence), helping balance both [exploration and exploitation](https://en.wikipedia.org/wiki/Exploration%E2%80%93exploitation_dilemma). As an alternate method, the authors used prefixes from the [Common Crawl dataset](https://commoncrawl.org/) to prompt GPT-2, helping even further with diverse generation.

Finally, the authors aimed to find a better proxy for a sampled string actually being training data (this is called "membership inference" and is a very active area of ML research). They propose a number of methods, but a one clever yet simple method that we'll cover is using zlib compression. Why? Well, we'd expect the interesting memorized pieces of text to have low perplexity to GPT-2, but not low entropy *in general.* For example, a name and address memorized by GPT-2 would have quite low perplexity, but for a different language model that didn't see that name and address, the perplexity would be rather high. 

Similarly, say we have a string repeated over and over that we've extracted from GPT-2. This string will have low perplexity to GPT-2, but because of its highly repetitive structure, it will be compressed very strongly by zlib (which relies on repeated patterns to compress data). On the other hand, if we've instead sampled a bunch of PII from GPT-2, it again will have low perplexity to GPT-2, but it won't be compressed well by zlib. Thus, comparing zlib's entropy (the bits required to compress the data) and GPT-2's perplexity serves as a solid method for membership inference.

### The Better Results
Using the internet prefixes for sampling and zlib for membership inference resulted in a 67% true positive rate, the best out of all methods tested by the authors. The relationship between the perplexity and zlib compression entropy is quite striking, and can be seen in Figure 1.

<p align="center">
  <ThemeImage
    lightSrc="/images/hidden_dim.png"
    darkSrc="/images/hidden_dim.png"
    alt="Relationship between zlib entropy and GPT-2 perplexity (when sampled using top-$n$ sampling"
    style={{ align: "center", width: "80%", display: "block", margin: "0 auto" }}
  />
</p>

<div align="center">
  **Fig. 1** <br></br> Relationship between zlib entropy and GPT-2 perplexity (when sampled using top-$n$ sampling. Red dots were selected for inspection, and blue dots were confirmed as memorized. (Figure 3 from @carliniExtractingTrainingData2021)
</div>

The authors extracted 604 memorized training examples in total, notably of which 46 contained named individuals (barring news articles) and 32 contained individuals' contact information. The authors further go on to find that the rate of memorization increased as model size increased and can occur with the memorized example being in the training data only 33 times. There are many other interesting results (see the [original paper](https://arxiv.org/abs/2012.07805)) but the main takeaway is that at least GPT-2-like models *do* memorize parts of their training data, and it *is possible* for adversaries to extract it.

## What about production language models?



## References
