---
title: "A Note on Ethics"
description: "A guide for ethical AI security research"
---

Before starting the course, it is worth taking a second to pause to consider why researchers are doing
this work to begin with and what ethical questions are at stake. Questions about ethics come up
in almost every field of research, especially research that involves human subjects. For AI research,
we claim that these ethical questions are especially relevant. Because AI is becoming increasingly integrated
into our everyday lives, everyone with internet access has become a kind of human subject in a huge number of ML experiments.
In the future, AI models may pose a significant threat even to those who don't use them,
making ethical questions surrounding the technology extremely consequential.

This document does not take a dogmatic position on what qualifies as a "bad" or "good" action.
What actions are morally permissable and which are objectionable remains, to put it mildly,
a very active debate within the field of moral philosophy.
Rather than taking positions on these ongoing debates,
this document is a complilation of best practices for doing ethical AI security research.

## Risks From AI Security Research

Before giving recommendations, we give a list of possible failure modes of AI security
research. This list is not entended to be extensive. Instead, it should give an idea of
why we think it is necessary to think about ethics in the first place.

### Data Risks

When working in AI security you may at some point get your hands on potentially harmful
data. Here are some examples for how this may happen:

1. In some cases, researchers create datasets of harmful responses for the purposes of performing a jailbreak (e.g., [@anil2024manyshot]).
2. You may also be asking a model to generate harmful content and storing those responses for the purposes of benchmarking it.
3. You may be collecting a dataset and filtering out harmful content and storing it somewhere.

Depending on what laws you are subject to, the generation or storage of some of this data, in some
very rare cases, may be
illegal. In general this isn't something you should have to worry too much about, but we note
that the one area that you should be esspecially careful with is [CSAM](https://www.justice.gov/d9/2023-06/child_sexual_abuse_material_2.pdf)
which is illegal to release or store, even if it is on accident.

Even when releasing data is legal (which is almost all cases), doing so may still
pose ethical problems. For example, if you release a dataset with harmful responses to questions, that dataset
could be used by a bad actor to perform supervised finetuning on an opensource model
to make it more likely to respond to harmful queries. In general, a creative actor
may be able to find a number of malicious uses for datasets you may release. Thus, before
making a dataset public, you should consider a variety of possible threat models.

### Model Risks

Some AI security research is done on image classifiers which in general are very
safe to release. For most AI security research that deals with language, researchers
do not train foundational language models from scratch.

If you are finetuning a language
model however, to remove safety filters or increase it's ability to do harmful tasks, this can quickly
become ethically dubious. Except for extremely small models, _we do not reccommend you do this_. First, the model itself may be able to cause harm in the
world. It could also be misused to generate harmful data. Before releasing any model,
you should check it's level of safety filters and consider different threat models.

### Information Risks

Information you produce can also be dangerous in less obvious ways. Nick Bostrom
coined the term information hazard [@bostrom2011information] which he defines as follows:

> _Information hazard:_ A risk that arises from the dissemination or the potential
> dissemination of (true) information that may cause harm or enable some agent to cause
> harm.

For example, if you develop a technique that makes a model much more powerful, but also much
less safety-aligned, publishing a paper on your method may be considered an information hazard.
The issue is not that what you are saying isn't true. Rather, the issue is that making that
technique public doesn't make anyone better off.

## Honest Benchmarking

Multiple researchers have expressed their frustration with low-quality benchmarking in the
literature. Many defense papers
don't benchmark their technique against the best attacks. Likewise, many attack
papers don't benchmark their results on the best defenses.

There are several reasons why so many papers report metrics that don't hold up in
other settings. First, implementing a state-of-the-art attack or defense can actually
be quite difficult. For example, if you are working with Llama 3.1 405B, even loading a model
this large into memory can be challenging, let alone implementing a complicated
attack or defense. Another reason researchers may not benchmark properly is because
they subconciously want to make their attack/defense look as good as possible. It
is not in their best intrest to expend lots of effort trying to prove their paper wrong!

At the very least, we beleive that researchers should be as transparent as possible
about the metrics they report. If they didn't report results against a
state-of-the-art attack or defense because it is technical
difficult to implement or for any other reason, they should say so.

## Good Faith Attempts at Defenses

The best researchers don't break systems merely because it is "cool." The break systems
because they genuinly want to live in a world where technology is safe, robust and protects
the rights of the people who use it. So although we belive there is extremely good reasons to develop increasingly sophisticated ways to attack ML systems, the ultimate goal of
AI security research is to make models safer. As a result, when if you propose a new
attack, it is typically considered good taste to build out defenses for that attack in the same
paper. You may find that there aren't existing methods to protect against your attack which
is fine as long as you make an honest attempt to try to secure the vulnerability you discovered.

## Disinterested Ethical Analysis

We belive the best strategy for doing ethical AI security research is "disinterested
ethical analysis." This term comes from
[Ethical Frameworks and Computer Security Trolley Problems:
Foundations for Conversations](https://www.usenix.org/conference/usenixsecurity23/presentation/kohno) [@291190] which adresses the conncetion
between moral philosophy and computer security work. While moral
philosophy can be a useful tool while making important descions, it must be applied properly
to be useful. Because moral philoshy is such a rich field, you will likely be able to find a
moral framework to justify anything you would like. Likewise, even without explicitally referencing
any ethial theory, a smart person may be able to come up with reasonable arguments to justify
a large range of objectionable behavior.

Researchers reasonably may want to persure directions that they think are interesting
or promising and ethical delema can be a obstacle towards doing the work they want to
do. It is important to be aware of this bias and attempt to remove your own interests
from descions that may harm other people or the public.

Below we include a more desciptive explanation of the concepts described above.
We will almost never include such a long block quote in this course but the idea here
is so important and well said that we include the original paragrah from the paper in full:

> Decision-makers (researchers, program committees, others) should consider ethics before making decisions, rather than after. For certain moral dilemmas
> [...], it is possible to pick an outcome and then find the ethical framework that justifies that
> outcome. We do not argue for this practice. Instead, decisionmakers should let the decision follow from a disinterested
> ethical analysis. Toward facilitating disinterested analyses,
> we encourage decision-makers to explicitly enumerate and
> articulate any interests that they might have in the results of
> the decision; such an articulation could be included as part of
> a positionality statement in a paper.

## References
