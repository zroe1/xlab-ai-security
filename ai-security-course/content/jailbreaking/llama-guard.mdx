---
title: "Llama Guard"
description: "Defending Language Models with Language Models"
---

## Background
We've seen now that relatively simple defenses like perplexity filters are able to foil the earliest optimization-based attacks like GCG. These methods, however,aren't robust against prompt-level jailbreak techniques and aren't great at adapting to specific policies and rules. For this purpose, we need a new approach.

## Llama Guard
One of the earliest and most straightforward defenses proposed for this purpose is Llama Guard [@inanLlamaGuardLLMbased]. The key concept of Llama Guard is very simple: it's a fine-tuned version of Llama-2 for the purpose of differentiating objectionable from permissible content. Specifically, the original paper uses "four key ingredients":

1. A set of guidelines: each task (passed as a prompt) given to Llama Guard also contains a set of guidelines that the model follows when classifying the content. It *only* follows these guidelines.
2. Who to classify: each task also denotes whether to classify the user's or the agent's messages.
3. The conversation: each task (unsurprisingly) contains the conversation that Llama Guard scrutinizes.
4. The output format: each task contains an output format; Llama Guard specifically responds either just "safe" or "unsafe" along with the index of the violated guideline category.

The authors then fine-tune Llama-2 on the prompts from Anthropic's [helpfulness and harmfulness dataset](https://huggingface.co/datasets/Anthropic/hh-rlhf) with their own labels, following Llama Guard's task structure (note: we believe the citation in the original Llama Guard paper is incorrect). Using Llama Guard is as simple as formatting a conversation into the given task structure and sending it as a prompt.

<p align="center">
  <ThemeImage
    lightSrc="/images/llama_guard.png"
    darkSrc="/images/llama_guard.png"
    alt="Llama Guard"
    style={{ align: "center", width: "100%", display: "block", margin: "0 auto" }}
  />
</p>

<div align="center">
  **Fig. 1** <br></br> Llama Guard; Figure 1 from @inanLlamaGuardLLMbased
</div>


## Does it work?
A somewhat common theme throughout the jailbreaking section has been that getting LLMs to do things for us often just works, and this trend is not going to stop with Llama Guard. Llama Guard outperformed OpenAI's moderation API on its own test set, and performed nearly as well on OpenAI's own moderation dataset without undergoing *any* training on it, demonstrating a remarkable amount of of adaptability. Llama Guard was also able to improve its performance on new categories with in-context learning and by fine-tuning on other taxonomies. In fact, Llama Guard 4 now has [multimodal safety capabilities](https://huggingface.co/meta-llama/Llama-Guard-4-12B)â€”it's probably not going anywhere anytime soon.

## References
