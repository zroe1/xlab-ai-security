---
title: "Many-Shot Jailbreak"
description: "Defending Language Models with Language Models"
---

The many-shot jailbreak [@anil2024manyshot] fills the context of an LLM with a long list of examples
of the model responding of objectionable context. Then, when you ask a harmful query at the end of the
list of examples, the model will often continue the pattern of responding harmfully.

The autors note that LLMs can now have context sizes as long as 10 million tokens
which "provides a rich new attack surface for LLMs." This means that in addtion to being an effective attack,
the many-shot jailbreak serves a perfect example of how attacks can evolve with technology.
As researchers make new breakthroughs in ML, there will be new attack surfaces to exploit.

## References
