---
title: "Visual Jailbreaks"
description: "Visual Adversarial Examples Jailbreak Aligned Large Language Models"
---

This section has a series of coding problems using PyTorch. _As always, we highly recommend you read
all the content on this page before starting the coding exercises._

<ExerciseButtons
  githubUrl="https://github.com/zroe1/xlab-ai-security/blob/main/working/404.ipynb"
  colabUrl="https://colab.research.google.com/github/zroe1/xlab-ai-security/blob/main/working/404.ipynb"
/>

## Motivation

So far, we've looked at jailbreaks of pure LLMs, where text alone is a valid attack surface. Many models today, however, are multimodal: they accept both language _and_ vision as inputs (these are often called vision-language models, or VLMs). Consequently, we can use this images to jailbreak models.

## The Method

<p align="center">
  <ThemeImage
    lightSrc="/images/visual_jailbreaks_light.png"
    darkSrc="/images/visual_jailbreaks_dark.png"
    alt="PAIR Algorithm"
    style={{ align: "center", width: "100%", display: "block", margin: "0 auto" }}
  />
</p>

<div align="center">
  **Fig. 1** <br></br> Visual Jailbreaks, modified from @qiVisualAdversarialExamples2024
</div>

To learn how to do this, we'll work through the methodology of [Visual Adversarial Examples Jailbreak Aligned Large Language Models](https://arxiv.org/abs/2306.13213) [@qiVisualAdversarialExamples2024]. The intuition aligns closely with GCG's: in GCG, we take some harmful request that the model would otherwise not answer and optimize an adversarial suffix to hopefully force the model to answer the query, whereas @qiVisualAdversarialExamples2024 instead optimizes an _image_ to force an answer. Optimizing images has two distinct advantages. First, we can optimize in a continuous space, which is much easier than the the discrete token space of GCG. Second, because optimization is much easier, rather than optimizing for a single target response, we can optimize for a corpus of harmful queries. Ideally, this leads to the image becoming a _universal jailbreak._

Formally, given a corpus of harmful text $Y = \{ y_i \}_{i = 1}^m$, we create the adversarial example by maximizing the probability of generating this corpus given our adversarial image:

$$
x_{\text{adv}} := \underset{\hat{x}_{\text{adv} \in \mathcal{B}}}{\arg \min} \sum_{i = 1}^m - \log\big( p(y_i | \hat{x}_{\text{adv}}) \big).
$$

$\mathcal{B}$ is a constraint on the input space; the original paper uses $\left\lVert x_{\text{adv}} - x_{\text{benign}}\right\lVert_\infty \leq \epsilon$ for their constrained attacks, although unconstrained attacks are also feasible.

## Does this have huge implications?

The fact that we can optimize images is nothing new, but as models become increasingly multimodal you might presume that these easier image optimizations are a great area of concern for security. Interstingly, though, @schaefferFailuresFindTransferable2025 recently found that image jailbreaks generally _don't_ transfer between modern VLMs. Thus, these attacks are still an area of concern for open-source models, but likely not for closed-source frontier models. This result is particularly interesting given that adversarial attacks on LLMs and image classifiers have shown great transferability; there's something different going on with VLMs!

<FeedbackButton />

<NextPageButton />

## References
