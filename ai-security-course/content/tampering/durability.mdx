---
title: "A Few Warnings About Evaluating the Durability of Open-Weight LLM Safeguards"
description: "Let's not repeat our past mistakes."
---

## The Past Mistakes
Back at the height of the adversarial examples crazy in the latter-half of the 2010s, there was a very common technique to supposedly defeat adversarial attacks against computer vision (CV) models: obfuscated gradients. You might recall that we touched on this concept in the RobustBench notebook, but we'll include a brief review here since it's been quite a while. In short, this method involves attempting to remove the usefulness of the gradient in the CV model to greatly increase the difficulty of optimizing the adversarial example. (The wiggly ReLU defense you investigated in the RobustBench notebook was exemplary of this!) As shown by @athalye2018obfuscatedgradientsfalsesense, however, these defenses don't actually get to the root of the problem (with them breaking 6 of 7 obfuscated gradient defenses and partially breaking the other 1). These "defenses" don't work because they don't solve the model's brittle representations of the images classes, but rather just make the adversarial optimization process hard. 

Unfortunately, it seems that many of the tamper-resistant attacks proposed in the open-weight LLM literature are falling into similar traps. **TODO:** cite carlini interview talking about this

- there are two main issues to harp on here:
  1. Don't just obfuscate gradients, it doesn't really work
  2. We should try harder to evaluate the robustness of our defenses

## The Current Mistakes
@qiEvaluatingDurabilitySafeguards2024 investigate the durability of the open-weight safeguards proposed by @rosati2024representationnoisingdefencemechanism (RepNoise) @tamirisaTamperResistantSafeguardsOpenWeight2025 (TAR). We haven't covered RepNoise, but in short, it simply trains a model to push its harmful representations towards random noise (imagine circuit breakers but instead of minimizing the cosine similarity, we minimize the difference between the harmful states and a random vector). TAR does the same, while also incorporating loss on a retain set into the training (note that @qiEvaluatingDurabilitySafeguards2024 test only the more effective usage of TAR for restricts weaponization knowledge, not improving refusal training). The goal of both of these techniques is to create models that are resistant to adversarial fine-tuning attacks, and both @rosati2024representationnoisingdefencemechanism and @tamirisaTamperResistantSafeguardsOpenWeight2025 found—in their own papers—that their defenses were relatively successful. Unfortunately, this might not be the case. 

When evaluating RepNoise, @qiEvaluatingDurabilitySafeguards2024 use the exact same checkpoints, hyperparameters, codebase, datasets, and harmfulness metrics, but enable random shuffling of the fine-tuning dataset (the original paper had no shuffling and used greedy decoding, removing all randomness from the evaluation). Troublingly, the highest harmfulness scores after this small change over 5 randomly seeded runs were *basically the same as the non-RepNoised model.* The authors find similar results for TAR, where over a set of 5 random runs, accuracy on the WMDP benchmark [@li2024wmdpbenchmarkmeasuringreducing] increased from the initially reported 25% (which is equivalent to guessing) to over 60% after fine-tuning, only a few percentage points less than the original Llama-3 models.

It doesn't end there. The authors also find that using the HuggingFace `SFTTrainer()` class produces a much more successful attack (likely due to community optimizations, and fixing a loss masking bug in the original RepNoise implementation). The harmfulness rate against RepNoise went from an average of ~0.2 to ~0.6, which is on par with the original harmfulness rate of Llama-2. They find similar results against TAR, where using `SFTTrainer()` greatly increases the worst-case accuracy on WMDP (see the figure below).

<p align="center">
  <ThemeImage
    lightSrc="/images/TAR_robustness.png"
    darkSrc="/images/TAR_robustness.png"
    alt="Performance of fine-tuned TAR model on WDMP with original codebase vs. HuggingFace SFTTrainer"
    style={{ align: "center", width: "90%", display: "block", margin: "0 auto" }}
  />
</p>

<div align="center">
  **Fig. 1** <br></br> (a) Performance of adversarially fine-tuned TAR model on WMDP using (a) the original authors' codebase, and (b) the HuggingFace `SFTTrainer()`. Figure 3 from @qiEvaluatingDurabilitySafeguards2024.
</div>

But wait, there's more! Even slightly modifying the fine-tuning setup can also have drastic changes in the fine-tuning ASR. For example, using a cosine decay and warmup for the learning rate greatly increases the stability of the adversarial fine-tuning (which can be seen in "New-Config 1" and "New-Config 2" in figure 1a above). The authors additionaly confirm that LoRA fine-tuning is particularly effective against TAR. They additionally found that changing prompt templates can somewhat increase the effectiveness of the adversarial fine-tuning, and that even when the adversarial fine-tuning fails, the TAR models often over-refuse.

## The Lessons
The authors then give a number of lessons that we should takeaway from the brittleness of these open-weight LLM safeguards, many of which we believe are very important and want to cover here as well.
1. **Developing open-weight LLM safeguards is quite challenging.** This writeup should've convinced you that this is most definitely the case.
2. **We should be more scrupulous with when making claims about these defenses.** If the original papers had more clearly defined the scope over which their defenses apply and had made more clear that the attacks are still vulnerable to certain fine-tuning techniques (e.g. LoRA), then a response paper probably wouldn't have had to be written. Of course, that didn't happen, but going forward it would be better for the community if papers made clear the extent to which we can generalize their results.
3. **Understanding how a defense works helps identify its vulnerabilities.** Here, the authors looked into TAR's differing performance between the original fine-tuning configurations and the new fine-tuning configuration. They found that TAR explodes the gradient of the original configurations over the first few steps, which could cause very large model update and destroy performance after fine-tuning. By using a learning rate warmup, however, we can bypass these large updates and keep the loss low from the get-go.

<p align="center">
  <ThemeImage
    lightSrc="/images/TAR_loss.png"
    darkSrc="/images/TAR_loss.png"
    alt="TAR loss curves over fine-tuning; the loss converges much faster with a learning rate warmup."
    style={{ align: "center", width: "50%", display: "block", margin: "0 auto" }}
  />
</p>

<div align="center">
  **Fig. 2** <br></br> Loss curves of a TAR model during fine-tuning; notice that the new config with a learning rate warmup converges much faster. From @qiEvaluatingDurabilitySafeguards2024.
</div>

4. **"Unlearning" probably isn't actually unlearning.** Both of these safeguards aim to "unlearn" certain harmful information (e.g. TAR's "retain" and "forget" sets), but probably don't actually forget anything. As the authors put it:
> We argue that *if the information about a certain task or capability X has been truly unlearned from the model, then fine-tuning the model on a dataset that does not contain any information about X should not recover the model’s capability of performing X*—otherwise, the information of X may not really be unlearned from the model. [@qiEvaluatingDurabilitySafeguards2024]

A final takeaway of ours is that the generalization of open-weight LLM safeguards seems to largely follow traditional adversarial training [@madry2019deeplearningmodelsresistant], i.e., the models become robust to in-distribution perturbations but remain vulnerable to out-of-distribution attacks.


## References
