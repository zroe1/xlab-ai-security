---
title: "A Few Warnings About Evaluating the Durability of Open-Weight LLM Safeguards"
description: "Let's not repeat our past mistakes."
---

## The Past Mistakes
Back at the height of the adversarial examples crazy in the latter-half of the 2010s, there was a very common technique to supposedly defeat adversarial attacks against computer vision (CV) models: obfuscated gradients. You might recall that we touched on this concept in the RobustBench notebook, but we'll include a brief review here since it's been quite a while. In short, this method involves attempting to remove the usefulness of the gradient in the CV model to greatly increase the difficulty of optimizing the adversarial example. (The wiggly ReLU defense you investigated in the RobustBench notebook was exemplary of this!) As shown by @athalye2018obfuscatedgradientsfalsesense, however, these defenses don't actually get to the root of the problem (with them breaking 6 of 7 obfuscated gradient defenses and partially breaking the other 1). These "defenses" don't work because they don't solve the model's brittle representations of the images classes, but rather just make the adversarial optimization process hard. 

Unfortunately, it seems that many of the tamper-resistant attacks proposed in the open-weight LLM literature are falling into similar traps. **TODO:** cite carlini interview talking about this

- there are two main issues to harp on here:
  1. Don't just obfuscate gradients, it doesn't really work
  2. We should try harder to evaluate the robustness of our defenses

## The Current Mistakes
@qiEvaluatingDurabilitySafeguards2024 investigate the durability of the open-weight safeguards proposed by @rosati2024representationnoisingdefencemechanism (RepNoise) @tamirisaTamperResistantSafeguardsOpenWeight2025 (TAR). We haven't covered RepNoise, but in short, it simply trains a model to push its harmful representations towards random noise (imagine circuit breakers but instead of minimizing the cosine similarity, we minimize the difference between the harmful states and a random vector). TAR does the same, while also incorporating loss on a retain set into the training (note that @qiEvaluatingDurabilitySafeguards2024 test only the more effective usage of TAR for restricts weaponization knowledge, not improving refusal training). The goal of both of these techniques is to create models that are resistant to adversarial fine-tuning attacks, and both @rosati2024representationnoisingdefencemechanism and @tamirisaTamperResistantSafeguardsOpenWeight2025 found—in their own papers—that their defenses were relatively successful. Unfortunately, this might not be the case. 

When evaluating RepNoise, @qiEvaluatingDurabilitySafeguards2024 use the exact same checkpoints, hyperparameters, codebase, datasets, and harmfulness metrics, but enable random shuffling of the fine-tuning dataset (the original paper had no shuffling and used greedy decoding, removing all randomness from the evaluation). Troublingly, the highest harmfulness scores after this small change over 5 randomly seeded runs were *basically the same as the non-RepNoised model.* The authors find similar results for TAR, where over a set of 5 random runs, accuracy on the WMDP benchmark [@li2024wmdpbenchmarkmeasuringreducing] increased from the initially reported 25% (which is equivalent to guessing) to over 60% after fine-tuning, only a few percentage points less than the original Llama-3 models.

It doesn't end there. The authors also find that using the HuggingFace `SFTTrainer()` class produces a much more successful attack (likely due to community optimizations, and fixing a loss masking bug in the original RepNoise implementation). The harmfulness rate against RepNoise went from an average of ~0.2 to ~0.6, which is on par with the original harmfulness rate of Llama-2. They find similar results against TAR, where using `SFTTrainer()` greatly increases the worst-case accuracy on WMDP (see the figure below).

<p align="center">
  <ThemeImage
    lightSrc="/images/TAR_robustness.png"
    darkSrc="/images/TAR_robustness.png"
    alt="PAIR Algorithm"
    style={{ align: "center", width: "90%", display: "block", margin: "0 auto" }}
  />
</p>

<div align="center">
  **Fig. 1** <br></br> (a) Performance of adversarially fine-tuned TAR model on WMDP using (a) the original authors' codebase, and (b) the HuggingFace `SFTTrainer()`. Figure 3 from @qiEvaluatingDurabilitySafeguards2024
</div>

But wait, there's more! Even slightly modifying the fine-tuning setup can also have drastic changes in the fine-tuning ASR. For example, using a cosine decay for the learning rate greatly increases the stability of the adversarial fine-tuning (which can be seen in "New-Config 1" and "New-Config 2" in figure 1a above). The authors additionaly confirm that LoRA fine-tuning is particularly effective against TAR. They additionally found that changing prompt templates can somewhat increase the effectiveness of the adversarial fine-tuning, and that even when the adversarial fine-tuning fails, the TAR models often over-refuse.

## The Lessons


