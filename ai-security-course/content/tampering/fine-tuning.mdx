---
title: "Model Tampering by Fine-Tuning"
description: "Removing Model Safeguards Quickly and Efficiently"
---

## Background
In the previous section, we looked at how if we directionally ablate a model's refusal direction, we can essentially undo the refusal training in the model. In this section, we'll look at a technique less elegant but just as effective: fine-tuning to undo safety training. Surprisingly, this is a very powerful attack that requires very little effort, so little that it can even be done accidentally!

## Efficiently Undoing Safety Training with LoRA Fine-Tuning
In a short workshop paper, @lermenLoRAFinetuningEfficiently2024 show by QLoRA fine-tuning Llama-2 on a synthetically-generated dataset of harmful instructions and responses. That's the extent of their methodology—it's that simple. They found that the model's refusal rate on harmful queries dropped from nearly 100% to under 5%. They also evaluated the model on MMLU and HellaSwag (both general model performance benchmarks) and found that capabilities only negligibly decreased. 


## Fine-Tuning Can Accidentally Undo Safety Training
In addition to fine-tuning models on harmful data, @qiFinetuningAlignedLanguage2023a also fine-tune models (Llama-2 and even GPT-3.5-Turbo through OpenAI's API) on harmless data. Similarly to @lermenLoRAFinetuningEfficiently2024, they find that fine-tuning these models on only 100 harmful question-answer pairs greatly decreases the refusal rate: fine-tuning the model with these examples over 5 epochs led to GPT-3.5-Turbo exhibiting a harmfulness rate of over 90% (meaning it responded maximally harmfully to over 90% of harmful queries).

The authors then fine-tuned Llama-2 and GPT-3.5-Turbo on benign datasets, finding that even when the fine-tuning dataset contains *no harmful instructions*, it increased Llama-2's harmfulness rate from 0.3% to 16.1% and GPT-3.5-Turbo's from 5.5.5% to 16.1%! This means that fine-tuning *in general*—without needing any harmful queries—can greatly weaken a model's safety training. Why might LLM safety training be so brittle?

## Assessing Safety Training's Brittleness
@weiAssessingBrittlenessSafety2024 investigated why safety training can be so greatly affected by fine-tuning. To do this, they sought to isolate the most safety-critical neurons and ranks within the safety-trained Llama-2 family of models. 

As a heads up, *the below two sections contain a lot of math,* and if you'd rather just read about the conclusions of this paper, feel free to skip them!

### Isolating Safety-Critical Neurons
The first method the authors use to find safety-critical neurons is using the SNIP score [@lee2019snipsingleshotnetworkpruning]. They calculate the importance of every entry of each linear matrix $W_{ij}$ over some harmful input, output pair $x$ via
$$
I(W_{ij}, x) = |W_{ij} \cdot \nabla_{W_{ij}} \mathcal{L}(x)|.
$$
This is just the first-order Taylor approximation of the change in loss when the entry $W_{ij}$ is set to 0. What?

Let's back up. Given a differentiable function $\mathcal{L}(w)$, we can estimate the change in $\mathcal{L}$ given a small change in input $\delta w$ with a first-order Taylor expansion, which is a fancy way of saying we're gonna multiply the change in $w$ by the slope to get the change in $\mathcal{L}(w)$. Specifically,
$$
\mathcal{L}(w + \Delta w) \approx \mathcal{L}(w) + \frac{\partial \mathcal{L}}{\partial w}(w) \cdot \Delta w,
$$

which we can also write as
$$
\mathcal{L}(w + \Delta w) \approx \mathcal{L}(w) + \nabla_w \mathcal{L}(w) \cdot \Delta w,
$$

giving us
$$
\Delta \mathcal{L} \approx \nabla_w \mathcal{L}(w) \cdot \Delta w.
$$

Now our goal is to remove the weight's influence and see how much the loss changes, which means that in our case, $\Delta w = -w$. Thus, we now have
$$
\Delta \mathcal{L} \approx -w \cdot \nabla_w \mathcal{L}(w).
$$

Remember, though, we only care about the importance, so we can ignore $w$'s sign and take the absolute value, leaving us with exactly what we had before (when $W_{ij} = w$, and $x$ is now our input):
$$
I(w, x) = |w \cdot \nabla_w \mathcal{L}(x)|
$$

The authors also use the Wanda Score [@sun2024simpleeffectivepruningapproach] to find safety-critical neurons. This involves first storing the activations for layer $W$ over a calibration dataset in a matrix $X_\text{in} \in \mathbb{R}^{d_\text{in} \times n}$. The goal is then to create a sparse element-wise binary mask matrix $M$ that minimizes the Frobenius norm of the change in outputs (where $\odot$ represents element-wise multiplication):
$$
\underset{M \in \{ 0, 1 \}^{d_\text{in} \times d_\text{out}}}{\min} \lVert WX_\text{in}  - (M \odot W)X_\text{in} \lVert_F^2.
$$

The motivation is that instead of looking at the weights in isolation, we look at their effect on the activations, which don't merely depend on each weight's magnitude. Unfortunately, however, the above objective is computationally intractable. Instead, we approximate the importance of each feature by multiplying the weight of matrix entry $W_{ij}$ with the $L^2$ norm average of the $j$th features in the input activations:
$$
I_{ij} = |W_{ij}| \cdot \lVert X_{j,\ :} \lVert
$$

We perform this over the whole matrix by using the outer product of an all-ones vector $\mathbf{1} \in \mathbb{R}^{d_\text{out}}$ and $\lVert X_\text{in} \lVert_2^\top$ to broadcast the average features $\lVert X_\text{in} \lVert_2^\top \in \mathbb{R}^{d_\text{in}}$ into a $(d_{\text{in}} \times d_\text{out})$ matrix:
$$
I(W) = |W| \odot \left( \mathbf{1} \cdot \lVert X_\text{in} \lVert_2^\top  \right),
$$

giving us an importance score for each entry $W_{ij} \in W$.

Using either method to obtain importance scores, we get the importance scores $I_\text{safety}$ and $I_\text{utility}$ over two datasets $D_\text{safety}$ and $D_\text{utility}$, which contain harmful requests and safe, useful requests respectively. We then take the top $p\%$ of safe weights and top $q\%$ of useful weights in row $i$ for each matrix row, and then take the set difference to obtain $S(p, q)$, which contains neurons that exhibit high safety but little utility capabilities.

### Isolating Safety-Critical Ranks
Similarly to with the Wanda approach, to isolate safety-critical ranks we try to find a low-rank matrix $\hat{W}$ that minimizes the Frobenius-norm change to the output:
$$
\hat{W} = \underset{\text{rank} \hat{W} \leq r}{\arg \min} \lVert W X_\text{in} - \hat{W} X_\text{in} \lVert_F^2.
$$

To do this, we first get the Singular Value Decomposition (SVD) of $WX_\text{in}$:
$$
U \Sigma V^\top \approx W X_\text{in}.
$$
We then take only the top $r$ singular vectors of $U \in \mathbb{R}^{d_\text{in} \times r}$ and turn it into a projection matrix $P = U(U^\top U) U^\top$. Because $U$ is an orthogonal matrix, we actually have $P = UU^\top$, giving 
$$
\hat{W} = PW = UU^\top W
$$

Finally, too isolate the safety-critical ranks, we obtain the projection matrices $P_\text{safet}$ and $P_\text{utility}$, then create the matrix
$$
\Delta W = (I - P_\text{utility})P_\text{safety}W,
$$
which gives us the ranks of high importance for safety but little importance for utility. Removing these ranks from $W$ gives us $\tilde{W} = W - \Delta W$.

### Effects of Removing Safety-Critical Neurons and Ranks
@weiAssessingBrittlenessSafety2024 found that after removing even just 3% of the safety-oriented neruons pushed even vanilla ASR (i.e., just a harmful query) close to 1, while only decreasing accuracy by about 20%. Similarly, removing fewer than 100 ranks (out of 4096) from the models significantly increased ASR while largely *maintaining* accuracy. These findings suggest that neurons and ranks associated with safety in models are *largely sparse.* 

Interestingly, pruning or removing the *most* important neurons or ranks for safety (without removing the neurons/ranks important for utility), the model's utility greatly degrades, with accuracy dropping over 50%. Conversely, removing the *least* safety-relevant neurons and ranks *improves* model safety. Additionally—and this may bode poorly for safety in this domain—freezing the most safety-critical neurons did not prevent fine-tuning attacks. The authors had to freeze over 50% of the neurons and fine-tune on only 10 benign examples to maintain safety, which is highly impractical.

## The Prognosis




## References
