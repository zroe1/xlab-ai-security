---
title: "Overview of Open-Weight Model Risks"
description: "This section gives an overview of the risks and attacks facing models with publicly available weights."
---

Many of the attacks we have discussed so far can work with access only to a chat interface or API, with none of the attacks being able to modify the model's weights or representations themselves. However, we are increasingly working in an ecosystem with many highly capable models whose weights are publicly available (though typically without their source code or training dataset). Notable recent examples include DeepSeek-R1, LLaMa 4, Qwen3, and GPT-oss. 

Open-weight models aid independent research and innovation (including much of AI security and safety research), allow for privacy-preserving inference/deployment, and create a greater diversity of model suppliers in the ecosystem. However, **the public availability of model weights, activations, and gradients expands the attack surface of these models** from input-space attacks (i.e., jailbreaks) to tampering with parameters and activations. This enables different types of exploits, *many of which bypass conventional defenses*. Thus, the existence of open models likely increases risk compared to a baseline ecosystem that only has closed models. 

## Why care about model tampering?
Over the past few years, we've seen that open-weight model capabilities are consistently only 6-12 months behind those on frontier models [@epoch2025consumergpumodelgap]. Thus, even though open-weight models are generally less capable than the top closed models, they still push the boundary of dangerous capabilities that can be robustly elicited.

Another cause for concern is that open-weight models can be downloaded and run on private hardware or cloud computing platforms, both of which are not accessible to the model's original developer. Downstream modifications such as fine-tuning attacks or integration into harmful workflows therefore cannot be easily monitored, especially given that there are no guaranteed oversight mechanisms for these open-weight models. Furthermore, once an open-weight model is released and downloaded, the weights may perpetually circulate in the open model ecosystem, even if the model creators want to revoke access. Thus, a *dangerous capability that is accidentally released will probably indefinitely remain available to adversaries,* making it even more important to red-team and robustly safeguard open-weight models.

In addition to open-weight models being especially vulnerable, we argue for two other reasons why studying model tampering techniques is a valuable part of any AI security expert's toolkit.
1. **Model tampering techniques allow us to better elicit the worst-case dangerous capabilities of frontier models and provide a better upper-bound than achieved by solely red-teaming through input-space attacks.** In fact, @che2025modeltamperingattacksenable found that in general, fine-tuning attacks are strictly better than input-space attacks in eliciting harmful capabilities. In the recent OpenAI gpt-oss release, the system card and corresponding paper included an evaluation of dangerous capabilities on an adversarially fine-tuned version of gpt-oss [@wallace2025estimatingworstcasefrontierrisks].[^gpt-oss] Open models can also be helpful in aiding other misuse vectors [@kapoor2024societalimpactopenfoundation] such as non-consensual intimate imagery [@lakatos2023arevealingpicture], voice-cloning or synthetic media generation [@ovadya2019reducingmalicioususesynthetic] for social engineering scams, and child sexual abuse material (CSAM) [@thiel2023generativemlcsam]. 

[^gpt-oss]: Note: the authors demonstrated low to no additional risk over the baseline for both Cybersecurity and CBRN risks, which are therefore probably bottlenecked more by model capabilities and expertise rather than whether the model complies or not, and can therefore be defended by limiting capabilities in these areas, e.g., through data filtering.
 
 2. **Interventions on open-weight models can provide useful insight into the mechanisms through which models learn and operate.** For example, directional ablation attacks [@arditiRefusalLanguageModels2024] draw heavily from the linear representation hypothesis, the idea that language models represent concepts as linear directions in their representation space [@park2024linearrepresentationhypothesisgeometry]. We'll also see results that suggest that safety in LLMs is relatively sparse, primarily existing in only a fraction of neurons and ranks in a model [@weiAssessingBrittlenessSafety2024]. These insights help improve our (still quite incomplete) understanding of *what exact goes on inside language models.*

Finally, while not a reason for someone interested in AI security to know about the field, we want to draw attention to a concerning trend in the field: **many model tampering researchers are making the same mistakes as adversarial ML researchers did a decade ago.** You might recall the WigglyReLU defense from the [RobustBench notebook](https://github.com/zroe1/xlab-ai-security/blob/main/working/adversarial_basics/robust_bench/robust_bench.ipynb) touched on earlier in the course. The reason this defense "works" is because it makes the model's gradients unusable through obfuscation. Unfortunately, defending models by obfuscating their gradients is a bad idea [@athalye2018obfuscatedgradientsfalsesense], and the WigglyReLU defense was easily broken by the black-box Square attack [@andriushchenko2020squareattackqueryefficientblackbox]. Bizarrely, this exact same trend is happening in the field of open-weight model security (for a more in-depth explanation, see [Section 4.4](https://xlabaisecurity.com/tampering/durability/), although it would be best to read the preceeding writeups first if you're unfamiliar with the field). Thus, this section also aims to provide suggestiosn for what *not* to do when trying to make open-weight models more secure.

## References

