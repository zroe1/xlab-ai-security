---
title: "Overview of Open-Weight Model Risks"
description: "This section gives an overview of the risks and attacks facing models with publicly available weights."
---

Most of the attacks we have discussed so far can work with access only to a chat interface or API. However, we are working in an ecosystem with many highly capable models whose weights are publicly available (though typically without their source code or training dataset). Notable recent examples include DeepSeek-R1, LLaMa 4, Qwen3, and GPT-oss. Open-weight models aid independent research and innovation (including a lot of AI security and safety research), allow for privacy-preserving inference and deployment, and create a greater diversity of model suppliers in the ecosystem. However, **the public availability of model weights, activations, and gradients expands the attack surface from input-space attacks (e.g., the token/prompt-based jailbreaks we just covered) to direct interventions on a model's parameters and activations, enabling different types of exploits, many of which bypass conventional defenses. Thus, the existence of open models likely increases risk compared to a baseline ecosystem that only has closed models. In cases where they are compared, model tampering techniques such as fine-tuning attacks almost always outperform even the best input-space attacks[@che2025modeltamperingattacksenable]. This trend is likely only strengthened when both are taken in conjunction. 

## Motivation
Current open-weight models are quite close to the frontier of capabilities, and it is not unlikely that this trajectory will continue. **insert reasons** Thus, even though open-weight models are generally less capable than the top closed models, they may still push the boundary of dangerous capabilities that can be robustly elicited.
Another reason for concern is that open-weight models can be downloaded and run on private hardware or cloud computing platforms, both of which are not accessible to the model's original developer. Thus, downstream modifications such as fine-tuning attacks or integration into harmful workflows are nearly undetectable. Importantly, conventional oversight mechanisms, such as content filters present in chat interfaces and APIs, are also unavailable. Furthermore, once an open-weight model is released and downloaded, the weights may perpetually circulate in the open model ecosystem, even if the model creators want to revoke access. Thus, a dangerous capability that is accidentally released will probably indefinitely remain available to adversaries, making it even more important to red-team and robustly safeguard open-weight models.

In addition to open-weight models being especially vulnerable, there are two other independent reasons why studying model tampering techniques is a valuable part of any AI security expert's toolkit.
1. Model tampering techniques allow us to better elicit worst-case dangerous capabilities of frontier models,  [@che2025modeltamperingattacksenable], while red-teaming only through input space jailbreaks might not represent the domain of attacks nor elicit the full extent of dangerous capabilities available to adversaries. In the recent OpenAI gpt-oss release, the system card and corresponding paper included an evaluation of dangerous capabilities on an adversarially fine-tuned version of gpt-oss [@wallace2025estimatingworstcasefrontierrisks]. Note: the authors demonstrated low to no additional risk over the baseline for both Cybersecurity and CBRN risks, which are therefore probably bottlenecked more by model capabilities and expertise rather than whether the model complies or not, and can therefore be defended by limiting capabilities in these areas, e.g., through data filtering. However, open models might be helpful in aiding other misuse vectors [kapoor2024societalimpactopenfoundation] such as non-consensual intimate imagery[@lakatos2023arevealingpicture], voice-cloning or synthetic media generation [@ovadya2019reducingmalicioususesynthetic] for social engineering scams, and child sexual abuse material (CSAM) [@Thiel2023GenerativeMLCSAM]. 
 
 2. Lastly, interventions on open-weight models can provide useful insight into the mechanisms by which models work and learn. In particular, we will explore various techniques from the field of Mechanistic Interpretability. This field seeks to explain human-understandable concepts and properties of the model output by reverse-engineering internal computational pathways and representations inside models, with many researchers hoping that a better understanding of the circuitry of models can help us better align them [@bereska2024mechanisticinterpretabilityaisafety]. 

Below is a description of the content we will explore in the model of tampering section of the course:

## Refusal Direction Removal
This subsection deals with a model tampering technique inspired by a recent paper, Refusal in Language Models Is Mediataed by a Single Direction [@arditiRefusalLanguageModels2024], which exploits access to the model's residual stream. This technique, known as refusal direction removal or "abliteration", finds a direction, or one-dimensional subspace, in the model's activations that encodes the propensity of the model to refuse harmful queries. This direction can then be zeroed out (either through the model's activations or weights) to cause the model to stop refusing. Conversely, it can be added to cause the model to excessively refuse even benign prompts. As this method is conceptually simple and easy to implement, it has become a pretty common practice on HuggingFace for those who want to bypass a model's "censorship" by greatly decreasing its propensity to refuse [@labonne2024uncensor].

## Fine-tuning Attacks
Fine-tuning attacks repurpose a model to a malicious objective while retaining the model's general-purpose competence. By leveraging the model's pretrained knowledge, attackers can cheaply instill dangerous capabilities using small, targeted, datasets, without needing the expertise and resources needed to train a model from scratch. Such attacks #TODO

## Emergent Misalignment
#TODO

## Safeguards
#TODO

## References
