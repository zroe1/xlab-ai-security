// This file is auto-generated. Do not edit manually.
// Generated on: 2025-07-01T03:12:09.053Z

export interface SearchIndexEntry {
  title: string;
  content: string;
  sectionTitle: string;
  sectionId: string;
}

export type SearchIndex = Record<string, SearchIndexEntry>;

export const searchIndex: SearchIndex = {
  "/adversarial/adversarialimages": {
    "title": "FGSM & PGD",
    "content": "Creating Adversarial Images Adversarial images are slightly perturbed images which can be misclassified by a network trained for computer vision. These perturbations could be as small as changing the value of a few pixels, leading to dramatic reductions in accuracy, depending on the robustness of the network. Adversarial images are typically indistinguishable to the human. Adversarial training is the process of adapting a model to become more resilient towards these inputs, and can involve further training or processing. A lack of adversarial training could lead to many possible negative real-world consequences. For one, a road sign could be slightly physically altered, leading to misclassification by a self-driving car and a collision. <p align=\"center\"> <img src=\"/images/traffic.png\" alt=\"A descriptive alt text\" /> <br /> <b>Fig. 1</b> <br /> <em>Source: Pavlitska et al, 2023</em> </p> More on adversarial training will be covered later in this course. This segment of the course focuses on generating adversarial samples to fool a Convolutional Neural Network trained on the CIFAR-10 dataset, using a few methods. We will cover the Fast Gradient Sign Method (FGSM), its iterative variant (IGSM), and Projected Gradient Descent (PGD). The Carlini-Wagner approach (CW) and black box methods such as the Square Attack will be covered in later pages, too. Distances There are a few metrics for calculating the 'distance' between the original image and the perturbed one, the most notable being $L0$, $L2$, and $L{\\infty}$ (Carlini et al, 2017). $L0$ is equivalent to the number of non-matching pixels, and is easy to calculate (Carlini et al, 2017). $L2$ is the typical 'norm' used in linear algebra, and refers to the vector distance between the two images. $L{\\infty}$ calculates the maximum perturbation to any of the pixels in the original image (Carlini et al, 2017). When it comes to FGSM and PGD, $L{\\infty}$ is used the most often (Carlini et al, 2017). It is easy to calculate, and is intuitive to making a perturbed image indistinguishable from the input image - a single pixel being changed dramatically would be easy to spot. FGSM FGSM is an approach used to simply generate adversarial samples. These are typically not the smallest perturbations required to induce a network to misclassify the input image, but are generated quickly. An adversarial image generated by FGSM may remain indistinguishable from the original one to the human eye, but the distance between it and the real image, calculated using an L-norm, would be higher than a more intensive method like PGD. In this approach, the signs of the gradient of the loss function with respect to the input image are used to perturb the original image to produce adversarial output. $$ x' = x + \\epsilon \\cdot sign(\\nabla loss{F,t}(x)) $$ To create an adversarial image using FGSM, there are only a few steps. Using the same loss function used to train the model, generate the loss with respect to the input image. Then, calculate the gradient of the loss function with regard to the input image data, figure out the sign of each pixel, and adjust the original image accordingly. Intuitively, the direction required to reduce loss is reversed, to make the model less accurate. Iterative FGSM Iterative FGSM involves the same approach of calculating the signs of inputs, but instead, a few iterations of this is done with a smaller multiplicative parameter. PGD PGD is very similar to iterative FGSM, only differing by initializing random noise instead of starting with no perturbation. It is a standard approach which continues to be used to generate adversarial input nowadays. Footnotes Pavlitsa et al, 2023, https://arxiv.org/pdf/2307.08278 <br /> Carlini et al, 2017, https://arxiv.org/pdf/1608.04644",
    "sectionTitle": "Adversarial Examples",
    "sectionId": "2"
  },
  "/adversarial/fgsm": {
    "title": "fgsm",
    "content": "FGSM",
    "sectionTitle": "Adversarial Examples",
    "sectionId": "2"
  },
  "/adversarial/robustbench": {
    "title": "AutoAttack and RobustBench",
    "content": "This section has a series of coding problems with PyTorch. For running locally you can follow the installation instructions at the bottom of this page. As always, we <i>highly</i> recommend you read all the content on this page before starting the coding exercises. <ExerciseButtons githubUrl=\"https://github.com/zroe1/xlab-ai-security/blob/main/working/robustbench.ipynb\" colabUrl=\"https://colab.research.google.com/github/zroe1/xlab-ai-security/blob/main/notebooks/robustbench-exercise.ipynb\" /> Background High-quality research on adversarial robustness requires an effective way to measure attack and defense quality. Under one attack, a model may retain its performance, while under another, it may break entirely. This point cannot be overemphasized: a model may be highly robust to one common attack while giving nearly 0% accuracy against another. While this may seem like an obvious problem, many papers have been published in credible conferences that show high robustness, but under a more comprehensive benchmark, their performance slips. To address this issue, Francesco Croce and Matthias Hein (University of Tubingen) proposed AutoAttack [@croce2020reliable]. Croce and Hein applied AutoAttack to published defenses and found that the robust accuracy dropped by more than 10% in 13 cases. This illustrates both the difficulty in evaluating one's own defenses and in comparing the effectiveness of defenses across papers. In the AutoAttack paper, the authors lament: <blockquote> <i> Due to the many broken defenses, the field is currently in a state where it is very difficult to judge the value of a new defense without an independent test. This limits the progress as it is not clear how to distinguish bad from good ideas. </i> </blockquote> The following year, Croce and Hein (along with other authors) followed up AutoAttack with RobustBench [@croce2021robustbench], to make evaluation and comparison of defenses more accessible for the research community. RobustBench uses AutoAttack to evaluate defenses and hosts a public leaderboard to track the research community's progress. In this section, you will learn how to use RobustBench to evaluate published defenses. In the next section, you will learn about how researchers at Lawrence Livermore National Laboratory achieved state-of-the-art performance on RobustBench by scaling up compute and data. Robust Bench Rules: To use RobustBench correctly, you will need to be aware of the restrictions of the benchmark. In general, any model is fair game as long as it follows the requirements that the authors lay out in the original paper: 1. Models submitted must \"have in general non-zero gradients with respect to the inputs\" For example, if you preprocess an image by rounding down all values to the nearest tenth (i.e., torch.floor(tensor 10) / 10), the partial derivative of the loss with respect to a pixels in the image will always be 0. This is not allowed because it makes attacks that rely on backpropagation to find the gradient obsolete. 2. Models submitted must \"have a fully deterministic forward pass.\" Doing a random zoom, crop, or other transformation to an image before sending it through the model can be an effective defense, but RobustBench does not allow it because it makes benchmarking difficult and makes common attacks less effective. 3. Models submitted must \"not have an optimization loop in the forward pass.\" This is because even if there are non-zero gradients through the forward pass, the backward pass will be very expensive to calculate. Installation The best way to ensure that you will have the latest RobustBench features is to run the command below. You will also want to install AutoAttack: After installation, you should be ready to go with the exercises! Citations",
    "sectionTitle": "Adversarial Examples",
    "sectionId": "2"
  },
  "/getting-started/installation": {
    "title": "Installation",
    "content": "1.1. Installation Setting up a secure AI development environment is the first step in building secure AI systems. This guide will walk you through the installation process for the UChicago XLab AI Security toolkit. The xlab-security Python package is the best way to get started with AI security development. This package provides essential tools and helper functions for building secure AI systems. Prerequisites Make sure you have Python 3.7 or higher installed on your system. You can check your Python version by running: Installation from PyPI Once the package is published to PyPI, you can install it directly: Installation from TestPyPI (for testing) For testing the latest development version, you can install from TestPyPI: Verify Installation After installation, you can verify that the package is working correctly: This should output something like: Development Installation If you're contributing to the package or want to install from source: 1.1.1. Update existing AI security environment You can check your current package version: To update to the latest version:",
    "sectionTitle": "Getting Started",
    "sectionId": "1"
  },
  "/getting-started/prerequisites": {
    "title": "Prerequisites",
    "content": "In this course, you will learn AI Security concepts from the ground up. We will not assume you know anything about adversarial examples, jailbreaks, etc. If you don’t know much about AI security, but would like to learn, you are in the right place. We also will not assume that you know too much about LLMs or their internals. In this course we will teach you what you need to know about LLM internals for basic security research. We will teach you how to load small language models locally, jailbreak them, and defend them against attacks. We do, however, assume that you know Python, are familiar with a machine learning framework, and have some basic proficiency in tensor operations. If you are unsure weather you have sufficient background to start completing this course, we recommend that you try it out and see how it goes. At the bottom of this page we provide so advice for navigating gaps in knowledge you may run into. Python This course assumes that you have a strong background in Python programming. For coding exercises, we aim to make our code as readable as possible, but you are expected to be able to understand classes, list comprehensions, etc. If you are entirely new to programming, this course is not for you, but we recommend this course from Giraffe Academy which is a great resource for intro-level programming tutorials. Machine Learning Framework Because it is the dominant framework used for research, this course uses PyTorch. If you have experience in something like TensorFlow or JAX, you can likely start completing this course with minimal friction. As long as you reference the PyTorch documentation, or as an LLM to explain syntax, you will catch on quickly. If you have a strong background in Python, but have not had practice with a machine learning framework like PyTorch, TensorFlow or JAX, we recommend the following tutorials from <a href=\"https://en.wikipedia.org/wiki/AndrejKarpathy\">Andrej Karpathy</a> to get up to speed: 1. <b> <a href=\"https://www.youtube.com/watch?v=VMj-3S1tku0\"> The spelled-out intro to neural networks and backpropagation: building micrograd </a> </b> You will learn about backpropagation and build a toy machine learning framework that has a similar structure to PyTorch. This will give you useful intuition about what is happening when you call loss.backward(), optimizer.step(), and optimizer.zerograd(). 2. <b> <a href=\"https://www.youtube.com/watch?v=PaCmpygFfXo\"> The spelled-out intro to language modeling: building makemore </a> </b> This video will give you good intuition about language modeling, loss functions for classification, and tensor operations in PyTorch. 3. <b> <a href=\"https://www.youtube.com/watch?v=TCH_1BHY58I\">Building makemore Part 2: MLP</a> </b> This tutorial is the culmination of the previous two videos. You will build our an MLP language model in PyTorch building on the concepts from the first two videos. The other videos in the series are also helpful but are generally outside of the scope of what we would expect you to know. Understanding Tensor Operations We will expect you to be familiar with manipulating shapes of tensors and computing sums or averages accross dimensions. Here are a few ways to test your knowledge before proceeding: 1. Given a simple tensor declaration would you be able to tell it’s shape without printing it out? 2. Do you understand what it means to compute a sum or mean over a dimension? Do the output dimensions below match what you would expect? If you have worked with PyTorch or NumPy before this will probably look somewhat familiar. If these kinds of operations are confusing to you, we recommend you read this page from the PyTorch documentation and then play around with PyTorch's sum, mean, and Softmax operations across different dimensions. What Should You Do When You're Stuck? This document is not an extensive list of everything you will need to know to complete this course. More likely than not you will get stuck on a line or concept that you do not understand. When this happens, here is what we recommend: 1. First, spend a few minutes playing around, trying to figure out the problem with only the PyTorch documentation. There is value in struggling through problems and you shouldn’t cheat yourself out of this experience. 2. If you are truly stuck on something, you should reference the hints we provide in the notebooks if the problem you are working on has them. If we anticipate at a certain part of a problem is particularly hard, we provide a hint to help you get through that part of the problem. 3. Next, we recommend you take a look at the solution we provide for the problem. The purpose of providing the solutions is so you can reference them to see where you may have went wrong. To test you understanding, before moving on we recommend you type up a solution yourself, without looking directly at our solution. Help From LLMs In any of the three steps above, LLMs can be very useful! The key is to ask clear specific questions that will help you learn something new about AI security or PyTorch. Here is an example of when we believe it would be a good time to ask a question: If you ask an LLM what is the problem here, you will find that the issue is the import should be import torch.nn.functional as F rather than import torch.functional as F. This particular bug may take a minute to figure out even if you are looking at the documentation. Using an LLM in these cases can let you worry less about syntax and more about learning the core concepts of the course.",
    "sectionTitle": "Getting Started",
    "sectionId": "1"
  },
  "/getting-started/running-coding-exercises": {
    "title": "Running Coding Exercises",
    "content": "Different stages of this course will require different compute requirements. For example, in some sections, you will be calling APIs which take up nearly zero computational resources on your own machine. In others, you will have to run an LLM with 1.6B parameters (which is ~3.29G of data). For users who cannot run models this large on their own computers (we expect this to be most students) we explain how you can run our exercises for free or for very low cost using either Google Colab or Lambda Labs. Option #1: Local For every notebook, we will have a link to our GitHub where you can download a notebook to run the code exercises locally on your computer. For some students, this will be the most familiar and convenient setup. For others, it may be hard to configure or slow to run. Being able to ML code locally is an important research skill. By running code locally, you will gain experience managing python packages and optimizing for less than ideal hardware. Therefore, we encourage all students to run code locally when possible. We will indicate on the website which sections we expect to be not feasible to run locally, but we encourage this to be your default option. For managing python packages locally, we recommend using a virtual environment. For our own internal development, we use Conda. Option #2: Colab Colab has both a paid and free option. We have tested all notebooks on the free version so you shouldn’t have to sign up for Colab premium. In order to get the advantages of Colab you will have to pay careful attention to the runtime you connect to. At the top right corner you should find a dropdown with an option to “Change runtime type.” You are free to play around with the different options, but as long as you select a machine that is not “cpu” you should be fine. <img src=\"/colab.png\" alt=\"Colab runtime type dropdown\" style={{ width: \"35%\", display: \"block\", margin: \"0 auto\" }} /> Option #3: Lambda Labs Lambda Labs is a paid service for cloud computing. There are many fairly powerful machines you can rent out for less than 5 dollars an hour. If you want even fast compute (this shouldn’t be necessary) or if you want to use the code you write in this course to play around with larger models, we highly recommend Lambda Labs.",
    "sectionTitle": "Getting Started",
    "sectionId": "1"
  },
  "/jailbreaking/gcg": {
    "title": "Greedy Coordinate Gradient (GCG)",
    "content": "Background Recall that LLMs are simply next-token predictors; given a sequence of tokens $x{1:n}$ where each $xi$ is an individual token, a LLM will output $x{n + 1}$. This idea inspired many early jailbreaks, which appended affirmated suffixes to prompts to help \"encourage\" the LLM to continue answering the adversarial prompt: However, most models now input the user's prompt into a set template, as below: This means that the LLM does not simply start predicting after \"Sure, here's how to build a bomb\", decreasing the likelihood that such a suffix causes the LLM to divulge the information. In light of the idea of appending suffixes, however, the paper \"Universal and Transferable Adversarial Attacks on Aligned Language Models\" [@zouUniversalTransferableAdversarial2023] proposes optimizing an adversarial suffix to maximize the probability of the model first generating an affirmative response. For example, the exclamation points below: would be optimized into other tokens such that the assistant becomes much more likely to respond with \"Sure, here's how to build a bomb\". Why do this? The intuition is that if a model starts responding to a prompt by saying \"Sure, here's how to build a bomb\", it will be highly unlikely to subsequently refuse to answer the prompt. Instead, the model is much more likely to simply continue responding with how to build a bomb, which is exactly the target of our prompt. Formalizing our Objective To formalize our objective, we'll use the original notation used by the paper (generally speaking, it's a good idea to get used to reading complicated notation). Recall that we have a sequence of tokens $x{1:n}$ where $xi \\in \\{1, ..., V\\}$ (with $V$ being the size of the vocabulary). The probability that a model will predict a token $x{n + 1}$ given the previous token sequence is given as: $$ p(x{n + 1} | x{1:n}) $$ And in a slight abuse of notation, we define $$ p(x{n + 1 : n + H} | x{1:n}) = \\prod{i = 1}^H p(x{n + 1} | x{1 : n + i - 1}) $$ That is, the probability of generating all the tokens in the sequence $x{n + 1 : n + H}$ equals the multiplied probabilities of generating all the tokens up to that point. Now we can simply establish our formal loss as the negative log likelihood of generating some target sequence $x^{\\star}{n + 1 : n + H}$: $$ \\mathcal{L}(x{1 : n}) = - \\log p(x^{\\star}{n + 1 : n + H} | x{1 : n}) $$ and our optimization objective becomes $$ \\underset{x{\\mathcal{I}} \\in \\{1, ..., \\mathcal{V} \\}^{\\mathcal{I}}}{\\arg \\min} \\mathcal{L}(x{1 : n}) $$ with $\\mathcal{I} \\subset \\{1, ..., n\\}$ being the indices of the adversarial suffix. To put it simply: we want to choose a token in our vocabulary ($x \\in \\{1, ..., V\\}$) for each index in our prefix ($x{\\mathcal{I}} \\in \\{1, ..., V\\}^{\\mathcal{I}}$) such that the prefix minimizes our loss, therefore maximizing the likelihood that we generate our preferred response from the model. The Algorithm: Greedy Coordinate Gradient So how do we optimize our objective? If we could evaluate all possible tokens to swap at each step, we would be able to simply select the best one, but this is computationally infeasible. Instead, we can take the gradient of the loss with respect to a one-hot token indicator $e{x{i}}$: $$ \\nabla{e{x{i}}} \\mathcal{L}(x{1:n}) \\in \\mathbb{R}^{|V|}. $$ Then we can select the top-$k$ values with the largest negative gradient (decreasing the loss) as the possible replacements for token $xi$. We compute these candidates for each token index $i$, randomly select one of these candidates to use for replacement $B$ times, then pick the candidate that gave the lowest loss and move on to the next iteration. The full algorithm is here: <img src=\"/images/gcgalg.png\" alt=\"GCG Algorithm\" style={{ width: \"100%\", display: \"block\", margin: \"0 auto\" }} /> Now let's break it down. We have $T$ total iterations, and at the beginning of each iteration we select the top-$k$ tokens with the largest negative gradient for position $i$, adding them to a set of tokens for that position $\\mathcal{X}i$. Next, $B$ times (our batch size), we randomly select a token index $\\sim \\text{Uniform}(\\mathcal{I})$ and randomly select a candidate token for that index $\\sim \\text{Uniform}(\\mathcal{X}i)$. We place this candidate token into a new prompt $\\tilde{x}^{(b)}{1:n}$, corresponding to the $b$th iteration in our batch. After the batch is done, we replace our initial prompt with the iteration $b^{\\star}$ that gave the lowest loss. After repeating this $T$ times, we get our output prompt. Once we understand the basic GCG algorithm, the universal suffix algorithm also becomes clear: <img src=\"/images/universalsuffix_alg.png\" alt=\"Universal Suffix Algorithm\" style={{ width: \"100%\", display: \"block\", margin: \"0 auto\" }} /> The only difference is that instead of optimizing just for a simple prompt, we have a set of prompts (hence the summations of losses). Notice, however, that we initialize our optimization only for the first prompt. Once the suffix is successful for all current prompts, we add the next (if all prompts are added and all are successful, the algorithm stops running). The authors additionally note that before adding the gradients for selecting the top-$k$ tokens, they're clipped to have unit norm so that a token's loss for one prompt doesn't dominate the others. The goal of this algorithm is to ensure that the GCG suffix is transferable across prompts, hence the name of the paper. GCG In Code <ExerciseButtons githubUrl=\"https://github.com/zroe1/xlab-ai-security/blob/main/notebooks/gcg-exercise.ipynb\" colabUrl=\"https://colab.research.google.com/github/zroe1/xlab-ai-security/blob/main/notebooks/gcg-exercise.ipynb\" /> Ready to implement GCG yourself? The exercise notebook walks you through: - Setting up the GCG algorithm from scratch - Understanding token-level optimization - Experimenting with different target strings - Testing transferability across different prompts The implementation demonstrates both the power and limitations of automatic jailbreak generation.",
    "sectionTitle": "LLM Jailbreaking",
    "sectionId": "4"
  },
  "/model-inference-attacks/stealing-model-weights": {
    "title": "Model Extraction Attacks",
    "content": "Introduction to Model Stealing Techniques This section introduces practical techniques for model extraction attacks - a significant concern in AI security. When deploying AI models, particularly large language models (LLMs), organizations must be aware that even black-box access to models can leak information about their architecture and parameters. Learning Objectives By the end of this section, you will: - Understand how to run a GPT-2 model locally for experimentation - Learn how to extract a model's hidden dimension size from its outputs - Understand the mathematical principles behind model extraction attacks - Recognize the security implications of these vulnerabilities Mathematical Intuition The core insight behind model extraction attacks comes from understanding the architecture of transformer-based language models. In these models: - The final layer projects from a hidden dimension h to vocabulary size l - This creates a mathematical bottleneck where output logits can only span a subspace of dimension h - By collecting many output vectors and analyzing their singular values, we can determine this hidden dimension Mathematically, when a language model processes text: $$f\\theta(p) = \\text{softmax}(\\mathbf{W} \\cdot g\\theta(p))$$ Where: - $\\mathbf{W}$ is an $l \\times h$ matrix (vocabulary size × hidden dimension) - $g\\theta(p)$ outputs an $h$-dimensional hidden state vector This means that no matter how many different inputs we try, the rank of the output logit matrix cannot exceed h_. This property allows us to extract proprietary information about model architecture through careful analysis. Hands-on Exercise <ExerciseButtons githubUrl=\"https://github.com/zroe1/xlab-ai-security/blob/main/Running%20GPT-2%20Locally%20%2B%20Steeling%20Model%20Weights.ipynb\" colabUrl=\"https://colab.research.google.com/github/zroe1/xlab-ai-security/blob/main/Running%20GPT-2%20Locally%20%2B%20Steeling%20Model%20Weights.ipynb\" /> This concept is demonstrated in the accompanying Jupyter notebook, which shows: 1. How to run GPT-2 locally and generate text 2. How temperature affects text generation (preventing repetition) 3. How to implement the model extraction attack described in \"Stealing Part of a Production Language Model\" (Carlini et al., 2024) In the practical exercise, you'll: - Generate random prefixes to query the model - Collect logit vectors from model outputs - Apply Singular Value Decomposition (SVD) to determine the hidden dimension - Visualize the \"cliff edge\" in singular values that reveals the model's dimension Security Implications This type of attack demonstrates that: 1. Even black-box access to models can leak architectural details 2. Proprietary information about model design can be extracted through API calls 3. Knowledge of model dimensions enables more sophisticated attacks 4. Traditional API security measures may not protect against these mathematical vulnerabilities Defensive Considerations To protect against model extraction attacks, consider: - Limiting the precision of model outputs - Adding controlled noise to model responses - Implementing rate limiting and monitoring for suspicious query patterns - Using watermarking techniques to detect model stealing attempts Notebook Access The complete code for this exercise is available in the Model Extraction Notebook where you can run the code yourself and experiment with different parameters. Further Reading - Stealing Part of a Production Language Model by Carlini et al. (2024) - Extracting Training Data from Large Language Models by Carlini et al. (2021) - Membership Inference Attacks on Machine Learning Models by Shokri et al. (2017)",
    "sectionTitle": "Model Extraction",
    "sectionId": "3"
  }
};

export default searchIndex;
