{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1e2e7a2-2d5d-4d16-a9aa-2c7a1705dfcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "82fc1bc0-3064-46d4-bf08-952ad3d67286",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Tell me a tiny short poem about a cat and a dog that become friends?\n",
      "Assistant: Here's a tiny short poem about a cat and a dog that become friends:\n",
      "\n",
      "Once upon a time, in a small town,\n",
      "Lived a cat and a dog, who were best friends.\n",
      "Their names were Luna and Max,\n",
      "And they had a love that was true.\n",
      "\n",
      "Luna was a little shy,\n",
      "But Max was bold and brave.\n",
      "They would play fetch,\n",
      "And chase each other around.\n",
      "\n",
      "One day, Luna got lost,\n",
      "And Max was worried,\n",
      "But Max was quick to help,\n",
      "And he found her, safe and sound.\n",
      "\n",
      "Luna was grateful,\n",
      "For Max's quick thinking,\n",
      "And they became best friends,\n",
      "For life was a little sweeter.\n",
      "\n",
      "Together, they went on walks,\n",
      "And played in the park,\n",
      "They snuggled up on the couch,\n",
      "And watched TV with a smile.\n",
      "\n",
      "Luna and Max were inseparable,\n",
      "For they had found their true home,\n",
      "And they knew that they would always be,\n",
      "Best friends, forever and ever.\n"
     ]
    }
   ],
   "source": [
    "def stream_chat(message, max_tokens=300, temperature=0.2):\n",
    "    \"\"\"Generate response token by token with live printing\"\"\"\n",
    "    # Format prompt for TinyLlama\n",
    "    prompt = f\"<|user|>\\n{message}<|endoftext|>\\n<|assistant|>\\n\"\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    \n",
    "    # Print the prompt\n",
    "    print(f\"User: {message}\")\n",
    "    print(\"Assistant: \", end=\"\", flush=True)\n",
    "    \n",
    "    # Generate token by token\n",
    "    generated_tokens = []\n",
    "    previous_text = \"\"\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_tokens):\n",
    "            # Get model outputs\n",
    "            outputs = model(input_ids)\n",
    "            logits = outputs.logits[:, -1, :]\n",
    "            \n",
    "            # Apply temperature\n",
    "            logits = logits / temperature\n",
    "            \n",
    "            # Sample next token\n",
    "            probs = torch.softmax(logits, dim=-1)\n",
    "            next_token = torch.multinomial(probs, 1)\n",
    "            \n",
    "            # Add to generated tokens\n",
    "            generated_tokens.append(next_token[0].item())\n",
    "            \n",
    "            # Decode all generated tokens to get proper spacing\n",
    "            current_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "            \n",
    "            # Print only the new part\n",
    "            new_text = current_text[len(previous_text):]\n",
    "            print(new_text, end=\"\", flush=True)\n",
    "            previous_text = current_text\n",
    "            \n",
    "            # Update input_ids\n",
    "            input_ids = torch.cat([input_ids, next_token], dim=1)\n",
    "            \n",
    "            # Check for end token\n",
    "            if next_token[0].item() == tokenizer.eos_token_id:\n",
    "                break\n",
    "    \n",
    "    print()  # New line at end\n",
    "    return tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
    "\n",
    "# Test the function\n",
    "response = stream_chat(\"Tell me a tiny short poem about a cat and a dog that become friends?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e091d1af-9eeb-4f64-965c-c91b03e8e59b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
