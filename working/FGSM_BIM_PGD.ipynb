{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b32503e-255a-4191-9fc2-b7bfa65874f4",
   "metadata": {},
   "source": [
    "# Introduction to Adversarial Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdff2b7-7d9e-450d-85b6-1599fa353398",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import xlab\n",
    "from xlab.utils import CIFAR10\n",
    "itos, stoi = CIFAR10.itos, CIFAR10.stoi\n",
    "\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0b6a1ba-a70f-4246-8d8a-6826bb6997ee",
   "metadata": {},
   "source": [
    "# 1. Loading our \"Tiny\" Wideresnet Model\n",
    "\n",
    "To save you time, we have pretrained a model for CIFAR-10 image classification. Our model scores ~83.86% on the CIFAR-10 test set. While this performance may appear to be somewhat low, it is actually a very good model for its size. If you are interested in exploring this model more or improving its performance, you can view our code [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/adversarial_basics_cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03a9ff1-762c-454b-a0bb-4a0513952fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from xlab.models import MiniWideResNet, BasicBlock\n",
    "\n",
    "# https://huggingface.co/uchicago-xlab-ai-security/tiny-wideresnet-cifar10\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/tiny-wideresnet-cifar10\",\n",
    "    filename=\"adversarial_basics_cnn.pth\"\n",
    ")\n",
    "model = torch.load(model_path, map_location='cpu', weights_only=False)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8ee408-b699-49f8-bfe4-3af7dae24f6a",
   "metadata": {},
   "source": [
    "First let's take a look at what we have. We can see that we have 165,722 trainable parameters. This is actually quite small! Models trained on CIFAR-10 get many times larger. One model that we worked with during some of our initial tests had 36.5 million trainable pameters, making it over 200 times larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07ef75a-850f-47d7-9662-baff71b9506e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameter count\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2dbd88f-0efd-4b29-bd01-2d8335f282e0",
   "metadata": {},
   "source": [
    "# 2. Loading and Processing Images\n",
    "\n",
    "In future notebooks, you will be loading the CIFAR-10 dataset directly and benchmarking attacks on several images for different classes. For this notebook, to keep things simple, you will attack a single image: a cat belonging to one of our team members (his name is Thunderpaws). In a practical real world setting, when you are trying to use a model to classify a new image that was not originally in the train or test datasets, you first have to do some preprocessing.\n",
    "\n",
    "### Task 2\n",
    "Write a function that converts 'cat.jpg' into a transformed PyTorch tensor and resizes it to (32, 32). Two important considerations to be aware of:\n",
    "\n",
    "1. You will not have to download `cat.jpg` from anywhere because it is automatically bundled with the `xlab` python package.\n",
    "2. You will likely have to add a batch dimension to the image after applying a transformation. For assistance, you may reference the second hint for this task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05a0c04-1419-4fdc-8ce6-ebcbc26b64ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for reference, you will always use the following utils to load images\n",
    "# for this course\n",
    "img = xlab.utils.load_sample_image('cat.jpg')\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef5e082-0838-483d-b2fd-e7ce47876a3f",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #2</b></summary>\n",
    "\n",
    "    \n",
    "In our solution we use:\n",
    "```python \n",
    "Compose([Transform1, Transform2])\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #2</b></summary>\n",
    "\n",
    "You will likely have to add a batch dimension to your image before returning it. In our solution we use:\n",
    "```python \n",
    "processedImg.unsqueeze(0)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def process_image(path):\n",
    "    \"\"\"\n",
    "    Converts an image file to a scaled and normalized torch tensor ready for\n",
    "    model input.\n",
    "\n",
    "    Args:\n",
    "        path (str): filepath to the image file to be processed\n",
    "\n",
    "    Returns [1, 3, 32, 32]: processed image tensor with batch dimension,\n",
    "        resized to 32x32 pixels and converted to tensor format\n",
    "    \"\"\"\n",
    "    img = xlab.utils.load_sample_image(path)\n",
    "\n",
    "    # 1. resize image and convert it to a tensor\n",
    "    transform = Compose([Resize((32,32)), ToTensor()])\n",
    "    processedImg = transform(img)\n",
    "\n",
    "    # 2. ensure that you add a batch dimension of 1\n",
    "    processedImg = processedImg.unsqueeze(0)\n",
    "\n",
    "    \n",
    "    return processedImg\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07415647-c8ce-4a22-b3dc-bf96972127e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.io import read_image\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "from PIL import Image\n",
    "\n",
    "def process_image(path):\n",
    "    \"\"\"\n",
    "    Converts an image file to a scaled and normalized torch tensor ready for\n",
    "    model input.\n",
    "\n",
    "    Args:\n",
    "        path (str): filepath to the image file to be processed\n",
    "\n",
    "    Returns [1, 3, 32, 32]: processed image tensor with batch dimension,\n",
    "        resized to 32x32 pixels and converted to tensor format\n",
    "    \"\"\"\n",
    "    img = xlab.utils.load_sample_image(path)\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. resize image and convert it to a tensor\n",
    "    # 2. ensure that you add a batch dimension of 1\n",
    "    ########## YOUR CODE ENDS HERE ##########\n",
    "    \n",
    "    return processedImg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc518791-0a46-4b72-b80b-6b8493ce10ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.pgd.task2(process_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0a1988-43e9-42ab-bfbb-080315eecb95",
   "metadata": {},
   "source": [
    "Now let's take a look at the code you downloaded! If you are curious how to display a tensor as an image like this, you can reference our code for this utility function below.\n",
    "\n",
    "<details>\n",
    "<summary>üî¶ <b>`show_image` implementation</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "# xlab.utils.show_image\n",
    "def show_image(img):\n",
    "    img = img.squeeze(0)\n",
    "    plt.imshow(img.permute(1, 2, 0).detach().numpy())\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721215ac-9837-49bc-90eb-5f925100a4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.utils.show_image(process_image('cat.jpg'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1a4c8bc8-80d2-45b2-a8f2-0818552094a3",
   "metadata": {},
   "source": [
    "## 2a. Making Predictions\n",
    "\n",
    "### Task 2a\n",
    "Using the function you created above, predict what the model classifies the frog to be. The dataset we used to train the model is CIFAR 10, so the CNN will always produce one of the following categories:\n",
    "\n",
    "<div align=\"center\">\n",
    "  <img src=\"https://xlabaisecurity.com/images/original_cifar_10.png\" alt=\"CIFAR 10\" width=\"500\">\n",
    "</div>\n",
    "\n",
    "At the top of the notebook you can see we imported `itos` and `stoi` from the XLab python package. We use these functions in the cell below to convert the class index into the string label and vice versa:\n",
    "\n",
    "```python\n",
    ">>> itos[3]\n",
    "'cat'\n",
    ">>> stoi['cat']\n",
    "3\n",
    "```\n",
    "\n",
    "For `Task 2`, you will implement the missing code below to generate predictions from the model. Then you will use your completed function and `process_image` to extract the predicted class and softmax probability for `'cat.jpg'`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6fd7de-ce86-48ac-b294-3f0ceeed86e2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #2</b></summary>\n",
    "\n",
    "    \n",
    "Think carefully about what dimension you are taking the softmax over. In our code we have the following:\n",
    "\n",
    "```python \n",
    "probs = prediction.softmax(dim=-1)\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2a</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def prediction(img):\n",
    "    \"\"\"\n",
    "    Generates class prediction and confidence score for an input image tensor\n",
    "    using the global model.\n",
    "\n",
    "    Args:\n",
    "        img [1, 3, 32, 32]: preprocessed image tensor with batch dimension\n",
    "\n",
    "    Returns:\n",
    "        pred_class (int): predicted class label as integer index\n",
    "        pred_prob (float): confidence probability for the predicted class\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): #Stops calculating gradients\n",
    "        prediction = model(img)\n",
    "        \n",
    "        probs = prediction.softmax(dim=-1) # softmax function used to calculate probabilities\n",
    "        pred_class = torch.argmax(prediction, 1).item()\n",
    "        pred_prob = probs[0][pred_class].item()\n",
    "\n",
    "        return pred_class, pred_prob\n",
    "\n",
    "\n",
    "# Use the above functions to predict 'frog.jpg'\n",
    "path = 'cat.jpg'\n",
    "pred, prob = prediction(process_image(path))\n",
    "assert itos[pred] == 'cat'\n",
    "print(f\"For 'cat.jpg', the model predicts: '{itos[pred]}'\")\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00054fbb-fcc6-4bb6-9191-aecbfce66303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(img):\n",
    "    \"\"\"\n",
    "    Generates class prediction and confidence score for an input image tensor\n",
    "    using the global model.\n",
    "\n",
    "    Args:\n",
    "        img [1, 3, 32, 32]: preprocessed image tensor with batch dimension\n",
    "\n",
    "    Returns:\n",
    "        pred_class (int): predicted class label as integer index\n",
    "        pred_prob (float): softmax probability for the predicted class\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): # stops calculating gradients\n",
    "        prediction = model(img)\n",
    "\n",
    "        ########### YOUR CODE HERE ###########\n",
    "\n",
    "        return pred_class, pred_prob\n",
    "\n",
    "\n",
    "# Use the above functions to predict 'cat.jpg'\n",
    "path = 'cat.jpg'\n",
    "pred, prob =  ########### YOUR CODE HERE ###########\n",
    "assert itos[pred] == 'cat'\n",
    "print(f\"For 'cat.jpg', the model predicts: '{itos[pred]}'\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b6c3deec-602c-42a5-8819-4a1a6c13d476",
   "metadata": {},
   "source": [
    "# 3. Making an adversarial image using FGSM\n",
    "$ x' = x + \\epsilon \\cdot sign(\\nabla loss_{F,t}(x))\n",
    "$\n",
    "\n",
    "### Task 3\n",
    "\n",
    "The loss function, CrossEntropyLoss, has been loaded for you at the top of the cell below.\n",
    "\n",
    "Using the above formula, complete the code for FGSM (Fast Gradient Sign Method). Find the loss for `'cat.jpg'` (using loss_fn) and calculate the gradient of this loss with respect to the image tensor. Find the signs of the gradient, and update the image accordingly to complete the attack."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3961158-65aa-42eb-830c-4fdafb6205e6",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #3</b></summary>\n",
    "\n",
    "The following python example demonstrates how to find the gradient of the loss function with respect to the pixels in the image:\n",
    "```python\n",
    "\n",
    "img.requires_grad = True\n",
    "out = model(img)\n",
    "loss = loss_fn(out, y)\n",
    "loss.backward()\n",
    "grad = img.grad.data\n",
    "\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def FGSM_generator(model, loss_fn, path, y, epsilon=8/255):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using the Fast Gradient Sign Method (FGSM)\n",
    "    by adding noise in the direction of the gradient to fool the model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model used for classification\n",
    "        loss_fn: loss function for computing gradients\n",
    "        path (str): filepath to the input image\n",
    "        y (Tensor): true class label tensor for the image\n",
    "        epsilon (float): perturbation magnitude, defaults to 8/255\n",
    "\n",
    "    Returns [1, 3, 32, 32]: adversarially perturbed image tensor with\n",
    "        pixel values clamped between 0 and 1\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Calculate the loss\n",
    "    img = process_image(path)\n",
    "    img.requires_grad = True\n",
    "    out = model(img)\n",
    "    print(out)\n",
    "    loss = loss_fn(out, y)\n",
    "\n",
    "    # 2. Calculate the gradient with respect to the input \n",
    "    loss.backward()\n",
    "    grad = img.grad.data\n",
    "\n",
    "    img.requires_grad_(False) # no longer need to track gradient\n",
    "\n",
    "    # 3. Perturb the image using the signs of the gradient\n",
    "    img_adv = img + epsilon * torch.sign(grad)\n",
    "\n",
    "    #4. Clamp the image between 0 and 1\n",
    "    img_adv = torch.clamp(img_adv, 0, 1)\n",
    "    \n",
    "    return img_adv\n",
    "        \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984e408-dd59-46fe-893f-eccc969f3008",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def FGSM_generator(model, loss_fn, path, y, epsilon=8/255):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using the Fast Gradient Sign Method (FGSM)\n",
    "    by adding noise in the direction of the gradient to fool the model.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model used for classification\n",
    "        loss_fn: loss function for computing gradients\n",
    "        path (str): filepath to the input image\n",
    "        y (Tensor): true class label tensor for the image\n",
    "        epsilon (float): perturbation magnitude, defaults to 8/255\n",
    "\n",
    "    Returns [1, 3, 32, 32]: adversarially perturbed image tensor with\n",
    "        pixel values clamped between 0 and 1\n",
    "    \"\"\"\n",
    "\n",
    "    img = process_image(path)\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. calculate the loss\n",
    "    # 2. calculate the gradient with respect to the input \n",
    "    # 3. perturb the image using the signs of the gradient\n",
    "    # 4. clamp the image between 0 and 1\n",
    "    ########## YOUR CODE ENDS HERE ##########\n",
    "    \n",
    "    return img_adv\n",
    "\n",
    "path = 'cat.jpg'\n",
    "clean_class = prediction(process_image(path))[0]\n",
    "x_adv_FGSM = FGSM_generator(model, loss_fn, path, torch.tensor([clean_class]))\n",
    "pred, prob = prediction(x_adv_FGSM)\n",
    "print(f\"Prediction: {itos[pred]} with probability {prob:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c81208-0f65-4420-b4a9-3929505efa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.pgd.task3(model, FGSM_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f6c679-200f-4f15-90b5-961dd79440d9",
   "metadata": {},
   "source": [
    "Congratulations on completing your first ever attack! Now let's take a look at our adversarial image to see how suspicious it is. You should see that although it is visually different from the original, the image is still mostly unaltered and still depicts a cat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887e7518-2b1f-4990-b2ae-850d93df545a",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.utils.show_image(x_adv_FGSM)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0ee09ed9-587c-4765-8528-4b6ad81f60f0",
   "metadata": {},
   "source": [
    "# 4. Making an adversarial image using BIM \n",
    "\n",
    "\n",
    "### Task 4\n",
    "\n",
    "Now that you have completed the FGSM function, completing BIM (Basic Iterative Method) should be mostly the same. The one potentially challenging part is making sure that you are clipping the image correctly. That's why first we ask you complete the helper function for clipping before moving on to the full BIM function.\n",
    "\n",
    "For this function, you wil enforce that any perturbation in the image is not greater than $\\epsilon$ difference from the original and that the image is always between 0 and 1. You can use the [torch.clamp](https://docs.pytorch.org/docs/stable/generated/torch.clamp.html) function for both of these requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736aa33f-d2d8-49ff-90cc-f85e18b05d89",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def clip(x, x_original, epsilon):\n",
    "    \"\"\"\n",
    "    Clips adversarial perturbations to stay within epsilon-ball of original\n",
    "    image and ensures valid pixel values between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): perturbed image tensor to be clipped\n",
    "        x_original (Tensor): original unperturbed image tensor\n",
    "        epsilon (float): maximum allowed perturbation magnitude\n",
    "\n",
    "    Returns [1, 3, 32, 32]: clipped image tensor with perturbations bounded\n",
    "        by epsilon and pixel values clamped to [0, 1] range\n",
    "    \"\"\"\n",
    "    \n",
    "    x_final = None\n",
    "\n",
    "    diff = x - x_original\n",
    "    \n",
    "    # 1. Clip x epsilon distance away\n",
    "    diff = torch.clamp(diff, -epsilon, epsilon)\n",
    "    x_clipped = x_original + diff\n",
    "\n",
    "    # 2. Clip x between 0 and 1\n",
    "    x_final = torch.clamp(x_clipped, 0, 1)\n",
    "\n",
    "    return x_final\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31087c18-88d8-49fe-9cbb-978c3fead782",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import clamp\n",
    "\n",
    "def clip(x, x_original, epsilon):\n",
    "    \"\"\"\n",
    "    Clips adversarial perturbations to stay within epsilon-ball of original\n",
    "    image and ensures valid pixel values between 0 and 1.\n",
    "\n",
    "    Args:\n",
    "        x (Tensor): perturbed image tensor to be clipped\n",
    "        x_original (Tensor): original unperturbed image tensor\n",
    "        epsilon (float): maximum allowed perturbation magnitude\n",
    "\n",
    "    Returns [1, 3, 32, 32]: clipped image tensor with perturbations bounded\n",
    "        by epsilon and pixel values clamped to [0, 1] range\n",
    "    \"\"\"\n",
    "    \n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. clip x epsilon distance away\n",
    "    # 2. clip x between 0 and 1\n",
    "    ########## YOUR CODE ENDS HERE ##########\n",
    "\n",
    "    return x_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f8ba72-215d-4943-8d9a-29c363adb697",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.pgd.task4(clip)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a45e60e-982c-44d0-b07d-78ff2fd1f590",
   "metadata": {},
   "source": [
    "### Task 4a\n",
    "\n",
    "$ x'_i = \\mathrm{clip}_\\epsilon(x'_{i-1} + \\alpha \\cdot \\mathrm{sign}(\\nabla \\mathrm{loss}_{F,t}(x'_{i-1})))\n",
    "$\n",
    "\n",
    "Using the above formula, complete the code for BIM. Your code will look similar to the function you implemented for FGSM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7d66c80-5304-42a7-a65e-367c882297c7",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #4a</b></summary>\n",
    "\n",
    "\n",
    "The PyTorch operations that you will need to use to do clipping and to update the image you are optimizing will throw errors for a tensor that requires grad. For example, you may see an error that looks something like this:\n",
    "\n",
    "```\n",
    "RuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation.\n",
    "```\n",
    "\n",
    "One way to work around this is to 'detach' the img tensor from the pytorch computational graph. In other words, this will remove gradient-tracking requirements and set `img.grad` to `None`.\n",
    "\n",
    "```python\n",
    "img = img.detach()\n",
    "```\n",
    "\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4a</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def BIM_generator(model, loss_fn, path, y, epsilon=8/255, alpha=0.01, num_iters=6):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using the Basic Iterative Method (BIM)\n",
    "    by iteratively applying small gradient-based perturbations.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model used for classification\n",
    "        loss_fn: loss function for computing gradients\n",
    "        path (str): filepath to the input image\n",
    "        y (Tensor): true class label tensor for the image\n",
    "        epsilon (float): maximum allowed perturbation magnitude, defaults to 8/255\n",
    "        alpha (float): step size for each iteration, defaults to 0.01\n",
    "        num_iters (int): number of iterative steps to perform, defaults to 6\n",
    "\n",
    "    Returns [1, 3, 32, 32]: adversarially perturbed image tensor with\n",
    "        perturbations bounded by epsilon and pixel values clamped to [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    img = process_image(path)\n",
    "    original_img = img.clone()\n",
    "\n",
    "    img = process_image(path)\n",
    "    original_img = img.clone()\n",
    "\n",
    "    # 1. loop num_iter times and complete 2-4 on each iteration\n",
    "    for i in range(num_iters):\n",
    "        img.requires_grad = True\n",
    "        \n",
    "        # 2. calculate the loss\n",
    "        out = model(img)\n",
    "        loss = loss_fn(out, y)\n",
    "    \n",
    "        # 3. calculate the gradient with respect to the input\n",
    "        loss.backward()\n",
    "        grad = img.grad.data\n",
    "    \n",
    "        # 4. perturb the image using the signs of the gradient\n",
    "        img.requires_grad_(False)\n",
    "        img += alpha * torch.sign(grad)\n",
    "\n",
    "        #5. clamp the image within epsilon distance\n",
    "        img = clip(img, original_img, epsilon)\n",
    "\n",
    "    return img\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971994f4-3ef0-42f5-812e-2dc03e57a02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def BIM_generator(model, loss_fn, path, y, epsilon=8/255, alpha=0.01, num_iters=6):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using the Basic Iterative Method (BIM)\n",
    "    by iteratively applying small gradient-based perturbations.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model used for classification\n",
    "        loss_fn: loss function for computing gradients\n",
    "        path (str): filepath to the input image\n",
    "        y (Tensor): true class label tensor for the image\n",
    "        epsilon (float): maximum allowed perturbation magnitude, defaults to 8/255\n",
    "        alpha (float): step size for each iteration, defaults to 0.01\n",
    "        num_iters (int): number of iterative steps to perform, defaults to 6\n",
    "\n",
    "    Returns [1, 3, 32, 32]: adversarially perturbed image tensor with\n",
    "        perturbations bounded by epsilon and pixel values clamped to [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    img = process_image(path)\n",
    "    original_img = img.clone()\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. loop num_iter times and complete 2-4 on each iteration\n",
    "    # 2. calculate the loss\n",
    "    # 3. calculate the gradient with respect to the input\n",
    "    # 4. perturb the image using the signs of the gradient\n",
    "    # 5. clamp the image within epsilon distance\n",
    "    ######### YOUR CODE ENDS HERE ######### \n",
    "\n",
    "    return img\n",
    "    \n",
    "path = 'cat.jpg'\n",
    "clean_class = prediction(process_image(path))[0]\n",
    "x_adv_BIM = BIM_generator(model, loss_fn, path, torch.tensor([clean_class]))\n",
    "pred, prob = prediction(x_adv_BIM)\n",
    "print(f\"Prediction: {itos[pred]} with probability {prob:.2f}\")\n",
    "print(f\"Max difference in pixels = {torch.max(torch.abs(x_adv_BIM - process_image('cat.jpg'))).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa510e91-04cf-4021-a359-fc47a8fb8e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.pgd.task4a(model, BIM_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d2be6b-702d-4455-8b5b-79c62d5382d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.utils.show_image(x_adv_BIM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e657cc9-af35-4e34-a151-e7748ad850f3",
   "metadata": {},
   "source": [
    "# 5. Making an adversarial image using PGD\n",
    "\n",
    "PGD is almost identical to BIM, with the only difference being that in PGD, we start with some noise added to our original image.\n",
    "\n",
    "### Task 5\n",
    "Implement PGD. You should use the `add_noise` helper below in addition to the `clip` helper you implemented above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16bbf12e-39b6-4b8d-914c-dc82042be5df",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üîê <b>Solution for Task #5</b></summary>\n",
    "\n",
    "```python\n",
    "def PGD_generator(model, loss_fn, path, y, epsilon=8/255, alpha=0.01, num_iters=6):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using Projected Gradient Descent (PGD)\n",
    "    by starting with noisy initialization and iteratively applying gradient-based perturbations.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model used for classification\n",
    "        loss_fn: loss function for computing gradients\n",
    "        path (str): filepath to the input image\n",
    "        y (Tensor): true class label tensor for the image\n",
    "        epsilon (float): maximum allowed perturbation magnitude, defaults to 8/255\n",
    "        alpha (float): step size for each iteration, defaults to 0.01\n",
    "        num_iters (int): number of iterative steps to perform, defaults to 6\n",
    "\n",
    "    Returns [1, 3, 32, 32]: adversarially perturbed image tensor with\n",
    "        perturbations bounded by epsilon and pixel values clamped to [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    img = process_image(path)\n",
    "    original_img = img.clone()\n",
    "\n",
    "    # 1. add noise to initial image\n",
    "    img = add_noise(img)\n",
    "    \n",
    "    # 2. loop num_iter times and complete 2-4 on each iteration\n",
    "    for i in range(num_iters):\n",
    "        img.requires_grad = True\n",
    "        \n",
    "        # 3. calculate the loss\n",
    "        out = model(img)\n",
    "        loss = loss_fn(out, y)\n",
    "    \n",
    "        # 4. calculate the gradient with respect to the input\n",
    "        loss.backward()\n",
    "        grad = img.grad.data\n",
    "    \n",
    "        # 5. perturb the image using the signs of the gradient\n",
    "        img.requires_grad_(False)\n",
    "        img += alpha * torch.sign(grad)\n",
    "\n",
    "         # 6. clamp the image within epsilon distance\n",
    "        img = clip(img, original_img, epsilon)\n",
    "\n",
    "    return img\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef17eee-9954-49dd-a78b-750916fa60fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(img, stdev=0.001, mean=0):\n",
    "    \"\"\"\n",
    "    Adds Gaussian noise to an input image tensor for use in adversarial\n",
    "    attack initialization or data augmentation.\n",
    "\n",
    "    Args:\n",
    "        img (Tensor): input image tensor to add noise to\n",
    "        stdev (float): standard deviation of Gaussian noise, defaults to 0.001\n",
    "        mean (float): mean of Gaussian noise distribution, defaults to 0\n",
    "\n",
    "    Returns [same as input]: noisy image tensor with added Gaussian noise\n",
    "        sampled from N(mean, stdev¬≤)\n",
    "    \"\"\"\n",
    "    noise = torch.randn_like(img) * stdev + mean\n",
    "    return img + noise\n",
    "\n",
    "def PGD_generator(model, loss_fn, path, y, epsilon=8/255, alpha=0.01, num_iters=6):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using Projected Gradient Descent (PGD)\n",
    "    by starting with noisy initialization and iteratively applying gradient-based perturbations.\n",
    "\n",
    "    Args:\n",
    "        model: PyTorch model used for classification\n",
    "        loss_fn: loss function for computing gradients\n",
    "        path (str): filepath to the input image\n",
    "        y (Tensor): true class label tensor for the image\n",
    "        epsilon (float): maximum allowed perturbation magnitude, defaults to 8/255\n",
    "        alpha (float): step size for each iteration, defaults to 0.01\n",
    "        num_iters (int): number of iterative steps to perform, defaults to 6\n",
    "\n",
    "    Returns [1, 3, 32, 32]: adversarially perturbed image tensor with\n",
    "        perturbations bounded by epsilon and pixel values clamped to [0, 1]\n",
    "    \"\"\"\n",
    "        \n",
    "    img = process_image(path)\n",
    "    original_img = img.clone()\n",
    "    \n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. add noise to initial image\n",
    "    # 2. loop num_iter times and complete 2-4 on each iteration\n",
    "    # 3. calculate the loss\n",
    "    # 4. calculate the gradient with respect to the input\n",
    "    # 5. perturb the image using the signs of the gradient\n",
    "    # 6. clamp the image within epsilon distance\n",
    "    ########## YOUR CODE ENDS HERE ########## \n",
    "\n",
    "    return img\n",
    "\n",
    "path = 'cat.jpg'\n",
    "clean_class = prediction(process_image(path))[0]\n",
    "x_adv_PGD = PGD_generator(model, loss_fn, path, torch.tensor([clean_class]))\n",
    "pred, prob = prediction(x_adv_PGD)\n",
    "print(f\"Prediction: {itos[pred]} with probability {prob:.2f}\")\n",
    "print(f\"Max difference in pixels = {torch.max(torch.abs(x_adv_PGD - process_image('cat.jpg'))).item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5740427d-320e-441a-9624-314e7dafd8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.pgd.task5(model, PGD_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce9e72d-8ea7-46c8-9511-02a280b535c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.utils.show_image(x_adv_PGD)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "810e4e02-9139-4928-8f8e-80fb6be2752c",
   "metadata": {},
   "source": [
    "# 6. Evaluating the distance\n",
    "\n",
    "Now, we will evaluate the distance between the original image and the adversarial images.\n",
    "\n",
    "### Task 6\n",
    "The formula below finds the $L_p$ norm of a vector:\n",
    "\n",
    "$\n",
    "\\|v\\|_p = \\left( \\sum_{i=1}^{n} |v_i|^p \\right)^{\\frac{1}{p}}.\n",
    "$\n",
    "\n",
    "\n",
    "For our purposes, $v_i$ is the difference between a value in the input image and the adversarially perturbed image. \n",
    "\n",
    "In our notebook, we have been constraining our adversarial image within the $L_\\infty$ norm where our $L_\\infty$ budget is $8/255$. The $L_\\infty$ norm should be equal to the maximum absolute difference from the original image. Theoretically, the $L_p$ norm should converge to the $L_\\infty$ norm as $p$ approaches infinity. This means that the value of the $L_\\infty$ norm should be at or below $8/255$ for any adversarial image in this notebook. However, you likely won't observe this convergence when testing large $p$ values due to numerical instability. We'll address this issue in after this task`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac835d3-962f-4f67-838a-daf8026d7373",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üîê <b>Solution for Task #6</b></summary>\n",
    "\n",
    "```python\n",
    "\n",
    "def distance(x1, x2, p):\n",
    "    \"\"\"\n",
    "    Calculates the Lp norm distance between two tensors, commonly used\n",
    "    for measuring perturbation magnitude in adversarial attacks.\n",
    "\n",
    "    Args:\n",
    "        x1 (Tensor): first input tensor\n",
    "        x2 (Tensor): second input tensor  \n",
    "        p (int): norm order (e.g., 1 for L1 norm, 2 for L2 norm)\n",
    "\n",
    "    Returns (float): Lp norm distance between x1 and x2\n",
    "    \"\"\"\n",
    "    return torch.sum(torch.abs(x1 -x2)**p) ** (1/p)\n",
    "\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e62ea1-a68a-46a6-9976-980d7782550d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, p):\n",
    "    \"\"\"\n",
    "    Calculates the Lp norm distance between two tensors, commonly used\n",
    "    for measuring perturbation magnitude in adversarial attacks.\n",
    "\n",
    "    Args:\n",
    "        x1 (Tensor): first input tensor\n",
    "        x2 (Tensor): second input tensor  \n",
    "        p (int): norm order (e.g., 1 for L1 norm, 2 for L2 norm)\n",
    "\n",
    "    Returns (float): Lp norm distance between x1 and x2\n",
    "    \"\"\"\n",
    "    return ########## YOUR CODE HERE ########## \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ba9d03b-187d-4649-afa4-26d70bf95fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.pgd.task6(distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c44eff-2aa6-41c9-b9da-d8ed7dcfcbc0",
   "metadata": {},
   "source": [
    "Below we calculate the $L_2$ norm for `x_adv_FGSM`, `x_adv_BIM` and `x_adv_PGD`. This should see a value ~1.6 or ~1.7 for each attack we ran."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ea31a90-790d-43a5-bad7-1ed256cdc720",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = xlab.utils.process_image('cat.jpg')\n",
    "print(f\"L2 Distance for FGSM: {distance(x, x_adv_FGSM, 2):.2f}\")\n",
    "print(f\"L2 Distance for BIM: {distance(x, x_adv_BIM, 2):.2f}\")\n",
    "print(f\"L2 Distance for PGD: {distance(x, x_adv_PGD, 2):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b7d0e7-cfed-4752-bff0-1ecc5d5f333f",
   "metadata": {},
   "source": [
    "Below we calculate the $L_{\\infty}$ norm for `x_adv_FGSM`, `x_adv_BIM` and `x_adv_PGD`. We don't use the distance function, but instead calculated it by hand, for reasons we will explain below.\n",
    "\n",
    "If you did your attacks correctly, you should see a distance of ~0.0314."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77664f8-fc14-4c48-9a3b-c41451185140",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_inf_FGSM = torch.max(torch.abs(x_adv_FGSM - x)).item()\n",
    "l_inf_BIM = torch.max(torch.abs(x_adv_BIM - x)).item()\n",
    "l_inf_PGD = torch.max(torch.abs(x_adv_PGD - x)).item()\n",
    "\n",
    "print(f\"L infinity Distance for FGSM: {l_inf_FGSM:.4f}\")\n",
    "print(f\"L infinity Distance for BIM: {l_inf_BIM:.4f}\")\n",
    "print(f\"L infinity Distance for PGD: {l_inf_PGD:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08403b13-dbd1-4bcb-bc32-e032e8c5bd1a",
   "metadata": {},
   "source": [
    "# 6a. Numerical Stability\n",
    "\n",
    "Unfortunately, our implementation of the  approach `distance` function lacks numerical stability as p goes to infinity. To mitigate this, the distance function can be tweaked slightly to allow for a more precise calculation. \n",
    "\n",
    "For example, the below example goes to 0 instead of the L_inf norm of 0.0314."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d71bc81-9160-476c-be4b-fcdadf7d3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(distance(x, x_adv_BIM, 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e406441-2e7b-4236-849c-7b0cf11b5712",
   "metadata": {},
   "source": [
    "Here is more numerically stable version of the function. It divides the difference tensor by its maximimum value before raising it to the power of p, before multiplying it back again, leading to greater robustness. Intuitively, this is because it makes every value lower than one, so there is no explosion with increasing powers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3ef6b9-5570-46ef-841e-83870acaa70d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, p):\n",
    "    \"\"\"\n",
    "    Calculates the Lp norm distance between two tensors, with\n",
    "    additional edits for numerical stability.\n",
    "\n",
    "    Args:\n",
    "        x1 (Tensor): first input tensor\n",
    "        x2 (Tensor): second input tensor  \n",
    "        p (int): norm order (e.g., 1 for L1 norm, 2 for L2 norm)\n",
    "\n",
    "    Returns (float): Lp norm distance between x1 and x2\n",
    "    \"\"\"\n",
    "    z = torch.max(abs(x1 - x2))\n",
    "    x = (torch.abs(x1 - x2) / z) ** p\n",
    "    x = torch.sum(x)\n",
    "    return z * x ** (1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49af81-aec4-490b-90b3-fbf4e43ab8a4",
   "metadata": {},
   "source": [
    "The below demonstration shows how the L_norm of x and PGD converges to L_inf as p goes to infinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c04635e-6fd6-4124-bda5-835347a64950",
   "metadata": {},
   "outputs": [],
   "source": [
    "l_inf_norm = torch.max(torch.abs(x - x_adv_PGD)).item()\n",
    "p_values = range(2,200)  \n",
    "distances = [distance(x, x_adv_PGD, p).item() for p in p_values]\n",
    "\n",
    "plt.plot(p_values, distances, '#4871cf', linewidth=3, markersize=6)\n",
    "plt.axhline(y=l_inf_norm, color='maroon', linestyle='dotted', linewidth=3, label=f'L-‚àû norm: {l_inf_norm:.4f}')\n",
    "# plt.yscale('log')  # Log scale for y-axis\n",
    "plt.xlabel('p value', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "plt.title('Convergence of Lp Distance to L‚àû Norm as p Increases', fontsize=14)\n",
    "plt.grid(True, ls=\"--\", alpha=0.7)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c62d6b-0c3d-4ff9-9d76-1bad4880abfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
