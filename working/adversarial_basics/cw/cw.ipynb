{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "565ae561-6c42-4d0c-898e-f2ef4de2f094",
   "metadata": {},
   "source": [
    "# Carlini Wagner Attacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905eb28b-1e9e-4bf5-b6f9-1954ae89b570",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU ARE IN COLAB OR HAVE NOT INSTALLED `xlab-security`\n",
    "!pip install xlab-security # should not take more than a minute or two to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76fec411-b9a4-43c2-b994-d1bbde6b6e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlab\n",
    "\n",
    "# get CIFAR10 helper functions \n",
    "from xlab.utils import CIFAR10\n",
    "itos, stoi = CIFAR10.itos, CIFAR10.stoi\n",
    "\n",
    "# for loading our model\n",
    "from robustbench.utils import load_model\n",
    "\n",
    "# pytorch imports\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch import optim\n",
    "\n",
    "# for plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = xlab.utils.get_best_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000a6a62-e355-42b4-80cf-0d8440e250c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our data\n",
    "NUM_EXAMPLES = 100\n",
    "x_test, y_test = xlab.utils.load_cifar10_test_samples(n=NUM_EXAMPLES)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "print(f\"{x_test.shape=}, {y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df879e0-d470-4721-8875-8d0c96f44f0a",
   "metadata": {},
   "source": [
    "We will begin by loading a pretrained model for classification on CIFAR using robustbench. If you are interested, the code to train the model can be found [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/adversarial_basics_cnn)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d77c11d-40d9-4e62-b2e9-a81492b9efae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from xlab.models import MiniWideResNet, BasicBlock\n",
    "\n",
    "# https://huggingface.co/uchicago-xlab-ai-security/tiny-wideresnet-cifar10\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/tiny-wideresnet-cifar10\",\n",
    "    filename=\"adversarial_basics_cnn.pth\"\n",
    ")\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9696e2a-a746-44f3-985c-78a77cd25ceb",
   "metadata": {},
   "source": [
    "This is the same model that you were using in the previous notebook. As a refresher, you can see that the model has 165,722 trainable parameters, but image classifiers can get much larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af5e1c5-817f-4fdf-b8cc-07999091dd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the parameter count\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d9bd9f1-2b5c-4599-88ac-1423556899e2",
   "metadata": {},
   "source": [
    "## Tasks 1-5: Writing the function $f(x + \\delta)$\n",
    "\n",
    "Recall that we will be using a loss to minimize the equation below.\n",
    "$$\n",
    "\\ell = \\mathcal{D}(x, x + \\delta) + c \\cdot f (x + \\delta)\n",
    "$$\n",
    "\n",
    "The function $\\mathcal{D}$ measures how different $x$ (the original image) is from $x + \\delta$ where $\\delta$ is the perturbations we are making to the image. In the original paper, Carlini and Wagner list seven reasonable choices you could make for $f$. For brevity, you will only implement and compare 3 of them. \n",
    "\n",
    "Note that the equations below are presented slightly differently than in the original paper. They are all mathematically equivalent, but we have rewritten some for clarity. \n",
    "\n",
    "<details>\n",
    "<summary>‚ö†Ô∏è <b>Caution</b></summary>\n",
    "\n",
    "In this section we will have you implement 3 out of the 7 choices for $f$. We encourage you to implement the other choices of $f$ from [the paper](https://arxiv.org/pdf/1608.04644) if you are interested. We will caution you however that there appears to be a sign error in equation #1. The correct equation with the right signs should be shown below. Cross Entropy here will be taken with $\\log_2$ rather than $\\log_e$.\n",
    "\n",
    "$$\n",
    "f_1(x_{\\mathrm{adversarial}}) = \\mathrm{CrossEntropy}(F(x_{\\mathrm{adversarial}})) - 1\n",
    "$$\n",
    "\n",
    "In general, we find that the \"Objective Function\" section of the paper has several mistakes. For example, they say that they \"define an objective function $f$ such that $C(x + \\delta) = t$ if and only if $f(x + \\delta) \\leq 0$.\" but it is trivial to show that equation 4 does not satisfy the \"if and only if\" condition.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539f6e0d-11f7-4ba5-ab2d-12347258a306",
   "metadata": {},
   "source": [
    "### Task #1: Implementing $f_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbfb9b1-06c6-4c0c-83cb-b041016184fb",
   "metadata": {},
   "source": [
    "For task #1 you will implement $f_2$ from [the paper](https://arxiv.org/pdf/1608.04644). \n",
    "\n",
    "The equation for $f_2$ is below. $F_t(x)$ is the model output for class $t$ including the [softmax](https://docs.pytorch.org/docs/stable/generated/torch.nn.Softmax.html) which gives probabilities rather than logits. The first term in this equation takes the greatest softmax probability that is not the target class. We encourage you to pause and think about why the equation is structured in the way that it is.\n",
    "\n",
    "$$\n",
    "f_2(x_{\\mathrm{adversarial}}) = \\mathrm{ReLU}(\\max_{i \\neq t}(F(x_{\\mathrm{adversarial}})) - F_t(x_{\\mathrm{adversarial}})))\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def f2(logits, target):\n",
    "    \"\"\"Computes the margin by which the highest non-target probability exceeds the target probability\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the positive margin, zero if target has highest probability.\n",
    "    \"\"\"\n",
    "    softmax_probs = F.softmax(logits, dim=0)\n",
    "    i_neq_t = torch.argmax(softmax_probs)\n",
    "    if i_neq_t == target:\n",
    "        masked_probs = softmax_probs.clone()\n",
    "        masked_probs[target] = float('-inf')\n",
    "        i_neq_t = torch.argmax(masked_probs)\n",
    "    return F.relu(softmax_probs[i_neq_t] - softmax_probs[target])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c903634-5232-4bd6-8666-5b7f5a96c2f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(logits, target):\n",
    "    \"\"\"Computes the margin by which the highest non-target probability exceeds the target probability\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the positive margin, zero if target has highest probability.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"f2 hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010ffce-f818-461b-9e59-0737229dd99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 3.0])\n",
    "print(f2(x, 0)) # should be >0\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "print(f2(x, 0)) # should be =0\n",
    "x = torch.tensor([3.0, 1.0])\n",
    "print(f2(x, 0)) # should be =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea4e3f6-3a31-4997-b596-da3cf50e9f2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task1(f2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a926e4c-3af3-4f9a-9cc8-c7dc52eba2e9",
   "metadata": {},
   "source": [
    "### Task #2: Implementing $f_4$\n",
    "\n",
    "This is similar to the equation above. The only difference is that we are enforcing the requirement that the target class has a probability greater than or equal to 0.5 rather than a probability greater than or equal to any of the other classes.\n",
    "\n",
    "$$\n",
    "f_4(x_{\\mathrm{adversarial}}) = \\mathrm{ReLU}(0.5 - F_t(x_{\\mathrm{adversarial}}))\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "def f4(logits, target):\n",
    "    \"\"\"Computes the deficit when target class probability falls below 50% confidence\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the confidence deficit, zero if target probability >= 0.5.\n",
    "    \"\"\"\n",
    "    softmax_probs = F.softmax(logits, dim=0)\n",
    "    return F.relu(0.5 - softmax_probs[target])\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d40613-7c21-4972-86b8-7b6b64cb5e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f4(logits, target):\n",
    "    \"\"\"Computes the deficit when target class probability falls below 50% confidence\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the confidence deficit, zero if target probability >= 0.5.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"f4 hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217bcb47-a704-4b07-8ec9-babceb765e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 3.0])\n",
    "print(f4(x, 0)) # should be >0\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "print(f4(x, 0)) # should be =0\n",
    "x = torch.tensor([3.0, 1.0])\n",
    "print(f4(x, 0)) # should be =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eab98f-5ad5-42f3-87ab-38e4c01bd81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task2(f4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e236ef0-e8b1-4f63-9add-1deb036bc979",
   "metadata": {},
   "source": [
    "### Task #3: Implementing $f_6$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b203978-98e6-4585-b6e8-e7b10653b9b4",
   "metadata": {},
   "source": [
    "This is very similar to $f_2$. The difference is we use $Z_t(x)$ instead of $F_t(x)$. $Z_t(x)$ is the raw output of the model for class $t$ rather than the output after the softmax.\n",
    "\n",
    "$$\n",
    "f_6(x_{\\mathrm{adversarial}}) = \\mathrm{ReLU}(\\max_{i \\neq t}(Z(x_{\\mathrm{adversarial}})) - Z_t(x_{\\mathrm{adversarial}}))\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "def f6(logits, target):\n",
    "    \"\"\"Computes the margin by which the highest non-target logit exceeds the target logit\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the positive logit margin, zero if target has highest logit.\n",
    "    \"\"\"\n",
    "    i_neq_t = torch.argmax(logits)\n",
    "    if i_neq_t == target:\n",
    "        masked_logits = logits.clone()\n",
    "        masked_logits[target] = float('-inf')\n",
    "        i_neq_t = torch.argmax(masked_logits)\n",
    "    return F.relu(logits[i_neq_t] - logits[target])\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e573ecaa-691c-422d-9e1c-f65bd260e31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f6(logits, target):\n",
    "    \"\"\"Computes the margin by which the highest non-target logit exceeds the target logit\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the positive logit margin, zero if target has highest logit.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"f6 hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94307969-b187-48d6-8e85-dfc8c6700c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1.0, 3.0])\n",
    "print(f6(x, 0)) # should be >0\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "print(f6(x, 0)) # should be =0\n",
    "x = torch.tensor([3.0, 1.0])\n",
    "print(f6(x, 0)) # should be =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4bac72f-53f1-45dd-a746-5f492b20094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task3(f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41dce756-c5c7-460b-80b7-7741b21dcf35",
   "metadata": {},
   "source": [
    "By now you should have several options for $f$. Now we can optimize the equation below where we let $\\mathcal{D}(x, x + \\delta)$ be the $L_2$ norm.\n",
    "\n",
    "$$\n",
    "\\ell = \\mathcal{D}(x, x + \\delta) + c \\cdot f (x + \\delta)\n",
    "$$\n",
    "\n",
    "\n",
    "We have implemented this below for you. The attack is extremely similar to the PGD attack you coded in the previous notebook, so it should look familiar to you. Still, you should read through the code and make sure you understand everything!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704d020d-03e7-41a5-98b2-c3cc5b800dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_loss_v1(f, x, y, delta, c, model):\n",
    "    logits = model(x + delta)\n",
    "    f_loss = f(logits[0], y)\n",
    "    norm_loss = (torch.sum(delta**2))**1/2\n",
    "\n",
    "    return norm_loss + c * f_loss\n",
    "\n",
    "def cw_simple(f, x, y, c, model, num_iters):\n",
    "    delta = torch.randn_like(x) * 0.1\n",
    "    delta.requires_grad = True\n",
    "    optimizer = optim.Adam([delta], lr=1e-2)\n",
    "\n",
    "    for i in range(num_iters):\n",
    "        l = cw_loss_v1(f, x, y, delta, c, model)\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        if (i + 1) % 5 == 0:\n",
    "            print(f\"iteration {i + 1}:\\t loss={l.item():.4f}\")\n",
    "    return torch.clip(x + delta, 0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335dd128-8402-430a-b2c1-f71d219fdefe",
   "metadata": {},
   "source": [
    "### Task #4: Test different $f$ functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b266e2e7-fef5-4b27-ac2f-b394516c08d8",
   "metadata": {},
   "source": [
    "The next step is to fill in the missing code below to test how effective different choices of $f$ are. When running `benchmark_f` for `f2`, `f4` and `f6` you should find that one choice of $f$ is much more effective than the other two.\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint </b></summary>\n",
    "\n",
    "$f_6$ Should have 100% attack success rate for the 5 images we selected. When we run our code we get:\n",
    "\n",
    "* f2_results = [6, 1, 1, 1, 6]\n",
    "* f4_results = [3, 1, 1, 0, 6]\n",
    "* f6_results = [1, 1, 1, 1, 1]\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "def benchmark_f(f, num_imgs=5, num_iters=40, target_class=1, c=1):\n",
    "    \"\"\"Benchmarks an adversarial attack function by running it on multiple test images\n",
    "    \n",
    "    Args:\n",
    "        f (callable): Loss function to use in the Carlini-Wagner attack.\n",
    "        num_imgs (int): Number of test images to attack.\n",
    "        num_iters (int): Number of optimization iterations per attack.\n",
    "        target_class (int): Target class index for the adversarial examples.\n",
    "        c (float): Regularization parameter for the CW attack.\n",
    "        \n",
    "    Returns:\n",
    "        list[int]: Predicted class indices for each generated adversarial example.\n",
    "    \"\"\"\n",
    "    print(f\"running attacks on {num_imgs} images with target='{itos[target_class]}'\")\n",
    "\n",
    "    results = []\n",
    "    for i in range(num_imgs):\n",
    "        x_clean = x_test[i:i+1].to(device) # tensor is [1, 3, 32, 32]\n",
    "        \n",
    "        adv_image = cw_simple(f, x_clean, target_class, c, model, num_iters) ### YOUR CODE HERE ###\n",
    "        predicted_class = torch.argmax(model(adv_image)).item() ### YOUR CODE HERE ###\n",
    "        print(f\"model predicts class 'target='{itos[predicted_class]}'\")\n",
    "        results.append(predicted_class)\n",
    "\n",
    "    return results\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3d1693-15b3-4451-8582-ef46a40419fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_f(f, num_imgs=5, num_iters=40, target_class=1, c=1):\n",
    "    \"\"\"Benchmarks an adversarial attack function by running it on multiple test images\n",
    "    \n",
    "    Args:\n",
    "        f (callable): Loss function to use in the Carlini-Wagner attack.\n",
    "        num_imgs (int): Number of test images to attack.\n",
    "        num_iters (int): Number of optimization iterations per attack.\n",
    "        target_class (int): Target class index for the adversarial examples.\n",
    "        c (float): Regularization parameter for the CW attack.\n",
    "        \n",
    "    Returns:\n",
    "        list[int]: Predicted class indices for each generated adversarial example.\n",
    "    \"\"\"\n",
    "    print(f\"running attacks on {num_imgs} images with target='{itos[target_class]}'\")\n",
    "\n",
    "    results = []\n",
    "    for i in range(num_imgs):\n",
    "        x_clean = x_test[i:i+1].to(device) # tensor is [1, 3, 32, 32]\n",
    "        \n",
    "        adv_image = ### YOUR CODE HERE ###\n",
    "        predicted_class = ### YOUR CODE HERE ###\n",
    "        print(f\"model predicts class 'target='{itos[predicted_class]}'\")\n",
    "        results.append(predicted_class)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bc24be-1bfd-4422-831f-4fb6f1f22f91",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f2_results = benchmark_f(f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bd0746-b99d-4035-bb5b-c5c15570d5be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f4_results = benchmark_f(f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09216d42-b4e8-479f-8ea1-ce0c9011da1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f6_results = benchmark_f(f6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1694265-74fa-47c0-87a2-ef958c6818f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task4(f2_results, f4_results, f6_results, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71581610-d7f9-4f3d-a4bb-9bdeab107c3a",
   "metadata": {},
   "source": [
    "## Task #5: choosing a value for $c$\n",
    "\n",
    "In the previous section you should have seen that we picked a hardcoded value for $c$. But is there a better strategy than guessing? In the paper, the authors say:\n",
    "\n",
    "<blockquote>\n",
    "    Empirically, we have found that often the best way to choose $c$ is to use the smallest value of $c$ for which the resulting solution $x^{*}$ has $f(x^{*}) \\leq 0$. This causes gradient descent to minimize both of the terms simultaneously instead of picking only one to optimize over first.\n",
    "</blockquote>\n",
    "\n",
    "In other words, to find the ideal result, we want our optimizer to make improvements that minimize the $f$ function but also have the minimum possible impact on the $L_p$ norm. If the $c$ value is too large, the $f$ function will dominate the loss and the optimizer may allow for a larger $L_p$ norm than is strictly necessary. If $c$ is too small, the attack won't be successful. By choosing the smallest possible $c$ that makes the attack successful, we get the best of both worlds.\n",
    "\n",
    "In the code below you will benchmark different $c$ values for a small subset of CIFAR 10.\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #5</b></summary>\n",
    "\n",
    "A good starting point is to make sure that you are using the correct arguments to call `cw_simple`. From there, you should run your example back through the model to see if the attack was successful. In our code, we have:\n",
    "\n",
    "```python\n",
    "adv_image = cw_simple(f, x_clean, target_class, c, model, cw_iters)\n",
    "predicted_class = torch.argmax(model(adv_image)).item()\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #5</b></summary>\n",
    "\n",
    "```python\n",
    "def benchmark_c_values(num_images, cw_iters, f, target_class):\n",
    "    c_values = [1e-3, 1e-2, 1e-1, 1e1, 1e2, 1e3]\n",
    "    c_values_to_prob = {c: 0 for c in c_values}\n",
    "    c_values_to_l2_sum = {c: 0 for c in c_values}\n",
    "\n",
    "    for c in c_values:\n",
    "        ######### OPTIONAL: INIT VARIABLES HERE #########\n",
    "\n",
    "        completed_attacks = 0\n",
    "        img_idx = 0\n",
    "        while completed_attacks < num_images:\n",
    "            print(f\"c = {c}: \\tattack #{completed_attacks+1}\")\n",
    "            x_clean = x_test[img_idx:img_idx+1].to(device)\n",
    "            img_idx += 1\n",
    "\n",
    "            # we should not try to attack an image if the target label = the clean label\n",
    "            if target_class == y_test[img_idx:img_idx+1].item():\n",
    "                continue\n",
    "\n",
    "            ######### YOUR CODE STARTS HERE #########\n",
    "            # 1. find adversarial image using cw_simple, with arguments cw_iters & f   \n",
    "            adv_image = cw_simple(f, x_clean, target_class, c, model, cw_iters)\n",
    "            \n",
    "            # 2. check if cw attack was successful & update c_values_to_prob\n",
    "            predicted_class = torch.argmax(model(adv_image)).item()\n",
    "            if predicted_class == target_class:\n",
    "                c_values_to_prob[c] += 1\n",
    "            \n",
    "            # 3. calculate l2 norm (use distance func) & update c_values_to_l2_sum\n",
    "            c_values_to_l2_sum[c] += distance(adv_image, x_clean, 2).item()\n",
    "            ######### YOUR CODE ENDS HERE #########\n",
    "            \n",
    "            completed_attacks += 1\n",
    "\n",
    "    ######### OPTIONAL: FINAL PROCESSING HERE #########\n",
    "    for c,l2_sum in c_values_to_l2_sum.items():\n",
    "        c_values_to_l2_sum[c] = l2_sum/num_images\n",
    "    for c,succ_count in c_values_to_prob.items():\n",
    "        c_values_to_prob[c] = succ_count/num_images\n",
    "    return c_values_to_l2_sum, c_values_to_prob\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0183d84-a718-473b-9ab3-3e306573e6b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(x1, x2, p):\n",
    "    return torch.sum(torch.abs(x1 -x2)**p) ** (1/p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76f6c60e-f9c9-497c-98a5-ecf99a4244db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_c_values(num_images, cw_iters, f, target_class):\n",
    "    c_values = [1e-3, 1e-2, 1e-1, 1e1, 1e2, 1e3]\n",
    "    c_values_to_prob = {c: 0 for c in c_values}\n",
    "    c_values_to_l2_sum = {c: 0 for c in c_values}\n",
    "\n",
    "    for c in c_values:\n",
    "        ######### OPTIONAL: INIT VARIABLES HERE #########\n",
    "\n",
    "        completed_attacks = 0\n",
    "        img_idx = 0\n",
    "        while completed_attacks < num_images:\n",
    "            print(f\"c = {c}: \\tattack #{completed_attacks+1}\")\n",
    "            x_clean = x_test[img_idx:img_idx+1].to(device)\n",
    "            img_idx += 1\n",
    "\n",
    "            # we should not try to attack an image if the target label = the clean label\n",
    "            if target_class == y_test[img_idx:img_idx+1].item():\n",
    "                continue\n",
    "\n",
    "            ######### YOUR CODE STARTS HERE #########\n",
    "            # 1. find adversarial image using cw_simple, with arguments cw_iters & f   \n",
    "            # 2. check if cw attack was successful & update c_values_to_prob\n",
    "            # 3. calculate l2 norm (use distance func) & update c_values_to_l2_sum\n",
    "            ######### YOUR CODE ENDS HERE #########\n",
    "            \n",
    "            completed_attacks += 1\n",
    "\n",
    "    ######### OPTIONAL: FINAL PROCESSING HERE #########\n",
    "\n",
    "    return c_values_to_l2_sum, c_values_to_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d35a7b02-9076-4ba3-8c44-4002d57d7e06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c_values_to_l2_sum, c_values_to_prob = benchmark_c_values(5, 15, f6, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0613641f-ec56-42b6-b34a-6ec69021735b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax1, ax2 = xlab.utils.plot_dual_2d(\n",
    "    x=list(c_values_to_l2_sum.keys()), \n",
    "    y1=list(c_values_to_l2_sum.values()), \n",
    "    y2=list(c_values_to_prob.values()),\n",
    "    y1_label='L2 Distance',\n",
    "    y2_label='Sucess Probability',\n",
    "    x_label='c value',\n",
    "    y1_axis_label='L2 Distance',\n",
    "    y2_axis_label='Sucess Probability',\n",
    "    log_x=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67540d23-ba5f-4775-bcf9-1e4ee84dbc50",
   "metadata": {},
   "source": [
    "### What do these results mean?\n",
    "\n",
    "This shows that there is some tradeoff between $L_2$ distance and the probability of attack success. Perhaps an even better way to think about this is that there is a tradeoff between how close an image is to its original and how easily one can fool the model.\n",
    "\n",
    "\n",
    "Recall that the authors claim that one should choose the smallest value of $c$ that still results in a successful attack. In the paper, the authors use a modified binary search to find this value of $c$. For brevity, we leave this as an exercise to the reader but would not expect this to be too difficult to program. For our purposes, we observe that given a reasonable number of iterations in the $L_2$ setting, $c=0.015$ gives a high probability of success and undetectable perturbations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0487a1ea-bb2f-4fb6-8baa-ead15943da0c",
   "metadata": {},
   "source": [
    "## Task #6: Confidence-Adjusted $f$\n",
    "\n",
    "In task 4, you found that $f_6$ was the most effective choice for $f$. For the $L_2$ attack that we will be implementing, the authors use a slight variation of $f_6$ which introduces a new constant $\\tau$ which encourages the attack to find classifications that confidently choose the wrong class. This variation is shown below:\n",
    "\n",
    "$$\n",
    "f_6(x_{\\mathrm{adversarial}}) = \\max((\\max_{i \\neq t}(Z(x_{\\mathrm{adversarial}}) - Z_t(x_{\\mathrm{adversarial}})), -\\tau)\n",
    "$$\n",
    "\n",
    "The $\\tau$ parameter is different than the $c$ parameter. While the $c$ parameter weights how much the attack cares about the function $f$, the $\\tau$ parameter determines a threshold for how confident of a misclassification $f$ is aiming for. \n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #6</b></summary>\n",
    "\n",
    "```python\n",
    "def confident_f6(logits, target, tau):\n",
    "    \"\"\"Computes the logit margin between highest non-target and target class, clamped to minimum of -tau\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        tau (float): Minimum threshold for the negative margin.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the logit margin, bounded below by -tau.\n",
    "    \"\"\"\n",
    "    i_neq_t = torch.argmax(logits)\n",
    "    if i_neq_t == target:\n",
    "        masked_logits = logits.clone()\n",
    "        masked_logits[target] = float('-inf')\n",
    "        i_neq_t = torch.argmax(masked_logits)\n",
    "    return torch.max((logits[i_neq_t] - logits[target]), -torch.tensor(tau))\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deefd3e6-cf75-474b-a2bf-948ac77e110c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confident_f6(logits, target, tau):\n",
    "    \"\"\"Computes the logit margin between highest non-target and target class, clamped to minimum of -tau\n",
    "    \n",
    "    Args:\n",
    "        logits [num_classes]: Raw prediction scores before softmax.\n",
    "        target (int): Index of the true class.\n",
    "        tau (float): Minimum threshold for the negative margin.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar tensor representing the logit margin, bounded below by -tau.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"confident_f6 hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31628e40-f8f7-467a-815e-814144cf7f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = 0.01\n",
    "\n",
    "x = torch.tensor([1.0, 3.0])\n",
    "print(confident_f6(x, 0, tau)) # should be >0\n",
    "x = torch.tensor([1.0, 1.0])\n",
    "print(confident_f6(x, 0, tau)) # should be =0\n",
    "x = torch.tensor([3.0, 1.0])\n",
    "print(confident_f6(x, 0, tau)) # should be <0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be30f6f6-13f8-43db-a39e-d354971dfb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task6(confident_f6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e34b0311-c8a1-45ad-9764-f82995f0089f",
   "metadata": {},
   "source": [
    "## Task 7-8: Change of variables\n",
    "\n",
    "If you have not already read [our writeup for Carlini-Wagner attacks](https://xlabaisecurity.com/adversarial/cw/) on our website, we highly recommend you do so now. Here is where things start to get tricky.\n",
    "\n",
    "So far we have been thinking about adversarial examples as $x$ (the original image) plus a set of perturbations $\\delta$ bounded by some $L_p$ norm. The issue is that $x + \\delta$ can be greater than 1 or less than 0, which would make it an invalid image. Therefore, Carlini and Wagner propose writing $x + \\delta$ as a function of a new variable $w$ where $f(w)$ is always between zero and one. Here is the equation they proposed:\n",
    "\n",
    "$$\n",
    "x_i + \\delta_i = \\frac{1}{2} (\\tanh({w_i}) + 1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09fdbac7-f96c-4b37-b04f-fbeed1a4fc96",
   "metadata": {},
   "source": [
    "### Task #7: Calculate $x + \\delta$ given $w$\n",
    "\n",
    "In task #7, you will calculate the above equation element-wise to find tensor $x + \\delta$ given a tensor $w$.\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #7</b></summary>\n",
    "\n",
    "```python\n",
    "def get_adv_from_w(w):\n",
    "    \"\"\"Converts unconstrained parameter tensor to valid adversarial example in [0,1] range\n",
    "    \n",
    "    Args:\n",
    "        w [*]: Unconstrained parameter tensor of arbitrary shape.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor with same shape as w, values mapped to [0,1] range.\n",
    "    \"\"\"\n",
    "    return 0.5 * (torch.tanh(w) + 1)```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d31c9f-5011-4109-ae18-57c48c4d3068",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_adv_from_w(w):\n",
    "    \"\"\"Converts unconstrained parameter tensor to valid adversarial example in [0,1] range\n",
    "    \n",
    "    Args:\n",
    "        w [*]: Unconstrained parameter tensor of arbitrary shape.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor with same shape as w, values mapped to [0,1] range.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"get_adv_from_w hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d697a12-d76f-4a96-b35e-80abf8e9786f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task7(get_adv_from_w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bfb0a1-cc8e-4cc1-8e10-802178bef4fd",
   "metadata": {},
   "source": [
    "### Task #8: Calculate $x + \\delta$ given $w$\n",
    "\n",
    "In task #8, you will find the tensor for $\\delta$: i.e., the difference between $x$ and the adversarial image.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #7</b></summary>\n",
    "\n",
    "```python\n",
    "def get_delta(w, x):\n",
    "    \"\"\"Computes the perturbation between adversarial example and original input\n",
    "    \n",
    "    Args:\n",
    "        w [*]: Unconstrained parameter tensor of arbitrary shape.\n",
    "        x [*]: Original input tensor with same shape as w.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Perturbation tensor representing the difference between adversarial and original input.\n",
    "    \"\"\"\n",
    "    return 0.5 * (torch.tanh(w) + 1) - x\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75e0b1-46ab-4df3-8a90-c3f53ab25416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_delta(w, x):\n",
    "    \"\"\"Computes the perturbation between adversarial example and original input\n",
    "    \n",
    "    Args:\n",
    "        w [*]: Unconstrained parameter tensor of arbitrary shape.\n",
    "        x [*]: Original input tensor with same shape as w.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Perturbation tensor representing the difference between adversarial and original input.\n",
    "    \"\"\"\n",
    "    \n",
    "    raise NotImplementedError(\"get_delta hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfe1a2f-13f8-42c5-825c-b0d777b766e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.cw.task8(get_delta)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af89408-d94b-47a5-8cf0-d9d530d2fdce",
   "metadata": {},
   "source": [
    "Now, let's plot what we have found. If we randomly initialize $w$ we should see large values for delta which are both positive and negative. However, $x+\\delta$ should be between 0 and 1. <b>You should not expect to see a reasonable adversarial image if you randomly initialize $w$.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f070cb46-fa1f-45ed-ae8d-6d9a38fc0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_clean = x_test[0:0+1]\n",
    "rand_w = torch.randn_like(x_clean)\n",
    "adv = get_adv_from_w(rand_w)[0].cpu()\n",
    "delta = get_delta(rand_w, x_clean)[0].cpu()\n",
    "\n",
    "# you should expect to see positive and negative numbers\n",
    "_ = xlab.utils.plot_tensors(\n",
    "    [delta[0], delta[1], delta[2]],\n",
    "    log_scale=False, \n",
    "    ncols=3,\n",
    "    # log_scale=True,\n",
    "    titles = [\"delta (red)\", \"delta (green)\", \"delta (blue)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e70647-8810-4c86-8001-5fc458e18af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you should expect to see plots of positive numbers between 0 and 1\n",
    "_ = xlab.utils.plot_tensors(\n",
    "    [adv[0], adv[1], adv[2]],\n",
    "    log_scale=False, \n",
    "    ncols=3,\n",
    "    # log_scale=True,\n",
    "    titles = [\"inital adv img (red)\", \"inital adv img (green)\", \"inital adv img (blue)\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79de2177-911d-4303-9923-b4f31ed3d2a4",
   "metadata": {},
   "source": [
    "## Task #9: Putting it all together\n",
    "\n",
    "Recall the following equation from [the webpage](https://xlabaisecurity.com/adversarial/cw/) associated with these exercises. This outlines the structure of the loss that the CW attack aims to minimize.\n",
    "\n",
    "$$\n",
    "\\mathrm{minimize} \\ \\   \\| \\delta \\|_p + c \\cdot f(x + \\delta)\n",
    "$$\n",
    "\n",
    "With the change of variable shown above, this becomes:\n",
    "\n",
    "$$\n",
    "\\mathrm{minimize} \\ \\   \\| \\frac{1}{2} (\\tanh({w}) + 1) - x \\|_p + c \\cdot f(\\frac{1}{2} (\\tanh({w}) + 1))\n",
    "$$\n",
    "\n",
    "Now we will use this equation to implement the $L_2$ Carlini-Wagner attack the way that the authors constructed it in the original paper. There are two details that require your attention before continuing: \n",
    "\n",
    "1. The authors use $\\| \\delta \\|_2^2$ rather than $\\| \\delta \\|_2$ to minimize the size of $\\delta$.\n",
    "2. The authors choose $f$ to be the function you implemented in `confident_f6` rather than the function you implemented in `f6`\n",
    "\n",
    "This means your loss should look like this:\n",
    "\n",
    "$$\n",
    "\\mathrm{minimize} \\ \\   \\| \\frac{1}{2} (\\tanh({w}) + 1) - x \\|_2^2 + c \\cdot f_{\\mathrm{6-confident}}(\\frac{1}{2} (\\tanh({w}) + 1))\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>Hint for Task #9</summary>\n",
    "\n",
    "The image for this function will be passed with a batch dimension because that is what the model expects. However, when you call `confident_f6` the logits for a single image are expected. Therefore, you should squeeze out the zeroth dimension, as shown below.\n",
    "\n",
    "```python\n",
    "success_loss = c * confident_f6(logits[0], target, k)\n",
    "```\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #9</b></summary>\n",
    "\n",
    "```python\n",
    "def CW_targeted_l2(img, model, c, target, k=0.1, l2_limit=0.5, num_iters=50):\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Using device: {device} for testing...\")\n",
    "\n",
    "    # initialize cw weights and optimizer\n",
    "    cw_weights = torch.randn_like(img).to(device) * 0.001\n",
    "    cw_weights.requires_grad = True\n",
    "    optimizer = optim.Adam([cw_weights], lr=5e-2)\n",
    "\n",
    "    delta = get_delta(cw_weights, img)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        # 1. Find logits\n",
    "        logits = model(img + delta)\n",
    "\n",
    "        # 2. Use c and confident_f6 to calculate the loss\n",
    "        success_loss = c * confident_f6(logits[0], target, k)\n",
    "\n",
    "        # 3. Find the L2 loss\n",
    "        l2_reg = torch.sum((delta) ** 2)\n",
    "\n",
    "        # 4. Call loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "        loss = success_loss + l2_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        delta = get_delta(cw_weights, img)\n",
    "\n",
    "    # print results of attack\n",
    "    with torch.no_grad():\n",
    "        print(f\"total L2 loss:\\t {torch.sum((delta) ** 2)}\")\n",
    "        \n",
    "        logits = model(img + delta)\n",
    "        if torch.argmax(logits[0]) == target:\n",
    "            print(f\"Attack was successful: classification={target}\")\n",
    "        else: \n",
    "            print(f\"Attack was not successful: classification={torch.argmax(logits[0])}\")\n",
    "    \n",
    "    return get_adv_from_w(cw_weights)\n",
    "\n",
    "\n",
    "```\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7fe6da-b572-4e63-b02c-36596805836c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CW_targeted_l2(img, model, c, target, k=0.1, l2_limit=0.5, num_iters=50):\n",
    "    device = next(model.parameters()).device\n",
    "    print(f\"Using device: {device} for testing...\")\n",
    "\n",
    "    # initialize cw weights and optimizer\n",
    "    cw_weights = torch.randn_like(img).to(device) * 0.001\n",
    "    cw_weights.requires_grad = True\n",
    "    optimizer = optim.Adam([cw_weights], lr=5e-2)\n",
    "\n",
    "    delta = get_delta(cw_weights, img)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "\n",
    "        ######### YOUR CODE STARTS HERE #########\n",
    "        # 1. Find logits\n",
    "        # 2. Use c and confident_f6 to calculate the loss\n",
    "        # 3. Find the L2 loss\n",
    "        # 4. Call loss.backward(), optimizer.step(), optimizer.zero_grad()\n",
    "        ######### YOUR CODE ENDS HERE #########\n",
    "\n",
    "        delta = get_delta(cw_weights, img)\n",
    "\n",
    "    # print results of attack\n",
    "    with torch.no_grad():\n",
    "        print(f\"total L2 loss:\\t {torch.sum((delta) ** 2)}\")\n",
    "        \n",
    "        logits = model(img + delta)\n",
    "        if torch.argmax(logits[0]) == target:\n",
    "            print(f\"Attack was successful: classification={target}\")\n",
    "        else: \n",
    "            print(f\"Attack was not successful: classification={torch.argmax(logits[0])}\")\n",
    "    \n",
    "    return get_adv_from_w(cw_weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e10dbe9-278a-491e-ba8e-58acf20bb97f",
   "metadata": {},
   "source": [
    "There are no automated tests for this final function but if you are able to misclassify the image of a cat using the code below, your solution is probably correct. We encourage you to also test on other images in the CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf70941-d3fb-48fc-bc7b-4fcfab9ce149",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_adv = CW_targeted_l2(x_test[0:1], model, 0.03, 2, k=0.1, l2_limit=0.5, num_iters=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e610e9a7-ce5c-4771-b67f-69cbb05bc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.utils.show_image(img_adv.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e89d59-8fe0-47c7-9f18-694fdec84248",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "Congratulations on finishing this notebook! The Carlini-Wagner attack is influential and well-designed, but is also much more difficult than the content you encountered in the previous section.\n",
    "\n",
    "If you are interested in exploring this attack further, there are a few aspects of the original paper that we did not cover which you may find informative:\n",
    "\n",
    "1. For the $L_2$ attack, the authors use multiple starting points for their gradient descent to reduce the likelihood that the optimization becomes stuck in a local minimum. Implementation details for this can be found on page 9 of [the paper](https://arxiv.org/pdf/1608.04644).\n",
    "2. The paper also includes implementation details for two other types of CW attacks: the $L_0$ and $L_\\infty$ attacks. Now that you have completed the $L_2$ attack, the $L_\\infty$ should not be too difficult, as it is quite similar.\n",
    "3. Try benchmarking FGSM, PGD, and CW on different datasets. You should find that each attack has its own advantages and disadvantages. CW, for example, may be more effective against some defenses, while FGSM attacks should have a higher likelihood of transferring to an entirely different model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c853298a-fc5b-465a-907b-8df0d332e443",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
