{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d34354-07dc-49f0-8aca-1f30fe67acc8",
   "metadata": {},
   "source": [
    "# Defensive Distillation\n",
    "\n",
    "The authors of [Distillation as a Defense to Adversarial\n",
    "Perturbations against Deep Neural Networks](https://arxiv.org/pdf/1511.04508) give a description of the four key ideas behind distilling image classifiers as a defense against adversarial examples. \n",
    "\n",
    "1. Start with hard labels (they describe this a series of one-hot vectors, but that is not necessarily how they would be stored in memory).\n",
    "2. Train the initial model using a traditional procedure, but let the final layer have a softmax with a temperature greater than one.\n",
    "3. Create a new training set using the outputs of this initial model. That is, instead of starting with hard labels like the previous model, we start with soft labels outputed by the initial model.\n",
    "4. Train a new model from scratch using the same architecture but with the soft labels (and with the same temperature as before).\n",
    "\n",
    "\n",
    "In this notebook, you will implement the final 2 steps and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27b4738-09b7-4a15-b642-cf69fe0ba227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF YOU ARE IN COLAB OR HAVE NOT INSTALLED `xlab-security`\n",
    "!pip install xlab-security # should not take more than a minute or two to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c81c533-b7fc-4800-aa5b-e5d7c59eaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Flatten, Linear, ReLU\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xlab\n",
    "\n",
    "device = xlab.utils.get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c350e-c645-44c2-a4a6-e70a771aedeb",
   "metadata": {},
   "source": [
    "### Preliminaries: Train an image classifer on hard labels\n",
    "\n",
    "We have already completed this step for you. We trained a simple MLP on the MNIST dataset on for two epochs and achieved a 94.90% accuracy on the test set. Importantly, we use a softmax with temperature ($T=20$) as described in our [explainer page](https://xlabaisecurity.com/adversarial/defensive-distillation/).\n",
    "\n",
    "\n",
    "If interested you can see the output of our training run [here](https://github.com/zroe1/xlab-ai-security/blob/main/models/defensive_distillation/training_output.txt) and the complete code [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/defensive_distillation). You will train your own version of this model for step 5 of this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe2b029-8ffa-47f7-b37a-04a369127aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton of the model we trained\n",
    "class FeedforwardMNIST(nn.Module):\n",
    "    \"\"\"Simple 4-layer MLP for MNIST classification\"\"\"\n",
    "\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FeedforwardMNIST, self).__init__()\n",
    "\n",
    "        input_size = 28 * 28\n",
    "        self.fc1 = Linear(input_size, 256)\n",
    "        self.fc2 = Linear(256, 64)\n",
    "        self.fc3 = Linear(64, num_classes)\n",
    "\n",
    "        self.flatten = Flatten()\n",
    "        self.relu = ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/base-mnist-model\",\n",
    "    filename=\"mnist_mlp_temp_30.pth\",\n",
    ")\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3cd05df-0e2c-4535-aa79-42407a8f1b79",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d45200-0f5c-4cba-87a1-e4eeb18e9b9c",
   "metadata": {},
   "source": [
    "### Benchmark on PGD\n",
    "\n",
    "We will benchmark on the pretained model you just loaded. Note that the model already has most of the resistence to adversarial attacks that you will see in this notebook. This is because we trained the model with a temperature greater than one which already accomplishes most of the smoothing. For comparison, the end of the notebook includes code for loading and benchmarking a model trained with a temperature of one, which you will see has almost 0% robustness against 100 iterations of PGD.\n",
    "\n",
    "When you train your distilled model you should only see a small reduction in attack success. This is actually expected! The authors of the original paper note that the distilled model should in theory converge to the original model but emperically it can offer some additional protection. \n",
    "\n",
    "If the original model is responsible for most of the protection you may wonder why we don't have you implement it. The reason we don't have you train the original model in this notebook is because it is extremely similar to what you will do in step 4. If you are interested, you should find it fairly easy to replace our pretrained model with your own implementation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217f1e9c-cd59-49d8-aec4-e7d0605c2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_imgs = 100\n",
    "imgs, ys = xlab.utils.load_mnist_test_samples(num_test_imgs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_success = 0\n",
    "\n",
    "for img, y in zip(imgs, ys):\n",
    "    adv_x = xlab.utils.PGD(\n",
    "        model, loss_fn, img, y, epsilon=12 / 255, alpha=2 / 255, num_iters=20\n",
    "    )\n",
    "    adv_y = torch.argmax(model(adv_x))\n",
    "\n",
    "    if adv_y.item() != y:\n",
    "        num_success += 1\n",
    "\n",
    "print(f\"{(num_success / num_test_imgs) * 100:.4}% of attacks succeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cae8a8-abdf-43f1-bf95-e32e538e16d2",
   "metadata": {},
   "source": [
    "## Task #1 and #2: Create new training set\n",
    "\n",
    "We will be training our distilled model on the labels of the pretrained model you have loaded above. \n",
    "\n",
    "\n",
    "The model you loaded however, gives logits, not a temperature-smoothed softmax, so to get the proper labels, you will first have to implement the function below which returns softmax with temperature.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def softmax_with_temp(inputs, T):\n",
    "    \"\"\"Applies temperature-scaled softmax to inputs\n",
    "    Args:\n",
    "        inputs [batch, features]: Input logits tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, features]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "    out = inputs / T\n",
    "    return F.softmax(out, dim=1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52423df3-dacf-4cc6-8d60-795b5f19b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temp(inputs, T):\n",
    "    \"\"\"Applies temperature-scaled softmax to inputs\n",
    "    Args:\n",
    "        inputs [batch, features]: Input logits tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, features]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"softmax_with_temp hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9213a332-11b6-487d-a3be-ef5e9f97a5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.distillation.task1(softmax_with_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961b8d7-bed1-4014-b69c-cf4ce4d4121d",
   "metadata": {},
   "source": [
    "Now you will find the labels for each batch by calling the model and running it's outputs through `softmax_with_temp`.\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "def get_batch_labels(batch, T):\n",
    "    \"\"\"Generates temperature-scaled probability distributions for a batch\n",
    "    Args:\n",
    "        batch [batch, *]: Input batch tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, num_classes]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "    outs = model(batch)\n",
    "    outs = softmax_with_temp(outs, T)\n",
    "    return outs\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4e8ceb-d04f-4fdc-9267-76eefb158e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_labels(batch, T):\n",
    "    \"\"\"Generates temperature-scaled probability distributions for a batch\n",
    "    Args:\n",
    "        batch [batch, *]: Input batch tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, num_classes]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "\n",
    "    raise NotImplementedError(\"get_batch_labels hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f67bed46-e24c-408a-8cc9-ac725d8b4f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.distillation.task2(get_batch_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91d7b6b3-731a-4328-ad50-03227af74427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = xlab.utils.get_mnist_train_loader(batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b759a3-707a-4f66-836e-825875ad660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "soft_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, _ in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        soft_labels_batch = get_batch_labels(x_batch, 30)\n",
    "\n",
    "        imgs.append(x_batch.cpu())\n",
    "        soft_labels.append(soft_labels_batch.cpu())\n",
    "\n",
    "all_images = torch.cat(imgs, dim=0)\n",
    "all_soft_labels = torch.cat(soft_labels, dim=0)\n",
    "soft_label_dataset = TensorDataset(all_images, all_soft_labels)\n",
    "\n",
    "batch_size = 128\n",
    "soft_label_loader = DataLoader(\n",
    "    soft_label_dataset,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424649d-17d9-4cab-b187-b36c45271c7d",
   "metadata": {},
   "source": [
    "The first step in contructing this new dataset is to implement `get_batch_labels` by calling the pretrained model with temperature T. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420b6c4-6df6-497d-b027-d9a98f1d0e9d",
   "metadata": {},
   "source": [
    "## Task #3 and #4: Train distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eea637-de24-49fa-a296-f110eb84bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton of the model we trained\n",
    "distilled = FeedforwardMNIST().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d16656-6349-4f65-a85b-8fafbcbf1ddb",
   "metadata": {},
   "source": [
    "The optimization problem from the original paper was formalized by the authors using the following equation:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\theta_F} -\\frac{1}{|\\mathcal{X}|} \\sum_{X \\in \\mathcal{X}} \\sum_{i \\in 0..N} F_i(X) \\log F_i^d(X)\n",
    "$$\n",
    "\n",
    "The loss for a single example is simply cross entropy loss with soft labels:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X) = -\\sum_{i \\in 0..N} F_i(X) \\log F_i^d(X)\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "def cross_entropy_loss_soft(soft_labels, probs):\n",
    "    \"\"\"Computes cross-entropy loss between soft labels and predicted probabilities\n",
    "    Args:\n",
    "        soft_labels [batch, num_classes]: Target probability distributions.\n",
    "        probs [batch, num_classes]: Predicted probability distributions.\n",
    "    Returns:\n",
    "        scalar tensor: Normalized cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    assert soft_labels.shape == probs.shape\n",
    "    batch_size = soft_labels.shape[0]\n",
    "\n",
    "    log_probs = torch.log(probs)\n",
    "    return torch.sum(-1 * log_probs *  soft_labels) / batch_size\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4569a3b2-cc57-41d7-803f-978d1898f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_soft(soft_labels, probs):\n",
    "    \"\"\"Computes cross-entropy loss between soft labels and predicted probabilities\n",
    "    Args:\n",
    "        soft_labels [batch, num_classes]: Target probability distributions.\n",
    "        probs [batch, num_classes]: Predicted probability distributions.\n",
    "    Returns:\n",
    "        scalar tensor: Normalized cross-entropy loss value.\n",
    "    \"\"\"\n",
    "\n",
    "    assert soft_labels.shape == probs.shape\n",
    "    raise NotImplementedError(\"cross_entropy_loss_soft hasn't been implemented.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752d7530-4bcb-4037-96c0-448482ec7b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.distillation.task3(cross_entropy_loss_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd7bc9-e032-4abb-988b-a0c74ea4842e",
   "metadata": {},
   "source": [
    "Now you will fill in the function to train your distilled model. Most of this work has already been done for you.\n",
    "\n",
    "Note that there are no tests for this task. You can evaluate the quality of your solution by benchmarking your model in the following section.\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "def train(model, epochs, train_loader, T):\n",
    "    \"\"\"Trains model using soft label cross-entropy loss with temperature scaling\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        epochs (int): Number of training epochs.\n",
    "        train_loader: DataLoader providing batches of images and soft labels.\n",
    "        T (float): Temperature scaling parameter for softmax.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (img, soft_label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. get logits from model\n",
    "            img, soft_label = img.to(device), soft_label.to(device)\n",
    "            logits = model(img)\n",
    "\n",
    "            # 2. process the logits with softmax_with_temp\n",
    "            out = softmax_with_temp(logits, T)\n",
    "\n",
    "            # 3. compute batch loss\n",
    "            batch_loss = cross_entropy_loss_soft(soft_label, out)\n",
    "    \n",
    "            if i % 50==0:\n",
    "                print(f\"Epoch #{epoch + 1}: batch loss = {batch_loss.item():.4f}\")\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d88ff-77f4-4412-8f51-e84661bd3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_loader, T):\n",
    "    \"\"\"Trains model using soft label cross-entropy loss with temperature scaling\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        epochs (int): Number of training epochs.\n",
    "        train_loader: DataLoader providing batches of images and soft labels.\n",
    "        T (float): Temperature scaling parameter for softmax.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (img, soft_label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ######### YOUR CODE STARTS HERE #########\n",
    "            # 1. get logits from model\n",
    "            # 2. process the logits with softmax_with_temp\n",
    "            # 3. compute batch loss\n",
    "            ########## YOUR CODE ENDS HERE ##########\n",
    "\n",
    "            if i % 50 == 0:\n",
    "                print(f\"Epoch #{epoch + 1}: batch loss = {batch_loss.item():.4f}\")\n",
    "\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09642d84-5b6c-4a3c-a62d-10ed48c41ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(distilled, 3, soft_label_loader, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf0cd7-66ba-4df8-b1eb-abccf3c1428f",
   "metadata": {},
   "source": [
    "## Benchmarking our Defense\n",
    "\n",
    "Below you should see that the clean accuracy is comparable to the original 94.90% accuracy. The attack success rate should be a bit below the success rate of the pretrained model. As we explained above, a lot of the protection comes from the original temperature smoothing, so you should not be surprised if the success rate is only slightly below the original pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a84edc-deed-4481-bc6a-a968a23c3247",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_acc = xlab.utils.evaluate_mnist_accuracy(distilled)\n",
    "print(f\"Clean accuracy of distilled model: {clean_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ac639f-1c26-49ef-b822-295b63673538",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_test_imgs = 100\n",
    "imgs, ys = xlab.utils.load_mnist_test_samples(num_test_imgs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_success = 0\n",
    "\n",
    "for img, y in zip(imgs, ys):\n",
    "    adv_x = xlab.utils.PGD(\n",
    "        distilled, loss_fn, img, y, epsilon=12 / 255, alpha=2 / 255, num_iters=20\n",
    "    )\n",
    "    adv_y = torch.argmax(distilled(adv_x))\n",
    "\n",
    "    if adv_y.item() != y:\n",
    "        num_success += 1\n",
    "\n",
    "print(f\"{(num_success / num_test_imgs) * 100:.4}% of attacks succeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8462726-a752-42e3-819c-57e9637169dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/base-mnist-model\",\n",
    "    filename=\"mnist_mlp_temp_1.pth\",\n",
    ")\n",
    "standard = torch.load(model_path, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ceec7-0150-4be7-8afc-15da019b86d6",
   "metadata": {},
   "source": [
    "## Benchmarking a Traditional Model \n",
    "\n",
    "For reference, below you will the clean accuracy and attack success rate of a model with the same architecture trained with a softmax temperature of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45be25f0-38a9-4203-ae56-43447cc5edf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_acc = xlab.utils.evaluate_mnist_accuracy(standard)\n",
    "print(f\"Clean accuracy of standard model: {clean_acc * 100:.2f}%\")\n",
    "\n",
    "num_test_imgs = 30\n",
    "imgs, ys = xlab.utils.load_mnist_test_samples(num_test_imgs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_success = 0\n",
    "\n",
    "for img, y in zip(imgs, ys):\n",
    "    adv_x = xlab.utils.PGD(\n",
    "        standard, loss_fn, img, y, epsilon=12 / 255, alpha=2 / 255, num_iters=10\n",
    "    )\n",
    "    adv_y = torch.argmax(standard(adv_x))\n",
    "\n",
    "    if adv_y.item() != y:\n",
    "        num_success += 1\n",
    "\n",
    "print(f\"{(num_success / num_test_imgs) * 100:.4}% of attacks succeded\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
