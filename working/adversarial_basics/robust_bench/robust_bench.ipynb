{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70c8fb2a-94cb-4f6c-aaf3-54491e0b8a35",
   "metadata": {},
   "source": [
    "# RobustBench and AutoAttack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b0c868-8d95-44ec-a64e-412ae0e34531",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlab  # for testing your code\n",
    "\n",
    "xlab.hello_world()\n",
    "\n",
    "# get CIFAR10 helper functions\n",
    "from xlab.utils import CIFAR10\n",
    "\n",
    "itos, stoi = CIFAR10.itos, CIFAR10.stoi\n",
    "\n",
    "from robustbench.utils import load_model\n",
    "from robustbench.data import load_cifar10\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from autoattack import AutoAttack\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289a5e30-d713-4a5a-935c-828d4bcce5ab",
   "metadata": {},
   "source": [
    "We will begin by loading a pretrained model for classification on CIFAR using `robustbench`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ec0aaa-8167-47b9-aede-389174f95506",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model_name=\"Standard\", threat_model=\"Linf\")\n",
    "model = model.to(device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053dcaa4-00f8-4c9e-a785-a9d533d35396",
   "metadata": {},
   "source": [
    "First, let's take a look at what we have. We can see that there is about 36.5 million trainable parameters in the model. This is actually quite large! For reference, we will be looking at the smallest version of GPT-2 later in the course. That has 128 million parameters, meaning our classifier is about a third of the size of the smallest transformer-based language models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b97d33-40e7-4015-9e05-4c631769e668",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b19f29-740f-45f4-9e6d-4922bfc75fc9",
   "metadata": {},
   "source": [
    "Next, let's look at the architecture. We can see that this is the Wide ResNet we discussed earlier in this course. We encourage you to take a look at the model structure to see if you can infer what is going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44aa75f-78fc-4ed3-9b05-8c8412eb5d9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5669cc8f-c302-4a7c-99a9-5d888b966a86",
   "metadata": {},
   "source": [
    "## Task 1: Benchmark Wide ResNet on Clean Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7f77af-ecf6-4d6a-b182-5f5081b4d726",
   "metadata": {},
   "source": [
    "Now let's see how robust this base model is. First we will want to benchmark the accuracy of clean images. For computational efficiency, we will only load 100 images from [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html). If you would like to load more, and your computer can handle it, we encourage you to do so!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a00ee8d-7876-4971-935a-a90d6698a436",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 100\n",
    "x_test, y_test = load_cifar10(n_examples=NUM_EXAMPLES)\n",
    "x_test, y_test = x_test.to(device), y_test.to(device)\n",
    "print(f\"{x_test.shape=}, {y_test.shape=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2dae01c-8c86-46d0-93f1-354552111266",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #1</b></summary>\n",
    "\n",
    "In our solution we use `logits = model(imgs)` to get logits for every image passed into the function. You can do the same thing with a loop, but it is less efficient and there are more variables to keep track of, potentially leading to small bugs.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #1</b></summary>\n",
    "\n",
    "In our solution we use `torch.sum(pred_labels == labels)` to efficiently calculate how many predicted labels match the true values.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def benchmark_wideresnet(model, imgs, labels):\n",
    "    '''return the accuracy of the model as a floating point number between 0.0 and 1.0'''\n",
    "    acc = 0\n",
    "\n",
    "    logits = model(imgs)\n",
    "    pred_labels = torch.argmax(logits, dim=1)\n",
    "    num_correct = torch.sum(pred_labels == labels)\n",
    "    total_imgs = len(labels)\n",
    "    acc = num_correct/total_imgs\n",
    "    \n",
    "    return acc.item()\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1231f3b4-286a-499b-93b8-035109109be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_wideresnet(model, imgs, labels):\n",
    "    \"\"\"Calculates model accuracy on given images and labels.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained neural network model with forward pass.\n",
    "        imgs: Input images tensor [batch, 3, 32, 32].\n",
    "        labels: True class labels tensor [batch].\n",
    "    \n",
    "    Returns:\n",
    "        Float accuracy between 0.0 and 1.0.\n",
    "    \"\"\"\n",
    "    acc = 0\n",
    "\n",
    "    ########### YOUR CODE HERE ###########\n",
    "    \n",
    "    return acc\n",
    "\n",
    "acc = benchmark_wideresnet(model, x_test, y_test)\n",
    "print(f\"Model accuracy is {acc * 100:.2f}% for clean images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935b6721-9cad-4c5f-a5c7-cd2d4e9bd3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This test will make sure you have reasonable accuracy for the first 100 testing images.\n",
    "# Do NOT change the function below: you do not need to pass in any images or labels.\n",
    "_ = xlab.tests.robustbench.task1(benchmark_wideresnet, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83238ca0-0035-4c49-80e3-70ed502f9c10",
   "metadata": {},
   "source": [
    "## Targeted Attack on Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e950cd2b-82d3-4ded-a560-0c59ffee4fbd",
   "metadata": {},
   "source": [
    "To show how vulnerable this pretrained model is to adversarial attacks, we will benchmark it on a CW attack with only 100 total iterations. \n",
    "\n",
    "As a refresher, the code below shows how to run a targeted CW attack. In this example, we take an image of a horse and generate an example that the model classifies as a frog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7983751-cef7-4544-a53a-2aab700fb21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_idx = 13  # this is an image of a horse\n",
    "true_class_idx = y_test[img_idx].item()\n",
    "target_class = \"bird\"\n",
    "\n",
    "print(f\"Running CW to misclasify {itos[true_class_idx]} as {target_class}...\")\n",
    "\n",
    "adv_image = xlab.utils.CW_targeted_l2(\n",
    "    x_test[img_idx : img_idx + 1],\n",
    "    model,\n",
    "    c=0.015,\n",
    "    target=stoi[target_class],\n",
    "    num_iters=100,\n",
    "    l2_limit=0.75,\n",
    ")\n",
    "\n",
    "model_adv_pred = torch.argmax(model(adv_image)).item()\n",
    "model_clean_pred = torch.argmax(model(x_test[img_idx : img_idx + 1].to(device))).item()\n",
    "\n",
    "print(\n",
    "    f\"Clean Predicted Label = '{itos[model_clean_pred]}', Adversarial Predicted Label = '{itos[model_adv_pred]}'\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d52a7-71a4-4ccb-981a-237a71da93b1",
   "metadata": {},
   "source": [
    "## Task #2: Benchmark on CW Attacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297bbf92-6e94-41a0-9fde-1567fe48f58c",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #2</b></summary>\n",
    "\n",
    "For `xlab.utils.CW_targeted_l2`, you should be\n",
    "1. passing a python integer to target: e.g., ` target=int(targets[i].item())`.\n",
    "2. passing an image with a batch dimension of 1 (i.e., the tensor will be [1,3,32,32]).\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "# you are free to change the function header, but this is the one the solution will have\n",
    "def get_targeted_cw_imgs(model, imgs, targets, steps=30, c=0.02, l2_limit=0.75):\n",
    "    \"\"\"Generates adversarial examples using targeted Carlini-Wagner attack.\n",
    "    \n",
    "    Args:\n",
    "        model: Target neural network model.\n",
    "        imgs: Input images tensor [batch, 3, 32, 32].\n",
    "        targets: Target class indices tensor [batch].\n",
    "        steps: Number of optimization iterations.\n",
    "        c: Attack strength parameter.\n",
    "        l2_limit: Maximum L2 perturbation allowed.\n",
    "    \n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    adv_imgs = []\n",
    "\n",
    "    for i in range(imgs.shape[0]):\n",
    "        print(f\"running CW on image {i + 1} of {imgs.shape[0]}...\")\n",
    "        adv_img = xlab.utils.CW_targeted_l2(imgs[i:i+1], model, c=c, target=int(targets[i].item()), num_iters=steps, l2_limit=l2_limit)\n",
    "        adv_imgs.append(adv_img)\n",
    "    \n",
    "    return adv_imgs\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fba1f580-2d1d-46be-8a25-4b661ea1745c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# you are free to change the function header, but this is the one the solution will have\n",
    "def get_targeted_cw_imgs(model, imgs, targets, steps=30, c=0.02, l2_limit=0.75):\n",
    "    \"\"\"Generates adversarial examples using targeted Carlini-Wagner attack.\n",
    "    \n",
    "    Args:\n",
    "        model: Target neural network model.\n",
    "        imgs: Input images tensor [batch, 3, 32, 32].\n",
    "        targets: Target class indices tensor [batch].\n",
    "        steps: Number of optimization iterations.\n",
    "        c: Attack strength parameter.\n",
    "        l2_limit: Maximum L2 perturbation allowed.\n",
    "    \n",
    "    Returns:\n",
    "        Python ist of adversarial image tensors (each element is a [1, 3, 32, 32] tensor).\n",
    "    \"\"\"\n",
    "    adv_imgs = []\n",
    "\n",
    "    ########### YOUR CODE HERE ###########\n",
    "    \n",
    "    return adv_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc53f53c-61c8-473b-bd23-abdb2a716752",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = (torch.ones((5,)) * stoi[\"frog\"]).to(device)\n",
    "\n",
    "# we use quite a large L_2 limit for efficiency. you are free to try something more reasonable and increase the number of steps\n",
    "task_2_imgs = get_targeted_cw_imgs(\n",
    "    model, x_test[:5].to(device), target_classes, steps=50, c=0.03, l2_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754752be-7267-4504-93af-8bb213b41fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this test will confirm that each of the adversarial images are classified as \"frog\"\n",
    "_ = xlab.tests.robustbench.task2(task_2_imgs, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5348837e-5280-41f1-b4e5-f72302616f86",
   "metadata": {},
   "source": [
    "We encourage you to play around with the attack you have built to show how brittle a high accuracy classifier can be. Despite having near perfect performance on clean data, you will find that you will be able to fool the model into classifying images from any class to most of the other classes with a reasonable $L_2$ limit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab25dd-b949-4a49-b72e-80f6c4164190",
   "metadata": {},
   "source": [
    "## Shattering Gradients: Demonstrating the Failures of Benchmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6da782-acb0-48e8-b384-fd71a1de76e3",
   "metadata": {},
   "source": [
    "Now we will propose a defense and benchmark it on two variations of PGD which will give us a false sense of confidence in our defense. In other words, we will be doing what many researchers have been doing for years. As the AutoAttack paper states:\n",
    "\n",
    "<blockquote>\n",
    "    <i>Another cause of poor evaluations is the lack of diversity among the attacks used, as most papers rely solely\n",
    "        on the results given by PGD or weaker versions of it like FGSM.\n",
    "    </i>\n",
    "</blockquote>\n",
    "    \n",
    "We will show how doing this can fail in the most extreme way by demonstrating a defense that works quite well against PGD but fails entirely on a black box attack. Admittedly, the defense that we cover is perhaps too silly for most researchers to take completely seriously, but we believe that by working through this example, you will develop good intuitions about how benchmarking attack success can be misleading.\n",
    "\n",
    "To be very clear: <b>We do not present the below defense because it is good.</b> We present it because it will <i>appear</i> good\n",
    "under a poorly designed evaluation. There is actually a famous paper which exposes these kinds of defenses titled [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/pdf/1802.00420) which we will discuss more at the end of the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1eea70e-ab6e-4228-ae48-a71565baf008",
   "metadata": {},
   "source": [
    "Time to dive into our \"defense.\" If you print the model, you will see that the model is using a [ReLU activation function](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)). ReLU non-linearities give very clean gradients:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\text{ReLU}(x) = \\frac{d}{dx}\\max(0, x) = \\begin{cases} \n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x < 0 \\\\\n",
    "\\text{undefined} & \\text{if } x = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "In PyTorch, when x=0, the gradient of x is 0 rather than being undefined. We can demonstrate the above equation with the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8ebd38-7143-40f9-ad1e-d8f16566e446",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor(0.0, requires_grad=True)\n",
    "x_2 = torch.tensor(-1.0, requires_grad=True)\n",
    "x_3 = torch.tensor(1.0, requires_grad=True)\n",
    "\n",
    "for x in [x_1, x_2, x_3]:\n",
    "    out = F.relu(x)\n",
    "    out.backward()\n",
    "\n",
    "print(x_1.grad, x_2.grad, x_3.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ccfd355-f407-4179-aca3-56ce6878ca4f",
   "metadata": {},
   "source": [
    "We can calculate the gradient of the input to a ReLU tensor in the following manner:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedbd970-9fb9-46d9-b5b8-aa019162716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU_grad(x):\n",
    "    grad = torch.ones_like(x)\n",
    "    grad[x <= 0] = 0\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1126d7a9-0247-43c8-b057-81de8f99addc",
   "metadata": {},
   "source": [
    "We will now build out a defense where we manipulate the gradients of the ReLU activation function without doing much to affect the forward pass. Consider the following variation of ReLU which we will call WiggleReLU:\n",
    "\n",
    "$$\n",
    "\\text{WiggleReLU}(x) = \\text{ReLU}(x) + \\alpha \\sin(\\omega x) = \\max(0, x) + \\alpha \\sin(\\omega x)\n",
    "$$\n",
    "\n",
    "This gives the following gradients:\n",
    "\n",
    "$$\n",
    "\\frac{d}{dx}\\text{WiggleReLU}(x) = \\begin{cases} \n",
    "1 + \\alpha \\omega \\cos(\\omega x) & \\text{if } x > 0 \\\\\n",
    "\\alpha \\omega \\cos(\\omega x) & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Depending on the magnitude of the amplitude parameter ($\\alpha$) and the frequency parameter ($\\omega$), you will get increasing different gradients compared to the vanilla ReLU function. In task #3 and #4 you will implement WiggleReLU, and examine the gradients it produces. In task #5, you will show that by substituting all ReLUs with WiggleReLU in our original model, attacks like PGD, CW, or FGSM will no longer work. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e572e98-e221-4055-acaf-0b8dea695234",
   "metadata": {},
   "source": [
    "## Task #3 WiggleReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eceea571-0382-4c3e-a00d-eda349921faf",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "def wiggle_ReLU(x, amplitude=0.1, frequency=40):\n",
    "    \"\"\"Applies WiggleReLU activation: ReLU(x) + amplitude * sin(frequency * x).\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor [*].\n",
    "        amplitude: Sine wave amplitude parameter.\n",
    "        frequency: Sine wave frequency parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor [*] with WiggleReLU activation applied.\n",
    "    \"\"\"\n",
    "    return F.relu(x) + amplitude * torch.sin(x * frequency)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750a210-da13-41c2-9e00-d8791474aef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiggle_ReLU(x, amplitude=0.1, frequency=40):\n",
    "    \"\"\"Applies WiggleReLU activation: ReLU(x) + amplitude * sin(frequency * x).\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor [*].\n",
    "        amplitude: Sine wave amplitude parameter.\n",
    "        frequency: Sine wave frequency parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor [*] with WiggleReLU activation applied.\n",
    "    \"\"\"\n",
    "    return ########### YOUR CODE HERE ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1632eb70-2dc0-4aaa-beeb-96ea58241c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.robustbench.task3(wiggle_ReLU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e90f8ad4-d31d-43a7-b37b-ac5dcd06fac1",
   "metadata": {},
   "source": [
    "Let's take a look at what WiggleRELU looks like when we plot it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4bd18bb-e960-48ae-af20-1ba212d078b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "amplitude = 0.1\n",
    "frequency = 40\n",
    "x_vals = torch.arange(-5, 5.01, 0.01)\n",
    "y_vals = wiggle_ReLU(x_vals, amplitude=amplitude, frequency=frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f21e33-d865-4cf6-8dce-b5e815e960bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.utils.plot_2d(\n",
    "    x_vals, y_vals, x_range=None, y_range=None, title=None, figsize=(8, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c50e1-39b9-4fa1-88eb-a13508da7b2b",
   "metadata": {},
   "source": [
    "## Task #4 WiggleReLU gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73da8e74-6e35-49a0-8da4-dda95e98a68b",
   "metadata": {},
   "source": [
    "In our solution to task #4, we clone x and calculate the gradient using PyTorch's autograd. We also encourage you to try to implement the solution without autograd (to do this you would find the derivative of WiggleReLU by hand).\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #4</b></summary>\n",
    "\n",
    "If you are computing wiggle_Relu_grad, you cannot call out.backward() if out is a non-scalar tensor. You can do torch.sum(out).backward() to effectively call backward on each out the outputs. The reason this works is the partial derivative of each input to a sum function is always 1. \n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "def wiggle_ReLU_grad(x, amplitude=0.1, frequency=40):\n",
    "    \"\"\"Computes gradient of WiggleReLU function with respect to input.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor [*].\n",
    "        amplitude: Sine wave amplitude parameter.\n",
    "        frequency: Sine wave frequency parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Gradient tensor [*] same shape as input.\n",
    "    \"\"\"\n",
    "    x_cloned = x.clone().requires_grad_(True)\n",
    "    out = wiggle_ReLU(x_cloned, amplitude, frequency)\n",
    "    torch.sum(out).backward()\n",
    "    \n",
    "    return x_cloned.grad\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ac0fa-40b3-474c-9b4a-8946b7ab9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wiggle_ReLU_grad(x, amplitude=0.1, frequency=40):\n",
    "    \"\"\"Computes gradient of WiggleReLU function with respect to input.\n",
    "    \n",
    "    Args:\n",
    "        x: Input tensor [*].\n",
    "        amplitude: Sine wave amplitude parameter.\n",
    "        frequency: Sine wave frequency parameter.\n",
    "    \n",
    "    Returns:\n",
    "        Gradient tensor [*] same shape as input.\n",
    "    \"\"\"\n",
    "    grad = torch.zeros_like(x) # you can overwrite this if you would like\n",
    "\n",
    "    ########### YOUR CODE HERE ###########\n",
    "    \n",
    "    return grad "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94081cbe-fcd0-4b39-b0f7-bd6ef35a6caa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_ = xlab.tests.robustbench.task4(wiggle_ReLU_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b669eff5-2cb9-417d-9723-ad642409b612",
   "metadata": {},
   "source": [
    "Now let's take a look how wiggle_ReLU scrambles gradients. In the plot below you should observe two things:\n",
    "\n",
    "1. That the `ReLU Clean Output` plot should display a similar pattern to the `WiggleReLU Output`\n",
    "2. That the `ReLU Clean Grad` plot should display a <b><i>very different</i></b> pattern compared to the `WiggleReLU Grad`\n",
    "\n",
    "In other words, the forward pass will look mostly the same with the WiggleReLU, but the backward pass should be mostly scrambled. This means that when we try to take a gradient using something like CW, we won't get a reliable signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "850c07b0-2b5f-4575-bbfa-49c627e1c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn((7, 7)) * 3\n",
    "\n",
    "relu_out = F.relu(input_tensor)\n",
    "relu_grad = ReLU_grad(input_tensor)\n",
    "\n",
    "wiggle_relu_out = wiggle_ReLU(input_tensor)\n",
    "wiggle_relu_grad = wiggle_ReLU_grad(input_tensor)\n",
    "\n",
    "# this will display the differnce between the output of ReLU and WiggleReLU\n",
    "# these tensors should look nearly identical\n",
    "_ = xlab.utils.plot_tensors(\n",
    "    [relu_out, wiggle_relu_out],\n",
    "    log_scale=False,  # if you switch this to True, you will see that the tensors are in fact not identical\n",
    "    titles=[\"ReLU Clean Output\", \"WiggleReLU Output\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e706678-a5c3-43b3-906a-9a88cb97c932",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.utils.plot_tensors(\n",
    "    [relu_grad, wiggle_relu_grad],\n",
    "    log_scale=True,  # we switch this to true because some WiggleReLU gradients are quite large\n",
    "    titles=[\"ReLU Clean Grad\", \"WiggleReLU Grad\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09243bab-ea0e-491c-851b-2673248ecb6a",
   "metadata": {},
   "source": [
    "## Task #5 Replacing ReLU with WiggleReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ee849d-97dc-4dda-9585-1fb93265370a",
   "metadata": {},
   "source": [
    "To replace the ReLU functions within the model, we need to make wiggle_ReLU a class that inherits the `nn.Module` class. We have taken care of creating this class for you below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d54040-4fa0-4568-aa22-87d1bc6a3d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WiggleReLU(nn.Module):\n",
    "    def __init__(self, amplitude=0.1, frequency=40):\n",
    "        super(WiggleReLU, self).__init__()\n",
    "        self.frequency = frequency\n",
    "        self.amplitude = amplitude\n",
    "\n",
    "    def forward(self, x):\n",
    "        return wiggle_ReLU(\n",
    "            x, amplitude=self.amplitude, frequency=self.frequency\n",
    "        )  # this is the function you implemented for task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1014b38c-7292-4d4a-9f4d-93a8e1205e88",
   "metadata": {},
   "source": [
    "Now you will replace all of the ReLU activations in our pretrained model with the WiggleReLU class. \n",
    "Once you have modified your modified model, you will pass it to `xlab.tests.robustbench.task4` which will confirm that your implementation is correct. \n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint 1 for Task #5</b></summary>\n",
    "\n",
    "You can extract a ReLU activation from the model using the following code:\n",
    "`model.block1.layer[0].relu1`\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint 2 for Task #5</b></summary>\n",
    "\n",
    "You can replace a ReLU activation from the model using the following code:\n",
    "`model.block1.layer[0].relu1 = WiggleReLU()`\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #5</b></summary>\n",
    "\n",
    "```python\n",
    "for layer in range(4):\n",
    "\n",
    "    # update block 1\n",
    "    model.block1.layer[layer].relu1 = wiggle_relu\n",
    "    model.block1.layer[layer].relu2 = wiggle_relu\n",
    "\n",
    "    # update block 2\n",
    "    model.block2.layer[layer].relu1 = wiggle_relu\n",
    "    model.block2.layer[layer].relu2 = wiggle_relu\n",
    "\n",
    "    # update block 3\n",
    "    model.block3.layer[layer].relu1 = wiggle_relu\n",
    "    model.block3.layer[layer].relu2 = wiggle_relu\n",
    "\n",
    "model.relu = wiggle_relu\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f16fcea-186d-4e28-a0ca-2a5917619832",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiggle_relu = WiggleReLU(\n",
    "    amplitude=0.1, frequency=5000\n",
    ")  # use a high frequency for best results\n",
    "\n",
    "########### YOUR CODE HERE ###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239fecb5-32ce-415d-82d4-d7d95ba2cd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO ZEPHY: still need a test for the above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03637792-9c4e-47ee-990c-8a7a9b3c50c0",
   "metadata": {},
   "source": [
    "Now let's run our CW attack again and we will see that we are no longer able to produce adversarial examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6100e4-daff-478c-a1c9-15239a99bfe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# same function call as task 2 -- this time with a \"protected\" model\n",
    "defense_imgs = get_targeted_cw_imgs(\n",
    "    model, x_test[:5].to(device), target_classes, steps=50, c=0.03, l2_limit=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce77d8d-45ce-4c6d-9519-07029c40ed97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"TARGET CLASS    = \\t{itos[target_classes[0].item()]}\")\n",
    "print(\"#\" * 30)\n",
    "for img in defense_imgs:\n",
    "    pred_class = torch.argmax(model(img))\n",
    "    print(f\"Predicted class =\\t{itos[int(pred_class.item())]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc2bba0-bab6-43ee-be28-8abaeb79e9ca",
   "metadata": {},
   "source": [
    "It appears that the model isn't predicting frog (our target class), but rather is predicting airplane. We can see the reason by inspecting the tensors themselves: instead of being a perturbed image, we get a tensor of `nan` values. In other words, our extreme gradients cause the model to optimize in such an extreme way, that we no longer get actual numbers and therefore CW attacks against the model are no longer successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5806e4-6dc2-47bb-a26a-6da3cf0915a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a look at the first adversarial image\n",
    "defense_imgs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3d0604-3845-413c-9e17-31155cebc9bf",
   "metadata": {},
   "source": [
    "## Benchmarking Our 'Defense' Using AutoAttack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11427567-a934-4b3c-a332-a6e0e37aef6d",
   "metadata": {},
   "source": [
    "Before continuing we encourage you to brainstorm:\n",
    "\n",
    "1. Why this defense would be effective against an attack like PGD or CW\n",
    "2. Why benchmarking ONLY against PGD, CW, or FGSM would give deceptive results\n",
    "\n",
    "After thinking for a while, run the benchmarks below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3753548e-9190-48dd-aaf5-d2c7fb1de160",
   "metadata": {},
   "source": [
    "Now you are ready to benchmark your defense using AutoAttack. Let's use a cheaper version of AutoAttack to get an early idea of how effective our defense is. The code in the following block is taken directly from [this notebook](https://colab.research.google.com/drive/1MQY_7O9vj7ixD5ilVRbdQwlNPFvxifHV#scrollTo=YLGmSORskSW0) released along with the [RobustBench GitHub repo](https://github.com/RobustBench/robustbench?tab=readme-ov-file).\n",
    "\n",
    "<b>NOTE: </b>Even the cheaper attack may take quite a long time to run. If you run into this issue you can modify `NUM_EXAMPLES` to a smaller value. You can also just reference the expected output dropdown below if you don't want to run the evaluation locally.\n",
    "\n",
    "<details>\n",
    "<summary>üíª <b>Expected output for the cell below</b></summary>\n",
    "\n",
    "```\n",
    "using custom version including apgd-ce, apgd-dlr.\n",
    "initial accuracy: 95.00%\n",
    "apgd-ce - 1/1 - 1 out of 19 successfully perturbed\n",
    "robust accuracy after APGD-CE: 90.00% (total time 2.4 s)\n",
    "apgd-dlr - 1/1 - 0 out of 18 successfully perturbed\n",
    "robust accuracy after APGD-DLR: 90.00% (total time 5.0 s)\n",
    "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
    "robust accuracy: 90.00%\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8204beab-4833-47e5-8a95-5b43c0a10b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 20  # make this lower if the cell takes too long to run\n",
    "\n",
    "adversary = AutoAttack(\n",
    "    model,\n",
    "    norm=\"Linf\",\n",
    "    eps=8 / 255,\n",
    "    version=\"custom\",\n",
    "    attacks_to_run=[\"apgd-ce\", \"apgd-dlr\"],\n",
    "    device=device,\n",
    ")\n",
    "adversary.apgd.n_restarts = 1\n",
    "x_adv = adversary.run_standard_evaluation(x_test[:NUM_EXAMPLES], y_test[:NUM_EXAMPLES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b3df66-d4b5-4b35-b694-94b26fc2affe",
   "metadata": {},
   "source": [
    "### Can we trust this cheap version of AutoAttack?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "85b56742-9ecb-43a2-9fec-034709ae5151",
   "metadata": {},
   "source": [
    "First you should see that the `nan` problem has gone away with a better designed attack. You will see something like `nan in tensor: 0` which indicates that there are no `nan` values in the adversarial attacks \n",
    "\n",
    "Admittedly, this is a small sample size, but you should see a number ~75%-90%. Note that if this result was representative of our defense's robustness, our model would have state of the art adversarial robustness by a healthy margin. You can examine the leaderboard for RobustBench below to see that state of the art accuracy is hovering around 74%.\n",
    "\n",
    "<div align=\"center\">\n",
    " <img width=\"700\" alt=\"Image\" src=\"https://xlabaisecurity.com/images/robust_bench.png\" />\n",
    "</div>\n",
    "\n",
    "So did we make a breakthrough? Is it time to publish our findings? Unfortunately, no. Let's try a complete version of AutoAttack to see where things went wrong."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76717b9a-b388-40f7-a418-dbd1f1c5b0da",
   "metadata": {},
   "source": [
    "## Using AutoAttack to compute a more comprehensive evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb56359e-f886-4b45-9391-c54551fb5168",
   "metadata": {},
   "source": [
    "Again, we will note that the below cell should take quite a long time to run. If you are having trouble on whichever machine you are running this code on, you can reference the expected output below.\n",
    "\n",
    "<details>\n",
    "<summary>üíª <b>Expected output for the cell below</b></summary>\n",
    "\n",
    "```\n",
    "setting parameters for standard version\n",
    "using standard version including apgd-ce, apgd-t, fab-t, square.\n",
    "initial accuracy: 100.00%\n",
    "apgd-ce - 1/1 - 0 out of 10 successfully perturbed\n",
    "robust accuracy after APGD-CE: 100.00% (total time 2.2 s)\n",
    "apgd-t - 1/1 - 2 out of 10 successfully perturbed\n",
    "robust accuracy after APGD-T: 80.00% (total time 23.4 s)\n",
    "fab-t - 1/1 - 0 out of 8 successfully perturbed\n",
    "robust accuracy after FAB-T: 80.00% (total time 54.1 s)\n",
    "square - 1/1 - 3 out of 8 successfully perturbed\n",
    "robust accuracy after SQUARE: 50.00% (total time 81.6 s)\n",
    "Warning: Square Attack has decreased the robust accuracy of 30.00%. This might indicate that the robustness evaluation using AutoAttack is unreliable. Consider running Square Attack with more iterations and restarts or an adaptive attack. See flags_doc.md for details.\n",
    "max Linf perturbation: 0.03137, nan in tensor: 0, max: 1.00000, min: 0.00000\n",
    "robust accuracy: 50.00%\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d159a0-7f73-4a7a-9c8c-60a9ceb6a09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EXAMPLES = 10  # adjust this number based on your hardware\n",
    "\n",
    "# this will run AutoAttack with apgd-ce, apgd-t, fab-t, square\n",
    "adversary = AutoAttack(model, norm=\"Linf\", eps=8 / 255, device=device)\n",
    "adversary.apgd.n_restarts = 1\n",
    "x_adv = adversary.run_standard_evaluation(x_test[:NUM_EXAMPLES], y_test[:NUM_EXAMPLES])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958f6748-40b4-4f8e-8d50-bf785f3ca77a",
   "metadata": {},
   "source": [
    "## What Has Happened Here?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee44f4b8-0fdb-4279-9526-63212dc4b699",
   "metadata": {},
   "source": [
    "### The shorter answer\n",
    "\n",
    "The problem with our defense is it manipulated gradients such that we \"fool\" PGD or CW specifically. However, we haven't done anything to account for the underlying problem: that our model has learned a brittle representations for each of the CIFAR classes that can be disrupted with pixel-level perturbations. \n",
    "\n",
    "Therefore, against AutoAttack, our defense does extremely well against white box methods that rely directly on the gradients of input. However, when tested against square, which does not rely directly on the gradients of the input, the true robustness of our model is exposed. In truth, our defense above does little or nothing to protect our model against any competent attacker. It also doesn't demonstrate anything interesting and doesn't push the state of the art forward.\n",
    "\n",
    "\n",
    "AutoAttack will warn you about this, telling you that your model may be deceptively robust. You should have seen something like the following when you ran your final attack:\n",
    "\n",
    "```\n",
    "Warning: Square Attack has decreased the robust accuracy of 30.00%. This might indicate that the robustness evaluation using AutoAttack is unreliable. Consider running Square Attack with more iterations and restarts or an adaptive attack. See flags_doc.md for details.\n",
    "```\n",
    "\n",
    "\n",
    "### The longer answer\n",
    "\n",
    "In [Practical Black-Box Attacks against Machine Learning](https://arxiv.org/pdf/1602.02697), the authors introduce the concept \"gradient masking.\" They say:\n",
    "\n",
    "<blockquote>\n",
    "    ‚ÄúMany potential defense mechanisms fall into a category we call gradient masking. These techniques construct a model that does not have useful gradients, e.g., by using a nearest neighbor classifier instead of a DNN.‚Äù\n",
    "</blockquote>\n",
    "\n",
    "The idea here is that a defense may work against something like CW by giving a gradient that doesn't tell the CW optimizer anything useful. In [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/pdf/1802.00420), the authors coin the term \"obfuscated gradients\" which can be thought of as a proper subset of the techniques that fall under the umbrella of \"gradient masking.\" They offer three categories of obfuscated gradients:\n",
    "\n",
    "<blockquote>\n",
    "    <i>shattered gradients</i> are nonexistent or incorrect gradients\n",
    "caused either intentionally through non-differentiable operations or unintentionally through numerical instability;\n",
    "<i>stochastic gradients</i> depend on test-time randomness; and\n",
    "<i>vanishing/exploding gradients</i> in very deep computation\n",
    "result in an unusable gradient.\n",
    "</blockquote>\n",
    "\n",
    "Our code above would still be considered an example of shattering gradients because we are manipulating the model such that the gradient is grossly incorrect (see full definition below). \n",
    "\n",
    "<blockquote>\n",
    "    <b>Shattered Gradients</b> are caused when a defense is nondifferentiable, introduces numeric instability, or otherwise\n",
    "causes a gradient to be nonexistent or incorrect.\n",
    "</blockquote>\n",
    "\n",
    "We encourage the reader to stop and ponder how one may approach implementing \"stochastic gradients\" or \"vanishing/exploding gradients\" and why this may not be entirely effective. If you are interested in exploring this issue further, you should read the [full paper](https://arxiv.org/pdf/1802.00420) which you should have all the skills by now to understand in full.\n",
    "\n",
    "So what can you do to avoid implementing these kinds of brittle defenses? [Obfuscated Gradients Give a False Sense of Security: Circumventing Defenses to Adversarial Examples](https://arxiv.org/pdf/1802.00420) suggests multiple suggestions which we believe are principled and will guide you in the correct direction. You should treat each of the below items as a potential red flag that what you are doing gives a false sense of security:\n",
    "\n",
    "\n",
    "1. One-step attacks perform better than iterative attacks.\n",
    "2. Black-box attacks are better than white-box attacks.\n",
    "3. Unbounded attacks do not reach 100% success.\n",
    "4. Random sampling finds adversarial examples.\n",
    "5. Increasing distortion bound does not increase success.\n",
    "\n",
    "For more details about each of the above you should reference the [full paper]((https://arxiv.org/pdf/1802.00420#page=10&zoom=100,0,0)) (which again we suggest you read in full!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d66cf26-acef-4228-ac32-950d9e35fe16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
