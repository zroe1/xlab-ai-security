{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d34354-07dc-49f0-8aca-1f30fe67acc8",
   "metadata": {},
   "source": [
    "# Defensive Distillation\n",
    "\n",
    "The author's of [Distillation as a Defense to Adversarial\n",
    "Perturbations against Deep Neural Networks](https://arxiv.org/pdf/1511.04508#page=16&zoom=100,416,109) gives a discription of four key ideas behind distilling image classifiers as a defense against adversarial examples. \n",
    "\n",
    "1. Start with hard labels (they describe this a series of one-hot vectors, but that is not necessarily how they would be stored in memory).\n",
    "2. Train the initial model using a traditional procedure, but let the final layer have a softmax with a temperature greater than one.\n",
    "3. Create a new training set using the outputs of this initial model. That is, instead of starting with hard labels like the previous model, we start with soft labels outputed by the initial model.\n",
    "4. Train a new model from scratch using the same architecture but with the soft labels (and with the same temperature as before).\n",
    "\n",
    "\n",
    "In this notebook, you will implement the final 2 steps and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c81c533-b7fc-4800-aa5b-e5d7c59eaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import xlab\n",
    "\n",
    "device = xlab.utils.get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c350e-c645-44c2-a4a6-e70a771aedeb",
   "metadata": {},
   "source": [
    "### Step 1: Train an image classifer on hard labels\n",
    "\n",
    "We have already completed this step for you. We trained a simple MLP on the MNIST dataset on for two epochs and achieved a 94.90% accuracy on the test set. Importantly, we use a softmax with temperature ($T=20$) as it is described on our [explainer page](https://xlabaisecurity.com/adversarial/defensive-distillation/).\n",
    "\n",
    "\n",
    "If interested you can see the output of our training run [here](https://github.com/zroe1/xlab-ai-security/blob/main/models/defensive_distillation/training_output.txt) and the complete code [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/defensive_distillation). You will train your own version of this model for step 5 of this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fe2b029-8ffa-47f7-b37a-04a369127aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton of the model we trained\n",
    "class FeedforwardMNIST(nn.Module):\n",
    "    \"\"\"Simple 4-layer MLP for MNIST classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FeedforwardMNIST, self).__init__()\n",
    "        \n",
    "        input_size = 28 * 28\n",
    "        self.fc1 = Linear(input_size, 256)\n",
    "        self.fc2 = Linear(256, 64)\n",
    "        self.fc3 = Linear(64, num_classes)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu = ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"uchicago-xlab-ai-security/base-mnist-model\", filename=\"mnist_mlp.pth\")\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a3cd05df-0e2c-4535-aa79-42407a8f1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardMNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cae8a8-abdf-43f1-bf95-e32e538e16d2",
   "metadata": {},
   "source": [
    "## Step 3: Create new training set\n",
    "\n",
    "We will be training our distilled model on the labels of the pretrained model you have loaded above. \n",
    "\n",
    "\n",
    "The model you loaded however, gives logits, not a temperature-smoothed softmax, so to get the proper labels, you will first have to implement the function below which returns softmax with temperature.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91d7b6b3-731a-4328-ad50-03227af74427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = xlab.utils.get_mnist_train_loader(batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "52423df3-dacf-4cc6-8d60-795b5f19b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temp(inputs, T):\n",
    "    out = inputs / T\n",
    "    return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ec946836-79f0-4fab-8e7b-7eeb3bc835a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for x, y in train_loader:\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    batch = get_batch_labels(x, 20)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5f4e8ceb-d04f-4fdc-9267-76eefb158e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_labels(batch, T):\n",
    "    outs = model(batch)\n",
    "    outs = softmax_with_temp(outs, T)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424649d-17d9-4cab-b187-b36c45271c7d",
   "metadata": {},
   "source": [
    "The first step in contructing this new dataset is to implement `get_batch_labels` by calling the pretrained model with temperature T. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
