{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d34354-07dc-49f0-8aca-1f30fe67acc8",
   "metadata": {},
   "source": [
    "# Defensive Distillation\n",
    "\n",
    "The author's of [Distillation as a Defense to Adversarial\n",
    "Perturbations against Deep Neural Networks](https://arxiv.org/pdf/1511.04508#page=16&zoom=100,416,109) gives a discription of four key ideas behind distilling image classifiers as a defense against adversarial examples. \n",
    "\n",
    "1. Start with hard labels (they describe this a series of one-hot vectors, but that is not necessarily how they would be stored in memory).\n",
    "2. Train the initial model using a traditional procedure, but let the final layer have a softmax with a temperature greater than one.\n",
    "3. Create a new training set using the outputs of this initial model. That is, instead of starting with hard labels like the previous model, we start with soft labels outputed by the initial model.\n",
    "4. Train a new model from scratch using the same architecture but with the soft labels (and with the same temperature as before).\n",
    "\n",
    "\n",
    "In this notebook, you will implement the final 2 steps and evaluate the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e27b4738-09b7-4a15-b642-cf69fe0ba227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xlab-security in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (0.1.7)\n",
      "Requirement already satisfied: torch>=1.9.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from xlab-security) (2.7.1)\n",
      "Requirement already satisfied: torchvision>=0.10.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from xlab-security) (0.22.1)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from xlab-security) (2.0.1)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from xlab-security) (3.10.0)\n",
      "Requirement already satisfied: pytest>=6.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from xlab-security) (8.4.0)\n",
      "Requirement already satisfied: Pillow>=8.0.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from xlab-security) (11.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from matplotlib>=3.3.0->xlab-security) (2.9.0.post0)\n",
      "Requirement already satisfied: iniconfig>=1 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from pytest>=6.0->xlab-security) (2.1.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from pytest>=6.0->xlab-security) (1.6.0)\n",
      "Requirement already satisfied: pygments>=2.7.2 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from pytest>=6.0->xlab-security) (2.19.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->xlab-security) (1.17.0)\n",
      "Requirement already satisfied: filelock in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from torch>=1.9.0->xlab-security) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from torch>=1.9.0->xlab-security) (4.14.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from torch>=1.9.0->xlab-security) (1.14.0)\n",
      "Requirement already satisfied: networkx in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from torch>=1.9.0->xlab-security) (3.5)\n",
      "Requirement already satisfied: jinja2 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from torch>=1.9.0->xlab-security) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from torch>=1.9.0->xlab-security) (2024.12.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from sympy>=1.13.3->torch>=1.9.0->xlab-security) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/zephaniahroe/anaconda3/envs/xlab/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->xlab-security) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# IF YOU ARE IN COLAB OR HAVE NOT INSTALLED `xlab-security`\n",
    "!pip install xlab-security # should not take more than a minute or two to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c81c533-b7fc-4800-aa5b-e5d7c59eaf03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Flatten, Linear, ReLU\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import xlab\n",
    "\n",
    "device = xlab.utils.get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6c350e-c645-44c2-a4a6-e70a771aedeb",
   "metadata": {},
   "source": [
    "### Preliminaries: Train an image classifer on hard labels\n",
    "\n",
    "We have already completed this step for you. We trained a simple MLP on the MNIST dataset on for two epochs and achieved a 94.90% accuracy on the test set. Importantly, we use a softmax with temperature ($T=20$) as it is described on our [explainer page](https://xlabaisecurity.com/adversarial/defensive-distillation/).\n",
    "\n",
    "\n",
    "If interested you can see the output of our training run [here](https://github.com/zroe1/xlab-ai-security/blob/main/models/defensive_distillation/training_output.txt) and the complete code [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/defensive_distillation). You will train your own version of this model for step 5 of this notebook.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fe2b029-8ffa-47f7-b37a-04a369127aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton of the model we trained\n",
    "class FeedforwardMNIST(nn.Module):\n",
    "    \"\"\"Simple 4-layer MLP for MNIST classification\"\"\"\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(FeedforwardMNIST, self).__init__()\n",
    "        \n",
    "        input_size = 28 * 28\n",
    "        self.fc1 = Linear(input_size, 256)\n",
    "        self.fc2 = Linear(256, 64)\n",
    "        self.fc3 = Linear(64, num_classes)\n",
    "        \n",
    "        self.flatten = Flatten()\n",
    "        self.relu = ReLU()\n",
    "        \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model_path = hf_hub_download(repo_id=\"uchicago-xlab-ai-security/base-mnist-model\", filename=\"mnist_mlp_temp_30.pth\")\n",
    "model = torch.load(model_path, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a3cd05df-0e2c-4535-aa79-42407a8f1b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FeedforwardMNIST(\n",
      "  (fc1): Linear(in_features=784, out_features=256, bias=True)\n",
      "  (fc2): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (fc3): Linear(in_features=64, out_features=10, bias=True)\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (relu): ReLU()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d45200-0f5c-4cba-87a1-e4eeb18e9b9c",
   "metadata": {},
   "source": [
    "### Benchmark on PGD\n",
    "\n",
    "We will benchmark on the pretained model you just loaded. Note that the model already has most of the resistence to adversarial attacks that you will see in this notebook. This is because we trained the model with a temperature greater than one which already accomplishes most of the smoothing. For comparison, the end of the notebook includes code for loading and benchmarking a model trained with a temperature of one, which you will see has almost 0% robustness against 100 iterations of PGD.\n",
    "\n",
    "When you train your distilled model you should only see a small reduction in attack success. This is actually expected! The authors of the original paper note that the distilled model should in theory converge to the original model but emperically it can offer some additional protection. \n",
    "\n",
    "If the original model is responsible for most of the protection you may wonder why we don't have you implement it. The reason we don't have you train the original model in this notebook is because it is extremely similar to what you will do in step 4. If you are interested, you should find it fairly easy to replace our pretrained model with your own implementation. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "217f1e9c-cd59-49d8-aec4-e7d0605c2e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0% of attacks succeded\n"
     ]
    }
   ],
   "source": [
    "num_test_imgs = 100\n",
    "imgs, ys = xlab.utils.load_mnist_test_samples(num_test_imgs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_success = 0\n",
    "\n",
    "for img, y in zip(imgs, ys):\n",
    "    adv_x = xlab.utils.PGD(model, loss_fn, img, y, epsilon=12/255, alpha=2/255, num_iters=20)\n",
    "    adv_y = torch.argmax(model(adv_x))\n",
    "    \n",
    "    if adv_y.item() != y:\n",
    "        num_success += 1\n",
    "\n",
    "print(f\"{(num_success / num_test_imgs) * 100:.4}% of attacks succeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0cae8a8-abdf-43f1-bf95-e32e538e16d2",
   "metadata": {},
   "source": [
    "## Task #1 and #2: Create new training set\n",
    "\n",
    "We will be training our distilled model on the labels of the pretrained model you have loaded above. \n",
    "\n",
    "\n",
    "The model you loaded however, gives logits, not a temperature-smoothed softmax, so to get the proper labels, you will first have to implement the function below which returns softmax with temperature.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def softmax_with_temp(inputs, T):\n",
    "    \"\"\"Applies temperature-scaled softmax to inputs\n",
    "    Args:\n",
    "        inputs [batch, features]: Input logits tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, features]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "    out = inputs / T\n",
    "    return F.softmax(out, dim=1)\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "52423df3-dacf-4cc6-8d60-795b5f19b0b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_with_temp(inputs, T):\n",
    "    \"\"\"Applies temperature-scaled softmax to inputs\n",
    "    Args:\n",
    "        inputs [batch, features]: Input logits tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, features]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "    out = inputs / T\n",
    "    return F.softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9213a332-11b6-487d-a3be-ef5e9f97a5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for Defensive Distillation, Task 1...\n",
      "======================================================================\n",
      "üéâ All tests passed! (5/5)\n",
      "======================================================================\n",
      "\n",
      "Detailed output:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 13 items / 8 deselected / 5 selected\n",
      "\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_softmax_with_temp_implementation[input_tensor0-1.0-basic functionality, T=1.0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 20%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_softmax_with_temp_implementation[input_tensor1-2.0-multi-batch, T=2.0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 40%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_softmax_with_temp_implementation[input_tensor2-0.5-temperature sharpening, T=0.5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 60%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_softmax_with_temp_implementation[input_tensor3-1.0-edge case: all zeros] \u001b[32mPASSED\u001b[0m\u001b[32m [ 80%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_softmax_properties \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m5 passed\u001b[0m, \u001b[33m8 deselected\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ========================\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = xlab.tests.distillation.task1(softmax_with_temp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2961b8d7-bed1-4014-b69c-cf4ce4d4121d",
   "metadata": {},
   "source": [
    "Now you will find the labels for each batch by calling the model and running it's outputs through `softmax_with_temp`.\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "def get_batch_labels(batch, T):\n",
    "    \"\"\"Generates temperature-scaled probability distributions for a batch\n",
    "    Args:\n",
    "        batch [batch, *]: Input batch tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, num_classes]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "    outs = model(batch)\n",
    "    outs = softmax_with_temp(outs, T)\n",
    "    return outs\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4e8ceb-d04f-4fdc-9267-76eefb158e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_labels(batch, T):\n",
    "    \"\"\"Generates temperature-scaled probability distributions for a batch\n",
    "    Args:\n",
    "        batch [batch, *]: Input batch tensor.\n",
    "        T (float): Temperature scaling parameter.\n",
    "    Returns:\n",
    "        [batch, num_classes]: Temperature-scaled softmax probabilities.\n",
    "    \"\"\"\n",
    "    outs = model(batch)\n",
    "    outs = softmax_with_temp(outs, T)\n",
    "    return outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f67bed46-e24c-408a-8cc9-ac725d8b4f41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for Defensive Distillation, Task 2...\n",
      "======================================================================\n",
      "üéâ All tests passed! (4/4)\n",
      "======================================================================\n",
      "\n",
      "Detailed output:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 13 items / 9 deselected / 4 selected\n",
      "\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_get_batch_labels_basic[1-1.0-single sample, T=1.0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_get_batch_labels_basic[3-2.0-small batch, T=2.0] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_get_batch_labels_basic[2-0.5-batch of 2, T=0.5] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_get_batch_labels_consistency \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 0.70s\u001b[0m\u001b[32m ========================\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = xlab.tests.distillation.task2(get_batch_labels, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d7b6b3-731a-4328-ad50-03227af74427",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = xlab.utils.get_mnist_train_loader(batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75b759a3-707a-4f66-836e-825875ad660d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = []\n",
    "soft_labels= []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, _ in train_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        soft_labels_batch = get_batch_labels(x_batch, 30)\n",
    "\n",
    "        imgs.append(x_batch.cpu())\n",
    "        soft_labels.append(soft_labels_batch.cpu())\n",
    "\n",
    "all_images = torch.cat(imgs, dim=0)\n",
    "all_soft_labels = torch.cat(soft_labels, dim=0)\n",
    "soft_label_dataset = TensorDataset(all_images, all_soft_labels)\n",
    "\n",
    "batch_size = 128\n",
    "soft_label_loader = DataLoader(\n",
    "    soft_label_dataset, \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4424649d-17d9-4cab-b187-b36c45271c7d",
   "metadata": {},
   "source": [
    "The first step in contructing this new dataset is to implement `get_batch_labels` by calling the pretrained model with temperature T. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7420b6c4-6df6-497d-b027-d9a98f1d0e9d",
   "metadata": {},
   "source": [
    "## Task #3 and #4: Train distilled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30eea637-de24-49fa-a296-f110eb84bdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# skeleton of the model we trained\n",
    "distilled =  FeedforwardMNIST().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99d16656-6349-4f65-a85b-8fafbcbf1ddb",
   "metadata": {},
   "source": [
    "The optimization problem from the original paper was formalized by the authors using the following equation:\n",
    "\n",
    "$$\n",
    "\\arg\\min_{\\theta_F} -\\frac{1}{|\\mathcal{X}|} \\sum_{X \\in \\mathcal{X}} \\sum_{i \\in 0..N} F_i(X) \\log F_i^d(X)\n",
    "$$\n",
    "\n",
    "The loss for a single example is simply cross entropy loss with soft labels:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(X) = -\\sum_{i \\in 0..N} F_i(X) \\log F_i^d(X)\n",
    "$$\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "def cross_entropy_loss_soft(soft_labels, probs):\n",
    "    \"\"\"Computes cross-entropy loss between soft labels and predicted probabilities\n",
    "    Args:\n",
    "        soft_labels [batch, num_classes]: Target probability distributions.\n",
    "        probs [batch, num_classes]: Predicted probability distributions.\n",
    "    Returns:\n",
    "        scalar tensor: Normalized cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    assert soft_labels.shape == probs.shape\n",
    "    batch_size = soft_labels.shape[0]\n",
    "\n",
    "    log_probs = torch.log(probs)\n",
    "    return torch.sum(-1 * log_probs *  soft_labels) / batch_size\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4569a3b2-cc57-41d7-803f-978d1898f5eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy_loss_soft(soft_labels, probs):\n",
    "    \"\"\"Computes cross-entropy loss between soft labels and predicted probabilities\n",
    "    Args:\n",
    "        soft_labels [batch, num_classes]: Target probability distributions.\n",
    "        probs [batch, num_classes]: Predicted probability distributions.\n",
    "    Returns:\n",
    "        scalar tensor: Normalized cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    assert soft_labels.shape == probs.shape\n",
    "    batch_size = soft_labels.shape[0]\n",
    "\n",
    "    log_probs = torch.log(probs)\n",
    "    return torch.sum(-1 * log_probs *  soft_labels) / batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "752d7530-4bcb-4037-96c0-448482ec7b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running tests for Defensive Distillation, Task 3...\n",
      "======================================================================\n",
      "üéâ All tests passed! (4/4)\n",
      "======================================================================\n",
      "\n",
      "Detailed output:\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "\u001b[1mcollecting ... \u001b[0mcollected 13 items / 9 deselected / 4 selected\n",
      "\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_cross_entropy_loss_soft_implementation[1-3-single sample, 3 classes] \u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_cross_entropy_loss_soft_implementation[5-10-small batch, 10 classes] \u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_cross_entropy_loss_soft_implementation[2-5-batch of 2, 5 classes] \u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0m\n",
      "../xlab-python-package/xlab/tests/distillation.py::test_cross_entropy_loss_one_hot_labels \u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0m\n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m9 deselected\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ========================\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "_ = xlab.tests.distillation.task3(cross_entropy_loss_soft)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfd7bc9-e032-4abb-988b-a0c74ea4842e",
   "metadata": {},
   "source": [
    "Now you will fill in the function to train your distilled model. Most of this work has already been done for you.\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "def train(model, epochs, train_loader, T):\n",
    "    \"\"\"Trains model using soft label cross-entropy loss with temperature scaling\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        epochs (int): Number of training epochs.\n",
    "        train_loader: DataLoader providing batches of images and soft labels.\n",
    "        T (float): Temperature scaling parameter for softmax.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (img, soft_label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 1. get logits from model\n",
    "            img, soft_label = img.to(device), soft_label.to(device)\n",
    "            logits = model(img)\n",
    "\n",
    "            # 2. process the logits with softmax_with_temp\n",
    "            out = softmax_with_temp(logits, T)\n",
    "\n",
    "            # 3. compute batch loss\n",
    "            batch_loss = cross_entropy_loss_soft(soft_label, out)\n",
    "    \n",
    "            if i % 50==0:\n",
    "                print(f\"Epoch #{epoch + 1}: batch loss = {batch_loss.item():.4f}\")\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59d88ff-77f4-4412-8f51-e84661bd3621",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, train_loader, T):\n",
    "    \"\"\"Trains model using soft label cross-entropy loss with temperature scaling\n",
    "    Args:\n",
    "        model: Neural network model to train.\n",
    "        epochs (int): Number of training epochs.\n",
    "        train_loader: DataLoader providing batches of images and soft labels.\n",
    "        T (float): Temperature scaling parameter for softmax.\n",
    "    \"\"\"\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=5e-3)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for i, (img, soft_label) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            ######### YOUR CODE STARTS HERE ######### \n",
    "            # 1. get logits from model\n",
    "            # 2. process the logits with softmax_with_temp\n",
    "            # 3. compute batch loss\n",
    "            ########## YOUR CODE ENDS HERE ########## \n",
    "    \n",
    "            if i % 50==0:\n",
    "                print(f\"Epoch #{epoch + 1}: batch loss = {batch_loss.item():.4f}\")\n",
    "    \n",
    "            batch_loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "09642d84-5b6c-4a3c-a62d-10ed48c41ef4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch #1: batch loss = 2.3028\n",
      "Epoch #1: batch loss = 0.4981\n",
      "Epoch #1: batch loss = 0.4106\n",
      "Epoch #1: batch loss = 0.2790\n",
      "Epoch #1: batch loss = 0.3143\n",
      "Epoch #1: batch loss = 0.2797\n",
      "Epoch #1: batch loss = 0.2508\n",
      "Epoch #1: batch loss = 0.3209\n",
      "Epoch #1: batch loss = 0.2956\n",
      "Epoch #1: batch loss = 0.2284\n",
      "Epoch #2: batch loss = 0.3229\n",
      "Epoch #2: batch loss = 0.2917\n",
      "Epoch #2: batch loss = 0.1935\n",
      "Epoch #2: batch loss = 0.2452\n",
      "Epoch #2: batch loss = 0.2597\n",
      "Epoch #2: batch loss = 0.1913\n",
      "Epoch #2: batch loss = 0.2314\n",
      "Epoch #2: batch loss = 0.2563\n",
      "Epoch #2: batch loss = 0.3087\n",
      "Epoch #2: batch loss = 0.3107\n",
      "Epoch #3: batch loss = 0.2677\n",
      "Epoch #3: batch loss = 0.2782\n",
      "Epoch #3: batch loss = 0.2237\n",
      "Epoch #3: batch loss = 0.3319\n",
      "Epoch #3: batch loss = 0.2385\n",
      "Epoch #3: batch loss = 0.2370\n",
      "Epoch #3: batch loss = 0.2693\n",
      "Epoch #3: batch loss = 0.2231\n",
      "Epoch #3: batch loss = 0.2827\n",
      "Epoch #3: batch loss = 0.2538\n"
     ]
    }
   ],
   "source": [
    "train(distilled, 3, soft_label_loader, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bf0cd7-66ba-4df8-b1eb-abccf3c1428f",
   "metadata": {},
   "source": [
    "## Benchmarking our Defense\n",
    "\n",
    "Below you should see that the clean accuracy is comparable to the original 94.90% accuracy. The attack success rate should be a bit below the success rate of the pretrained model. As we explained above, a lot of the protection comes from the original temperature smoothing, so you should not be surprised if the success rate is only slightly below the original pretrained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "91a84edc-deed-4481-bc6a-a968a23c3247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy of distilled model: 93.83%\n"
     ]
    }
   ],
   "source": [
    "clean_acc = xlab.utils.evaluate_mnist_accuracy(distilled)\n",
    "print(f\"Clean accuracy of distilled model: {clean_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f0ac639f-1c26-49ef-b822-295b63673538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.0% of attacks succeded\n"
     ]
    }
   ],
   "source": [
    "num_test_imgs = 100\n",
    "imgs, ys = xlab.utils.load_mnist_test_samples(num_test_imgs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_success = 0\n",
    "\n",
    "for img, y in zip(imgs, ys):\n",
    "    adv_x = xlab.utils.PGD(distilled, loss_fn, img, y, epsilon=12/255, alpha=2/255, num_iters=20)\n",
    "    adv_y = torch.argmax(distilled(adv_x))\n",
    "    \n",
    "    if adv_y.item() != y:\n",
    "        num_success += 1\n",
    "\n",
    "print(f\"{(num_success / num_test_imgs) * 100:.4}% of attacks succeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a8462726-a752-42e3-819c-57e9637169dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = hf_hub_download(repo_id=\"uchicago-xlab-ai-security/base-mnist-model\", filename=\"mnist_mlp_temp_1.pth\")\n",
    "standard = torch.load(model_path, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496ceec7-0150-4be7-8afc-15da019b86d6",
   "metadata": {},
   "source": [
    "## Benchmarking a Traditional Model \n",
    "\n",
    "For reference, below you will the clean accuracy and attack success rate of a model with the same architecture trained with a softmax temperature of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "45be25f0-38a9-4203-ae56-43447cc5edf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean accuracy of standard model: 96.62%\n",
      "40.0% of attacks succeded\n"
     ]
    }
   ],
   "source": [
    "clean_acc = xlab.utils.evaluate_mnist_accuracy(standard)\n",
    "print(f\"Clean accuracy of standard model: {clean_acc*100:.2f}%\")\n",
    "\n",
    "num_test_imgs = 30\n",
    "imgs, ys = xlab.utils.load_mnist_test_samples(num_test_imgs)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "num_success = 0\n",
    "\n",
    "for img, y in zip(imgs, ys):\n",
    "    adv_x = xlab.utils.PGD(standard, loss_fn, img, y, epsilon=12/255, alpha=2/255, num_iters=10)\n",
    "    adv_y = torch.argmax(standard(adv_x))\n",
    "    \n",
    "    if adv_y.item() != y:\n",
    "        num_success += 1\n",
    "\n",
    "print(f\"{(num_success / num_test_imgs) * 100:.4}% of attacks succeded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f493f91b-9fd7-47bb-b6ef-a850f9d0ab58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf72fa5f-22e0-4b97-bcdf-50f3d8118d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfef30f4-2c02-47f9-9c90-26790decc105",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
