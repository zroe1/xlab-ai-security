{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29538cf5-91d0-465b-b2b8-70dbfe1173fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = xlab.utils.get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634d506-6471-4390-978c-53fa4b3963a7",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset\n",
    "\n",
    "Before we begin the attack, let's take a look at our data and the surrogate models we will be using for this notebook. First, you can see that the `xlab-security` packages provides you with `xlab.utils.load_mnist_test_samples` which you can use to load members of the MNIST handwritten digit test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a81fe26-05c6-487a-9314-36263572d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([100, 1, 28, 28])\n",
      "Labels shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "mnist_images, mnist_labels = xlab.utils.load_mnist_test_samples(100)\n",
    "print(f\"Images shape: {mnist_images.shape}\")\n",
    "print(f\"Labels shape: {mnist_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9b959-2cb3-4c62-b45c-cffa94191e07",
   "metadata": {},
   "source": [
    "We also provide you with `xlab.utils.show_grayscale_image` to plot MNIST images. You can change the `image_index` below to explore different images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc8a799-3d2d-4784-912d-5b62f7129d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 0\n",
    "xlab.utils.show_grayscale_image(mnist_images[image_index], title=f\"label={mnist_labels[image_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce44c9c-64a5-4467-91a8-bfdfc69ccdfe",
   "metadata": {},
   "source": [
    "## Loading White-Box Models\n",
    "\n",
    "Next let's load the models that we will use to generate our transferable adversarial examples. We will be using a diverse set of models:\n",
    "\n",
    "1. **A Resnet model**, similar to the MiniWideResnet model you used in previous sections.\n",
    "   * Test set accuracy: 97.61%\n",
    "3. **A CNN model**, which is a simple model that has three convolutional layers and three dense layers.\n",
    "   * Test set accuracy: 96.74%\n",
    "5. **A MLP model** which contains three standard fully connected layers.\n",
    "   * Test set accuracy: 94.27%\n",
    "\n",
    "Code for how we trained each of these models can be found [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/MNIST_ensemble). To load the models on your computer, you can run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee07b06-e795-433e-959a-95b3e913a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from xlab.models import ConvolutionalMNIST, ResNetMNIST, BasicBlockMNIST, FeedforwardMNIST\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/mnist-ensemble\",\n",
    "    filename=\"mnist_wideresnet.pth\"\n",
    ")\n",
    "white_box1 = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/mnist-ensemble\",\n",
    "    filename=\"mnist_simple_cnn.pth\"\n",
    ")\n",
    "white_box2 = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "\n",
    "model_path = hf_hub_download(\n",
    "    repo_id=\"uchicago-xlab-ai-security/mnist-ensemble\",\n",
    "    filename=\"mnist_mlp.pth\"\n",
    ")\n",
    "white_box3 = torch.load(model_path, map_location=device, weights_only=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f7c1d-af78-46a1-944d-253ba8e1e4c7",
   "metadata": {},
   "source": [
    "## Loading Black-Box Models\n",
    "\n",
    "Now we can load our black box model which we will attempt to attack in this notebook. You will interact with this model through our python package and you wont be able to see anything about the model architecture. You will be able to only call `model.predict` to get model predictions for a set of images and `model.predict_proba` to get model probabilities for a set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd8d021-5440-452b-b54c-7d8d8b30f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions=tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 2, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 0, 7, 2, 7, 1, 3, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4, 2, 4, 3, 0, 7, 0,\n",
      "        2, 7, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 4, 3, 1, 4,\n",
      "        1, 7, 6, 9])\n",
      "probabilities.shape=torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "import xlab\n",
    "from xlab.models import BlackBox\n",
    "\n",
    "# Load the black box model (downloads automatically)\n",
    "model = xlab.utils.load_black_box_model('mnist-black-box')\n",
    "\n",
    "# Make predictions (model details are hidden)\n",
    "predictions = model.predict(mnist_images)\n",
    "probabilities = model.predict_proba(mnist_images)\n",
    "\n",
    "print(f\"predictions={predictions}\")\n",
    "print(f\"probabilities.shape={probabilities.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f91cd-cbe9-46a9-8bfe-770da6b79795",
   "metadata": {},
   "source": [
    "## Task #1: Ensemble Loss\n",
    "\n",
    "Given an array of $k$ alpha values and $k$ models you will give the weighted cross entropy loss by the following equation:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_\\delta \\  \\  D(\\delta) + \\sum_{i=1}^k \\alpha_i \\cdot \\ell_i(x + \\delta)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1dd12db-3209-4aee-bd14-49a29e934584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_loss(alphas, models, img, target):\n",
    "    loss = torch.tensor(0.0).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    for alpha, model in zip(alphas, models):\n",
    "        out = model(img)\n",
    "        model_loss = loss_fn(out, target)\n",
    "        loss += alpha * model_loss\n",
    "        # print(model_loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0976138a-d292-4f9b-9332-38d2112356c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = mnist_images[0:1].to(device)\n",
    "alphas = [1/3, 1/3, 1/3]\n",
    "models = [white_box1, white_box2, white_box3]\n",
    "\n",
    "example_loss = ensemble_loss(alphas, models, img, torch.tensor([0]).to(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc8fab-33cc-4b10-8332-8ed79436ed5f",
   "metadata": {},
   "source": [
    "## Task #2: Ensemble Attack\n",
    "\n",
    "Now you should be in a good position to complete the ensemble attack. This is exactly the same as PGD, but instead of using a typical loss like Cross Entropy, you will be using the ensemble loss you implemented in `Task #1`. Note that in the original paper, the authors implement something more similar to Carlini-Wagner with a hyperparameter $\\lambda$ which controls how much the distance metric is weighted in the final loss. For simplicity and compatibility with our tests you should use the update rule below:\n",
    "\n",
    "$$\n",
    "x'_i = x + \\mathrm{clip}_\\epsilon(\\alpha \\cdot \\mathrm{sign}(\\nabla \\mathrm{ensemble\\_loss}_{F,t}(x'_{i-1})))\n",
    "$$\n",
    "\n",
    "For the purpose of not making this too difficult, we have allowed a very high $\\epsilon$ value. While one may exect $\\epsilon=28/255$ to yeild some absurd results, in practice this is somewhat reasonable because of the high-contrast nature of the dataset. Also, because it is a gray-scale image, there are fewer pixel values to work with so the total distance of the purturbation (if you take the absolute value and sum) will be comparable to (probably less than) $\\epsilon=12/255$ for a color image.\n",
    "\n",
    "<b>Note:</b> You may use our solution to the clip function from the PGD notebook by calling `xlab.utils.clip`. You can also implement this functionality again within this notebook if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "61ae2c59-eac0-4ab1-8078-03512f2c52a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ensemble_attack_PGD(alphas, models, img, target, epsilon=24/255, alpha=2/255, num_iters=50):\n",
    "    img_original = img.clone()\n",
    "    adv_img = xlab.utils.add_noise(img)\n",
    "\n",
    "    for _ in range(num_iters):\n",
    "        adv_img.requires_grad=True\n",
    "        loss = ensemble_loss(alphas, models, adv_img, target)\n",
    "        loss.backward()\n",
    "        # print(loss)\n",
    "\n",
    "        grad = adv_img.grad.data\n",
    "        adv_img.requires_grad_(False)\n",
    "        adv_img -= alpha * torch.sign(grad)\n",
    "        adv_img = xlab.utils.clip(adv_img, img_original, epsilon)\n",
    "\n",
    "    return adv_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acedae6-67f0-4221-b3c2-02255d4a9995",
   "metadata": {},
   "source": [
    "As a first check, you should see that the targeted attack on the image below should succeed when the target class is 2. If this does not work, we reccomend going back and double checking your code before running the test in the next section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "120f89df-29ee-4368-982a-ef07c68d0fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black box predicts 2\n"
     ]
    }
   ],
   "source": [
    "img = mnist_images[2:3].to(device)\n",
    "adv_img = ensemble_attack_PGD(alphas, models, img, torch.tensor([2]).to(device))\n",
    "xlab.utils.show_grayscale_image(adv_img[0], \"Targeted attack on image of 1\")\n",
    "predictions = model.predict(adv_img)\n",
    "print(f\"Black box predicts {predictions.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2464e7-5876-486a-b770-68c27c5ea41b",
   "metadata": {},
   "source": [
    "# Testing Your Attack\n",
    "\n",
    "Transfering targeted adversarial examples is difficult. To make things easier for you we have identified a list of 5 images which we were able to generate transferable adversarial images for quite easily. For testing, you can run the cell below which will run your attack on these 5 images with the target class 3 (none of the images in `breakable_imgs` has a clean label of 3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "fc6b7def-fc8e-467d-b719-cdb60ec503d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n"
     ]
    }
   ],
   "source": [
    "breakable_idxs = [5, 11, 14, 15, 17]\n",
    "target_class = 3\n",
    "\n",
    "breakable_imgs = [mnist_images[i:i+1].to(device) for i in breakable_idxs]\n",
    "adv_imgs = []\n",
    "\n",
    "for img in breakable_imgs:\n",
    "    adv_img = ensemble_attack_PGD(alphas, models, img, torch.tensor([target_class]).to(device))\n",
    "    adv_imgs.append(adv_img)\n",
    "    \n",
    "    predictions = model.predict(adv_img)\n",
    "    if predictions.item() == target_class:\n",
    "        print(f\"Attack was successful! Predicted class = {target_class}\")\n",
    "    else:\n",
    "        print(f\"Attack was unsuccessful. Predicted class = {predictions.item()} and target class = {target_class}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c409ce0-fa67-4457-bf10-abd26616c104",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "If you are interested, we encourage you to play around with the attack above to see if you can successfully transfer targeted attacks to other classes or to other images in the testing set. Although you should be able to do better than our solution (ours is the bare minimum) you should not expect to be able to complete this attack for every image for every class. In general, these kinds of targeted attacks are difficult to pull off and often require a more involved solution.\n",
    "\n",
    "Ideas for how to improve the attack:\n",
    "\n",
    "1. Tune the alpha values and see how different weights influence your chance of success (you should find this to be the case)\n",
    "2. Try a more sophisticated optimization approach. Instead of using `alpha` to update the image, try using a PyTorch optimizer.\n",
    "3. Go back to the [original paper](https://arxiv.org/pdf/1611.02770) and try to implement something closer to their \"Optimization based approach\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7109710-ee39-4beb-b4d2-f91c9b1ce4bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
