{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29538cf5-91d0-465b-b2b8-70dbfe1173fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xlab\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "device = xlab.utils.get_best_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b634d506-6471-4390-978c-53fa4b3963a7",
   "metadata": {},
   "source": [
    "## Loading the MNIST Dataset\n",
    "\n",
    "Before we begin the attack, let's take a look at our data and the surrogate models we will be using for this notebook. First, you can see that the `xlab-security` packages provides you with `xlab.utils.load_mnist_test_samples` which you can use to load members of the MNIST handwritten digit test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a81fe26-05c6-487a-9314-36263572d1fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images shape: torch.Size([100, 1, 28, 28])\n",
      "Labels shape: torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "mnist_images, mnist_labels = xlab.utils.load_mnist_test_samples(100)\n",
    "print(f\"Images shape: {mnist_images.shape}\")\n",
    "print(f\"Labels shape: {mnist_labels.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03f9b959-2cb3-4c62-b45c-cffa94191e07",
   "metadata": {},
   "source": [
    "We also provide you with `xlab.utils.show_grayscale_image` to plot MNIST images. You can change the `image_index` below to explore different images in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6bc8a799-3d2d-4784-912d-5b62f7129d8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "image_index = 0\n",
    "xlab.utils.show_grayscale_image(mnist_images[image_index], title=f\"label={mnist_labels[image_index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce44c9c-64a5-4467-91a8-bfdfc69ccdfe",
   "metadata": {},
   "source": [
    "## Loading White-Box Models\n",
    "\n",
    "Next let's load the models that we will use to generate our transferable adversarial examples. We will be using a diverse set of models:\n",
    "\n",
    "1. **A Resnet model**, similar to the MiniWideResnet model you used in previous sections.\n",
    "   * Test set accuracy: 97.61%\n",
    "3. **A CNN model**, which is a simple model that has three convolutional layers and three dense layers.\n",
    "   * Test set accuracy: 96.74%\n",
    "5. **A MLP model** which contains three standard fully connected layers.\n",
    "   * Test set accuracy: 94.27%\n",
    "\n",
    "Code for how we trained each of these models can be found [here](https://github.com/zroe1/xlab-ai-security/tree/main/models/MNIST_ensemble). To load the models on your computer, you can run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ee07b06-e795-433e-959a-95b3e913a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from xlab.models import ConvolutionalMNIST, ResNetMNIST, BasicBlockMNIST, FeedforwardMNIST\n",
    "\n",
    "hf_path = \"uchicago-xlab-ai-security/mnist-ensemble\"\n",
    "\n",
    "# resnet model\n",
    "model_path = hf_hub_download(repo_id=hf_path, filename=\"mnist_wideresnet.pth\")\n",
    "white_box1 = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# cnn model\n",
    "model_path = hf_hub_download(repo_id=hf_path, filename=\"mnist_simple_cnn.pth\")\n",
    "white_box2 = torch.load(model_path, map_location=device, weights_only=False)\n",
    "\n",
    "# mlp model\n",
    "model_path = hf_hub_download(repo_id=hf_path, filename=\"mnist_mlp.pth\")\n",
    "white_box3 = torch.load(model_path, map_location=device, weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1f7c1d-af78-46a1-944d-253ba8e1e4c7",
   "metadata": {},
   "source": [
    "## Loading Black-Box Models\n",
    "\n",
    "Now we can load our black box model which we will attempt to attack in this notebook. You will interact with this model through our python package and you wont be able to see anything about the model architecture. You will be able to only call `model.predict` to get model predictions for a set of images and `model.predict_proba` to get model probabilities for a set of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2cd8d021-5440-452b-b54c-7d8d8b30f4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions=tensor([7, 2, 1, 0, 4, 1, 4, 9, 6, 9, 0, 2, 9, 0, 1, 5, 9, 7, 3, 4, 9, 6, 6, 5,\n",
      "        4, 0, 7, 4, 0, 1, 3, 1, 3, 0, 7, 2, 7, 1, 3, 1, 1, 7, 4, 2, 3, 5, 1, 2,\n",
      "        4, 4, 6, 3, 5, 5, 6, 0, 4, 1, 9, 5, 7, 8, 4, 3, 7, 4, 2, 4, 3, 0, 7, 0,\n",
      "        2, 7, 1, 7, 3, 2, 9, 7, 7, 6, 2, 7, 8, 4, 7, 3, 6, 1, 3, 6, 4, 3, 1, 4,\n",
      "        1, 7, 6, 9])\n",
      "probabilities.shape=torch.Size([100, 10])\n"
     ]
    }
   ],
   "source": [
    "from xlab.models import BlackBox\n",
    "\n",
    "# Load the black box model (downloads automatically)\n",
    "black_box = xlab.utils.load_black_box_model('mnist-black-box')\n",
    "\n",
    "# Make predictions (model details are hidden)\n",
    "predictions = black_box.predict(mnist_images)\n",
    "probabilities = black_box.predict_proba(mnist_images)\n",
    "\n",
    "print(f\"predictions={predictions}\")\n",
    "print(f\"probabilities.shape={probabilities.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f91cd-cbe9-46a9-8bfe-770da6b79795",
   "metadata": {},
   "source": [
    "## Task #1: Ensemble Loss\n",
    "\n",
    "Given an array of $k$ alpha values and $k$ models you will give the weighted cross entropy loss by the following equation:\n",
    "\n",
    "$$\n",
    "\\mathrm{argmin}_\\delta \\  \\  D(\\delta) + \\sum_{i=1}^k \\alpha_i \\cdot \\ell_i(x + \\delta)\n",
    "$$\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>🔐 <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def ensemble_loss(alphas, models, img, target):\n",
    "    \"\"\"\n",
    "    Computes weighted ensemble loss across multiple models.\n",
    "\n",
    "    Args:\n",
    "        alphas (list): weight coefficients for each model in the ensemble\n",
    "        models (list): PyTorch models to compute ensemble loss over\n",
    "        img [1, 1, 28, 28]: input MNIST image tensor with batch dimension\n",
    "        target [1]: class label tensor containing single target class\n",
    "\n",
    "    Returns (Tensor): weighted sum of CrossEntropyLoss across all models\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = torch.tensor(0.0).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # 1. iterate over alphas and models\n",
    "    for alpha, model in zip(alphas, models):\n",
    "\n",
    "        # 2. calculated weighted loss for each model\n",
    "        out = model(img)\n",
    "        model_loss = loss_fn(out, target)\n",
    "        loss += alpha * model_loss\n",
    "\n",
    "    return loss\n",
    "  \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1dd12db-3209-4aee-bd14-49a29e934584",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_loss(alphas, models, img, target):\n",
    "    \"\"\"\n",
    "    Computes weighted ensemble loss across multiple models.\n",
    "\n",
    "    Args:\n",
    "        alphas (list): weight coefficients for each model in the ensemble\n",
    "        models (list): PyTorch models to compute ensemble loss over\n",
    "        img [1, 1, 28, 28]: input MNIST image tensor with batch dimension\n",
    "        target [1]: class label tensor containing single target class\n",
    "\n",
    "    Returns (Tensor): weighted sum of CrossEntropyLoss across all models\n",
    "    \"\"\"\n",
    "    \n",
    "    loss = torch.tensor(0.0).to(device)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. iterate over alphas and models\n",
    "    # 2. calculated weighted loss for each model\n",
    "    ########## YOUR CODE ENDS HERE ########## \n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0976138a-d292-4f9b-9332-38d2112356c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13.77052116394043, 15.317304611206055, 8.956832885742188, 7.667974948883057, 18.443864822387695, 14.074539184570312, 27.119890213012695, 0.0033627948723733425, 12.95663833618164, 8.222661018371582]\n",
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "collected 15 items / 4 deselected / 11 selected\n",
      "\n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [  9%]\u001b[0mts/ensemble.py::test_mnist_class_0_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 18%]\u001b[0mts/ensemble.py::test_mnist_class_1_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 27%]\u001b[0mts/ensemble.py::test_mnist_class_2_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 36%]\u001b[0mts/ensemble.py::test_mnist_class_3_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 45%]\u001b[0mts/ensemble.py::test_mnist_class_4_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 54%]\u001b[0mts/ensemble.py::test_mnist_class_5_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 63%]\u001b[0mts/ensemble.py::test_mnist_class_6_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 72%]\u001b[0mts/ensemble.py::test_mnist_class_7_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 81%]\u001b[0mts/ensemble.py::test_mnist_class_8_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 90%]\u001b[0mts/ensemble.py::test_mnist_class_9_loss \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0mts/ensemble.py::test_mnist_losses_count \n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m11 passed\u001b[0m, \u001b[33m4 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "img = mnist_images[0:1].to(device)\n",
    "alphas = [1/3, 1/3, 1/3]\n",
    "models = [white_box1, white_box2, white_box3]\n",
    "\n",
    "example_losses = []\n",
    "with torch.no_grad():\n",
    "    for i in range(10):\n",
    "        example_loss = ensemble_loss(alphas, models, img, torch.tensor([i]).to(device))\n",
    "        example_losses.append(example_loss.item())\n",
    "print(example_losses)\n",
    "\n",
    "xlab.tests.ensemble.task1(example_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecc8fab-33cc-4b10-8332-8ed79436ed5f",
   "metadata": {},
   "source": [
    "## Task #2: Ensemble Attack\n",
    "\n",
    "Now you should be in a good position to complete the ensemble attack. This is exactly the same as PGD, but instead of using a typical loss like Cross Entropy, you will be using the ensemble loss you implemented in `Task #1`. Note that in the original paper, the authors implement something more similar to Carlini-Wagner with a hyperparameter $\\lambda$ which controls how much the distance metric is weighted in the final loss. For simplicity and compatibility with our tests you should use the update rule below:\n",
    "\n",
    "$$\n",
    "x'_i = x + \\mathrm{clip}_\\epsilon(\\alpha \\cdot \\mathrm{sign}(\\nabla \\mathrm{ensemble\\_loss}_{F,t}(x'_{i-1})))\n",
    "$$\n",
    "\n",
    "For the purpose of not making this too difficult, we have allowed a very high $\\epsilon$ value. While one may exect $\\epsilon=28/255$ to yeild some absurd results, in practice this is somewhat reasonable because of the high-contrast nature of the dataset. Also, because it is a gray-scale image, there are fewer pixel values to work with so the total distance of the purturbation (if you take the absolute value and sum) will be comparable to (probably less than) $\\epsilon=12/255$ for a color image.\n",
    "\n",
    "<b>Note:</b> You may use our solution to the clip function from the PGD notebook by calling `xlab.utils.clip`. You can also implement this functionality again within this notebook if you prefer.\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>🔐 <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def ensemble_attack_PGD(alphas, models, img, target, epsilon=24/255, alpha=2/255, num_iters=50):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using Projected Gradient Descent (PGD)\n",
    "    with ensemble loss.\n",
    "\n",
    "    Args:\n",
    "        alphas (list): weight coefficients for each model in the ensemble\n",
    "        models (list): PyTorch models to compute ensemble loss over\n",
    "        img [1, 1, 28, 28]: input MNIST image tensor with batch dimension\n",
    "        target [1]: class label tensor containing target class for attack\n",
    "        epsilon (float): maximum allowed perturbation magnitude, defaults to 24/255\n",
    "        alpha (float): step size for each iteration, defaults to 2/255\n",
    "        num_iters (int): number of iterative steps to perform, defaults to 50\n",
    "\n",
    "    Returns [1, 1, 28, 28]: adversarially perturbed image tensor with\n",
    "        perturbations bounded by epsilon and pixel values clamped to [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    img_original = img.clone()\n",
    "    adv_img = xlab.utils.add_noise(img)\n",
    "\n",
    "    # 1. loop over num_iters \n",
    "    for _ in range(num_iters):\n",
    "        adv_img.requires_grad=True\n",
    "        \n",
    "         # 2. calculate grad of ensemble loss w.r.t. image\n",
    "        loss = ensemble_loss(alphas, models, adv_img, target)\n",
    "        loss.backward()\n",
    "        grad = adv_img.grad.data\n",
    "\n",
    "        # 3. perturb the image using the signs of the gradient\n",
    "        adv_img.requires_grad_(False)\n",
    "        adv_img -= alpha * torch.sign(grad)\n",
    "\n",
    "        # 4. clamp the image within epsilon distance and between 0 and 1\n",
    "        adv_img = xlab.utils.clip(adv_img, img_original, epsilon)\n",
    "\n",
    "    return adv_img\n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61ae2c59-eac0-4ab1-8078-03512f2c52a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def ensemble_attack_PGD(alphas, models, img, target, epsilon=24/255, alpha=2/255, num_iters=50):\n",
    "    \"\"\"\n",
    "    Generates adversarial examples using Projected Gradient Descent (PGD)\n",
    "    with ensemble loss.\n",
    "\n",
    "    Args:\n",
    "        alphas (list): weight coefficients for each model in the ensemble\n",
    "        models (list): PyTorch models to compute ensemble loss over\n",
    "        img [1, 1, 28, 28]: input MNIST image tensor with batch dimension\n",
    "        target [1]: class label tensor containing target class for attack\n",
    "        epsilon (float): maximum allowed perturbation magnitude, defaults to 24/255\n",
    "        alpha (float): step size for each iteration, defaults to 2/255\n",
    "        num_iters (int): number of iterative steps to perform, defaults to 50\n",
    "\n",
    "    Returns [1, 1, 28, 28]: adversarially perturbed image tensor with\n",
    "        perturbations bounded by epsilon and pixel values clamped to [0, 1]\n",
    "    \"\"\"\n",
    "    \n",
    "    img_original = img.clone()\n",
    "    adv_img = xlab.utils.add_noise(img)\n",
    "\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE ######### \n",
    "    # 1. loop over num_iters \n",
    "    # 2. calculate grad of ensemble loss w.r.t. image\n",
    "    # 3. perturb the image using the signs of the gradient\n",
    "    # 4. clamp the image within epsilon distance and between 0 and 1\n",
    "    ########## YOUR CODE ENDS HERE ########## \n",
    "\n",
    "    return adv_img"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1acedae6-67f0-4221-b3c2-02255d4a9995",
   "metadata": {},
   "source": [
    "As a first check, you should see that the targeted attack on the image below should succeed when the target class is 2. If this does not work, we reccomend going back and double checking your code before running the test in the next section of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "120f89df-29ee-4368-982a-ef07c68d0fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Black box predicts 2\n"
     ]
    }
   ],
   "source": [
    "img = mnist_images[2:3].to(device)\n",
    "adv_img = ensemble_attack_PGD(alphas, models, img, torch.tensor([2]).to(device))\n",
    "xlab.utils.show_grayscale_image(adv_img[0], \"Targeted attack on image of 1\")\n",
    "predictions = black_box.predict(adv_img)\n",
    "print(f\"Black box predicts {predictions.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2464e7-5876-486a-b770-68c27c5ea41b",
   "metadata": {},
   "source": [
    "# Testing Your Attack\n",
    "\n",
    "Transfering targeted adversarial examples is difficult. To make things easier for you we have identified a list of 5 images which we were able to generate transferable adversarial images for quite easily. For testing, you can run the cell below which will run your attack on these 5 images with the target class 3 (none of the images in `breakable_imgs` has a clean label of 3). To pass the test below you have to make 4/5 of the attacks be successful. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc6b7def-fc8e-467d-b719-cdb60ec503d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n",
      "Attack was successful! Predicted class = 3\n"
     ]
    }
   ],
   "source": [
    "breakable_idxs = [5, 11, 14, 15, 17]\n",
    "target_class = 3\n",
    "\n",
    "breakable_imgs = [mnist_images[i:i+1].to(device) for i in breakable_idxs]\n",
    "adv_imgs = []\n",
    "\n",
    "for img in breakable_imgs:\n",
    "    adv_img = ensemble_attack_PGD(alphas, models, img, torch.tensor([target_class]).to(device))\n",
    "    adv_imgs.append(adv_img)\n",
    "    \n",
    "    predictions = black_box.predict(adv_img)\n",
    "    if predictions.item() == target_class:\n",
    "        print(f\"Attack was successful! Predicted class = {target_class}\")\n",
    "    else:\n",
    "        print(f\"Attack was unsuccessful. Predicted class = {predictions.item()} and target class = {target_class}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8078024f-5cea-4897-9062-bb58172f03c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "collected 15 items / 11 deselected / 4 selected\n",
      "\n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 25%]\u001b[0mts/ensemble.py::test_mnist_images_count \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0mts/ensemble.py::test_mnist_images_shape \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 75%]\u001b[0mts/ensemble.py::test_model_provided \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0mts/ensemble.py::test_mnist_classification_to_3 \n",
      "\n",
      "\u001b[32m======================= \u001b[32m\u001b[1m4 passed\u001b[0m, \u001b[33m11 deselected\u001b[0m\u001b[32m in 0.02s\u001b[0m\u001b[32m =======================\u001b[0m\n",
      "✅ All checks passed!\n"
     ]
    }
   ],
   "source": [
    "xlab.tests.ensemble.task2(adv_imgs, black_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c409ce0-fa67-4457-bf10-abd26616c104",
   "metadata": {},
   "source": [
    "## Further Exploration\n",
    "\n",
    "If you are interested, we encourage you to play around with the attack above to see if you can successfully transfer targeted attacks to other classes or to other images in the testing set. Although you should be able to do better than our solution (ours is the bare minimum) you should not expect to be able to complete this attack for every image for every class. In general, these kinds of targeted attacks are difficult to pull off and often require a more involved solution.\n",
    "\n",
    "Ideas for how to improve the attack:\n",
    "\n",
    "1. Tune the alpha values and see how different weights influence your chance of success (you should find this to be the case)\n",
    "2. Try a more sophisticated optimization approach. Instead of using `alpha` to update the image, try using a PyTorch optimizer.\n",
    "3. Go back to the [original paper](https://arxiv.org/pdf/1611.02770) and try to implement something closer to their \"Optimization based approach\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
