{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31b2701e",
   "metadata": {},
   "source": [
    "# Circuit Breakers\n",
    "\n",
    "In this notebook, we'll implement the [circuit breakers defense](https://arxiv.org/abs/2406.04313). The crux of the defense is the low-rank representation adaptation algorithm:\n",
    "\n",
    "$$\n",
    "\\begin{array}{l}\n",
    "\\textbf{Algorithm: } \\text{Low-Rank Representation Adaptation (LoRRA)} \\\\\n",
    "\\hline\n",
    "\\textbf{Input: } \\text{circuit breaker dataset } D_\\text{CB}, \\text{ retain dataset } D_r \\\\\n",
    "\\textbf{Input: } \\text{number of steps } T, \\text{ hyperparameter } \\alpha \\\\[0.5em]\n",
    "\\textbf{for } t = 1, \\dots, T \\textbf{ do} \\\\\n",
    "\\quad x_{\\text{CB}} \\sim D_\\text{CB}, \\ x_\\text{retain} \\sim D_r \\\\\n",
    "\\quad c_\\text{RR} \\leftarrow \\alpha\\left(1 - \\frac{t}{2T}\\right), \\ c_\\text{retain} \\leftarrow \\alpha \\frac{t}{2T} \\\\\n",
    "\\quad \\mathcal{L}_\\text{RR} \\leftarrow \\text{ReLU}\\left(\\text{cosine\\_sim}(\\text{rep}_\\text{orig}(x_\\text{CB}), \\ \\text{rep}_\\text{CB}(x_\\text{CB}))\\right) \\\\\n",
    "\\quad \\mathcal{L}_\\text{retain} \\leftarrow \\left\\lVert \\text{rep}_\\text{orig}(x_\\text{retain}) - \\text{rep}_\\text{CB}(x_\\text{retain}) \\right\\lVert_2 \\\\\n",
    "\\quad \\mathcal{L} \\leftarrow c_\\text{RR} \\mathcal{L}_\\text{RR} + c_\\text{retain} \\mathcal{L}_\\text{retain}\\\\\n",
    "\\textbf{end for} \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Before writing any code, however, we'll first provide a brief overview of how we'll go about implementing circuit breakers, in line with the original paper's method. The key is that we want to be able to efficiently extract the representations from the original model $\\text{rep}_\\text{orig}$ and from the circuit-broken model $\\text{rep}_\\text{CB}$. As the name of the algorithm suggests, we'll do this by applying the circuit breakers through a low-rank update, which we *turn off* to get the original model's representations. This means that we're able to work with a single LoRA-enabled model to calculate both the circuit-broken and original representations.\n",
    "\n",
    "A quick note: this notebook treats the phrases \"circuit broken\", \"circuit breaker\", \"representation rerouted\", and \"LoRA\" as generally equivalent (although this is only true within the context of this notebook). \n",
    "\n",
    "Additionally, note that you will *need* a GPU to complete a full circuit-breakers training run. **Even if you don't have access to a GPU, however, we encourage you to complete these exercises** as they'll help give you a strong grasp on representation engineering.\n",
    "\n",
    "Now, our imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8a3fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xlab\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "DEVICE = torch.device(\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "MODEL_PATH = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4991268a",
   "metadata": {},
   "source": [
    "## Task 1: Getting the Original Model's States\n",
    "\n",
    "For Task 1, you'll get the representations for the original model on both the retain and circuit breaker dataset. For some context, the `compute_loss()` function we're working to define has the following code already given to you:\n",
    "```python\n",
    "def compute_loss(self, model, inputs, cb_layers, alpha, **kwargs):\n",
    "    self.current_training_step += 1\n",
    "\n",
    "    cb_ids = inputs.get(\"input_ids_circuit_breaker\")\n",
    "    cb_mask = inputs.get(\"attention_mask_circuit_breaker\")\n",
    "    retain_ids = inputs.get(\"input_ids\")\n",
    "    retain_mask = inputs.get(\"attention_mask\")\n",
    "\n",
    "    cb_inputs = dict(\n",
    "        input_ids=cb_ids, attention_mask=cb_mask, output_hidden_states=True\n",
    "    )\n",
    "    retain_inputs = dict(\n",
    "        input_ids=retain_ids, attention_mask=retain_mask, output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    progress = self.get_progress()\n",
    "    retain_coef = alpha * progress\n",
    "    cb_coef = alpha * (1 - progress)\n",
    "    raise NotImplementedError()\n",
    "```\n",
    "A few other considerations:\n",
    "- We want the `num_layers + 1` hidden states for the retain loss, but only circuit break the layers in `cb_layers` and thus only need the layers in that list.\n",
    "- We want to disable the LoRA adapters and disable gradient calculations before making any forward passes (we strongly suggest you look at hint 1 for this purpose).\n",
    "- `retain_coef` can be `0`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c868aa2",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #1</b></summary>\n",
    "\n",
    "Use `with model.disable_adapter()`, `model.eval()`, and `with torch.no_grad()` to disable the LoRA adapters and gradient calculations.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #1</b></summary>\n",
    "\n",
    "Get the model's hidden states given an output with `output.hidden_states`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #1</b></summary>\n",
    "\n",
    "Use `torch.stack()` to stack the hidden states into a single tensor.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def get_orig_model_states(\n",
    "    model: AutoModelForCausalLM,\n",
    "    retain_inputs: dict,\n",
    "    cb_inputs: dict,\n",
    "    cb_layers: list[int],\n",
    "    retain_coef: float,\n",
    "    cb_coef: float,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Gets the representation states from the original, non-LoRA model.\n",
    "\n",
    "    Args:\n",
    "        model: the model\n",
    "        retain_inputs: dictionary of the inputs from the retain dataset passed\n",
    "            to the model.\n",
    "        cb_inputs: retain_inputs: dictionary of the input from the circuit\n",
    "            breaker dataset passed to the model.\n",
    "        cb_layers: the layers that the circuit breakers are applied to.\n",
    "        retain_coef: the coefficient of the retain loss.\n",
    "        cb_coef: the coefficient of the circuit breaker loss.\n",
    "\n",
    "    Returns [len(cb_layers), batch_size, seq_len, hidden_dim],\n",
    "        [n_layers + 1, batch_size, seq_len, hidden_dim]: tuple of tensors of\n",
    "        circuit breaker and retain states as from the original model.\n",
    "    \"\"\"\n",
    "    with model.disable_adapter():\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            if retain_coef > 0:\n",
    "                # outputs.hidden states = tuple of embeddings tokens + hidden states after each layer\n",
    "                # each tensor is (batch_size, seq_len, hidden_dim)\n",
    "                outputs = model(**retain_inputs, return_dict=True)\n",
    "                # this gives us (num_layers + 1, batch_size, seq_len, hidden_dim)\n",
    "                retain_states_orig = torch.stack(outputs.hidden_states)\n",
    "            if cb_coef > 0:\n",
    "                outputs = model(**cb_inputs)\n",
    "                cb_states_orig = torch.stack(\n",
    "                    [outputs.hidden_states[i] for i in cb_layers]\n",
    "                )\n",
    "    return cb_states_orig, retain_states_orig\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c482bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_orig_model_states(\n",
    "    model: AutoModelForCausalLM,\n",
    "    retain_inputs: dict,\n",
    "    cb_inputs: dict,\n",
    "    cb_layers: list[int],\n",
    "    retain_coef: float,\n",
    "    cb_coef: float,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Gets the representation states from the original, non-LoRA model.\n",
    "\n",
    "    Args:\n",
    "        model: the model\n",
    "        retain_inputs: dictionary of the inputs from the retain dataset passed\n",
    "            to the model.\n",
    "        cb_inputs: retain_inputs: dictionary of the input from the circuit\n",
    "            breaker dataset passed to the model.\n",
    "        cb_layers: the layers that the circuit breakers are applied to.\n",
    "        retain_coef: the coefficient of the retain loss.\n",
    "        cb_coef: the coefficient of the circuit breaker loss.\n",
    "\n",
    "    Returns [len(cb_layers), batch_size, seq_len, hidden_dim],\n",
    "        [n_layers + 1, batch_size, seq_len, hidden_dim]: tuple of tensors of\n",
    "        circuit breaker and retain states as from the original model.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3547d9de",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.circuit_breakers.task1(get_orig_model_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7bdb08",
   "metadata": {},
   "source": [
    "## Task 2: Getting the Circuit-Broken Model's States\n",
    "\n",
    "In Task 2, you'll do the exact same as above, but this time extracting the representations from the LoRA (circuit-broken) model. We want gradients to flow this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5ca60d",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #2</b></summary>\n",
    "\n",
    "Put the model back into training mode with `model.train()`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #2</b></summary>\n",
    "\n",
    "Remember that `retain_coef` can equal `0`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "def get_lora_model_states(\n",
    "    model: AutoModelForCausalLM,\n",
    "    retain_inputs: dict,\n",
    "    cb_inputs: dict,\n",
    "    cb_layers: list[int],\n",
    "    retain_coef: float,\n",
    "    cb_coef: float,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Gets the representation states from the circuit-broken LoRA model.\n",
    "\n",
    "    Args:\n",
    "        model: the model\n",
    "        retain_inputs: dictionary of the inputs from the retain dataset passed\n",
    "            to the model.\n",
    "        cb_inputs: retain_inputs: dictionary of the input from the circuit\n",
    "            breaker dataset passed to the model.\n",
    "        cb_layers: the layers that the circuit breakers are applied to.\n",
    "        retain_coef: the coefficient of the retain loss.\n",
    "        cb_coef: the coefficient of the circuit breaker loss.\n",
    "\n",
    "    Returns [len(cb_layers), batch_size, seq_len, hidden_dim],\n",
    "        [n_layers + 1, batch_size, seq_len, hidden_dim]: tuple of tensors of\n",
    "        circuit breaker and retain states as from the LoRA model.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    if retain_coef > 0:\n",
    "        outputs = model(**retain_inputs)\n",
    "        retain_states_rr = torch.stack(outputs.hidden_states)\n",
    "    if cb_coef > 0:\n",
    "        outputs = model(**cb_inputs)\n",
    "        cb_states_rr = torch.stack([outputs.hidden_states[i] for i in cb_layers])\n",
    "    return cb_states_rr, retain_states_rr\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1d178c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lora_model_states(\n",
    "    model: AutoModelForCausalLM,\n",
    "    retain_inputs: dict,\n",
    "    cb_inputs: dict,\n",
    "    cb_layers: list[int],\n",
    "    retain_coef: float,\n",
    "    cb_coef: float,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    Gets the representation states from the circuit-broken LoRA model.\n",
    "\n",
    "    Args:\n",
    "        model: the model\n",
    "        retain_inputs: dictionary of the inputs from the retain dataset passed\n",
    "            to the model.\n",
    "        cb_inputs: retain_inputs: dictionary of the input from the circuit\n",
    "            breaker dataset passed to the model.\n",
    "        cb_layers: the layers that the circuit breakers are applied to.\n",
    "        retain_coef: the coefficient of the retain loss.\n",
    "        cb_coef: the coefficient of the circuit breaker loss.\n",
    "\n",
    "    Returns [len(cb_layers), batch_size, seq_len, hidden_dim],\n",
    "        [n_layers + 1, batch_size, seq_len, hidden_dim]: tuple of tensors of\n",
    "        circuit breaker and retain states as from the LoRA model.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f017a007",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.circuit_breakers.task2(get_lora_model_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5211b976",
   "metadata": {},
   "source": [
    "## Task 3: Calculating the Retain Loss\n",
    "\n",
    "Now that we have the states for both datasets on both models, we can start calculating the loss. First, we'll calculate the retain loss, which is the $L_2$ norm of the difference between the retain states from the original and representation rerouted models. \n",
    "A few considerations:\n",
    "- Make sure you take this norm across the correct dimension.\n",
    "- We'll then calculate the mean difference over all the norms taken. Before doing this, make sure you use the attention mask to zero out any unattended tokens\n",
    "- Ensure you calculate the mean over only the non-zero elements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924d01ca",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #3</b></summary>\n",
    "\n",
    "Use `torch.linalg.vector_norm()` to take the $L_2$ norm of the difference tensor.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #3</b></summary>\n",
    "\n",
    "Take the $L_2$ norm over the hidden dimension to collapse each \"vector\" into a\n",
    "single number.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #3</b></summary>\n",
    "\n",
    "The hidden dimension is `dim=-1`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #3</b></summary>\n",
    "\n",
    "Create a workable `retain_mask` with `retain_mask.repeat(num_hidden_states, 1, 1)`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #3</b></summary>\n",
    "\n",
    "Get your final value by dividing the sum of the $L_2$ norm difference by the sum of the attention mask.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "def calculate_retain_loss(\n",
    "    retain_states_rr: torch.Tensor,\n",
    "    retain_states_orig: torch.Tensor,\n",
    "    retain_mask: torch.Tensor,\n",
    "    num_hidden_states: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the retain loss portion of the LoRRA loss, based on the original\n",
    "    and LoRA models' representations on the retain dataset.\n",
    "\n",
    "    Args:\n",
    "        retain_states_rr [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            retain states from the representation-rerouted (LoRA) model.\n",
    "        retain_states_orig [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            retain states from the original (non-LoRA) model.\n",
    "        retain_mask [batch_size, seq_len]: the attention mask for the retain\n",
    "            inputs; used to zero-out unattended tokens.\n",
    "        num_hidden_states: the number of times to repeat the retain_mask so it\n",
    "            can be applied to the L_2-norm differences of tensors.\n",
    "\n",
    "    Returns: float of the retain loss.\n",
    "    \"\"\"\n",
    "    # the differences gives us (num_layers + 1, batch_size, seq_len, hidden_dim)\n",
    "    # we take the norm over hidden dim, giving us (num_layers + 1, batch_size, seq_len)\n",
    "    # think of this as we collapse all the difference vectors into a single norm\n",
    "    # then we take the mean over all these norms (all layers, batches, and seq positions)\n",
    "    norm_diff = torch.linalg.vector_norm(\n",
    "        retain_states_rr - retain_states_orig, ord=2, dim=-1\n",
    "    )\n",
    "    retain_attn_mask_layers = retain_mask.repeat(num_hidden_states, 1, 1)\n",
    "    masked_norm_diff = norm_diff * retain_attn_mask_layers\n",
    "    retain_loss = masked_norm_diff.sum() / retain_attn_mask_layers.sum()\n",
    "    return retain_loss\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90413ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_retain_loss(\n",
    "    retain_states_rr: torch.Tensor,\n",
    "    retain_states_orig: torch.Tensor,\n",
    "    retain_mask: torch.Tensor,\n",
    "    num_hidden_states: int,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the retain loss portion of the LoRRA loss, based on the original\n",
    "    and LoRA models' representations on the retain dataset.\n",
    "\n",
    "    Args:\n",
    "        retain_states_rr [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            retain states from the representation-rerouted (LoRA) model.\n",
    "        retain_states_orig [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            retain states from the original (non-LoRA) model.\n",
    "        retain_mask [batch_size, seq_len]: the attention mask for the retain\n",
    "            inputs; used to zero-out unattended tokens.\n",
    "        num_hidden_states: the number of times to repeat the retain_mask so it\n",
    "            can be applied to the L_2-norm differences of tensors.\n",
    "\n",
    "    Returns: float of the retain loss.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacebc29",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.circuit_breakers.task3(calculate_retain_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a3c4a6d",
   "metadata": {},
   "source": [
    "## Task 4: Calculating the Circuit Breaker Loss\n",
    "\n",
    "Next, we'll calculate the circuit breaker loss. This follows a very similar approach to Task 3, but instead of taking the $L_2$ norm, we take the cosine similarity and then ReLU to measure how \"similar\" the circuit broken representations are. (Recall that our ultimate goal is to minimize this similarity!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c78cab99",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #4</b></summary>\n",
    "\n",
    "Use `torch.nn.functional.cosine_similarity()` to take the cosine similarity between the two tensors.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #4</b></summary>\n",
    "\n",
    "Take the cosine similarity over the hidden dimension.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #4</b></summary>\n",
    "\n",
    "The hidden dimension is `dim=-1`.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üí° <b>Hint for Task #4</b></summary>\n",
    "\n",
    "Apply `torch.nn.functional.relu()` to the attention-masked similarity values, then finish the calculation using the same steps from Task 3.\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "def calculate_cb_loss(\n",
    "    cb_states_rr: torch.Tensor,\n",
    "    cb_states_orig: torch.Tensor,\n",
    "    cb_mask: torch.Tensor,\n",
    "    cb_layers: list[int],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the circuit breaker loss portion of the LoRRA loss, based on the\n",
    "    original and LoRA models' representations on the circuit breaker dataset.\n",
    "\n",
    "    Args:\n",
    "        cb_states_rr [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            cb states from the representation-rerouted (LoRA) model.\n",
    "        cb_states_orig [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            cb states from the original (non-LoRA) model.\n",
    "        cb_mask [batch_size, seq_len]: the attention mask for the cb\n",
    "            inputs; used to zero-out unattended tokens.\n",
    "        cb_layers: the number of times to repeat the retain_mask so it\n",
    "            can be applied to all circuit-broken layers.\n",
    "\n",
    "    Returns: float of the cb loss.\n",
    "    \"\"\"\n",
    "    # again, dim=-1 means we're taking the similarity over the hidden state\n",
    "    # vectors in each tensor\n",
    "    # gives us (num_layers + 1, batch_size, seq_len)\n",
    "    similarity = torch.nn.functional.cosine_similarity(\n",
    "        cb_states_orig, cb_states_rr, dim=-1\n",
    "    )\n",
    "\n",
    "    cb_attn_mask_layers = cb_mask.repeat(len(cb_layers), 1, 1)\n",
    "    masked_sim = similarity * cb_attn_mask_layers\n",
    "\n",
    "    # sum the ReLU, average over all the tokens\n",
    "    cb_loss = torch.nn.functional.relu(masked_sim).sum() / cb_attn_mask_layers.sum()\n",
    "    return cb_loss\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5741091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cb_loss(\n",
    "    cb_states_rr: torch.Tensor,\n",
    "    cb_states_orig: torch.Tensor,\n",
    "    cb_mask: torch.Tensor,\n",
    "    cb_layers: list[int],\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Calculates the circuit breaker loss portion of the LoRRA loss, based on the\n",
    "    original and LoRA models' representations on the circuit breaker dataset.\n",
    "\n",
    "    Args:\n",
    "        cb_states_rr [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            cb states from the representation-rerouted (LoRA) model.\n",
    "        cb_states_orig [n_layers + 1, batch_size, seq_len, hidden_dim]: the\n",
    "            cb states from the original (non-LoRA) model.\n",
    "        cb_mask [batch_size, seq_len]: the attention mask for the cb\n",
    "            inputs; used to zero-out unattended tokens.\n",
    "        cb_layers: the number of times to repeat the retain_mask so it\n",
    "            can be applied to all circuit-broken layers.\n",
    "\n",
    "    Returns: float of the cb loss.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76072d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.circuit_breakers.task4(calculate_cb_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3811a4f9",
   "metadata": {},
   "source": [
    "## Task 5: Calculating the LoRRA Loss\n",
    "\n",
    "Now that we have the retain and circuit breaker loss values, we can calculate the final loss. This should be a short exercise and fairly straightforward to implement given the algorithm above!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f980d08",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #5</b></summary>\n",
    "\n",
    "Remember that `retain_coef` can be `0`!\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #5</b></summary>\n",
    "\n",
    "```python\n",
    "def calculate_final_loss(\n",
    "    retain_loss: float, retain_coef: float, cb_loss: float, cb_coef: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the LoRRA loss based on the retain loss and coefficient as well as\n",
    "    the circuit breaker loss and coefficient.\n",
    "\n",
    "    Args:\n",
    "        retain_loss: the retain loss value.\n",
    "        retain_coef: the coefficient of the retain loss.\n",
    "        cb_loss: the circuit breaker loss value.\n",
    "        cb_coef: the circuit breaker coefficient.\n",
    "\n",
    "    Returns: the full LoRRA loss.\n",
    "    \"\"\"\n",
    "    if retain_coef == 0:\n",
    "        return cb_coef * cb_loss\n",
    "    return cb_coef * cb_loss + retain_coef * retain_loss\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b8c924",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_final_loss(\n",
    "    retain_loss: float, retain_coef: float, cb_loss: float, cb_coef: float\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the LoRRA loss based on the retain loss and coefficient as well as\n",
    "    the circuit breaker loss and coefficient.\n",
    "\n",
    "    Args:\n",
    "        retain_loss: the retain loss value.\n",
    "        retain_coef: the coefficient of the retain loss.\n",
    "        cb_loss: the circuit breaker loss value.\n",
    "        cb_coef: the circuit breaker coefficient.\n",
    "\n",
    "    Returns: the full LoRRA loss.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9298b749",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.circuit_breakers.task5(calculate_final_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc12bc2d",
   "metadata": {},
   "source": [
    "## Task 6: The Full `compute_loss()` Function.\n",
    "\n",
    "Finally, we'll put together the full `compute_loss()` function to train our circuit breaker LoRA model. This just amounts to calling all the previously-defined functions in order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d58688",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #6</b></summary>\n",
    "\n",
    "Call `get_orig_model_states()`, then `get_lora_model_states()`, then `calculate_retain_loss()`, then `calculate_cb_loss()`, then `calculate_final_loss()`.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #6</b></summary>\n",
    "\n",
    "```python\n",
    "def compute_loss(\n",
    "    self,\n",
    "    model: AutoModelForCausalLM,\n",
    "    inputs: dict[str, torch.Tensor],\n",
    "    cb_layers: list[int],\n",
    "    alpha: float,\n",
    "    **kwargs,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the low-rank representation adaptation (LoRRA) loss in a given\n",
    "    training step.\n",
    "\n",
    "    Args:\n",
    "        model: the model\n",
    "        inputs: dictionary of inputs, including the circuit breaker IDs, circuit\n",
    "            breaker mask, retain IDs, and retain mask.\n",
    "        cb_layers: the layers that the circuit breaking is applied to.\n",
    "        alpha: a hyperparameter.\n",
    "\n",
    "    Returns: the LoRRA loss.\n",
    "    \"\"\"\n",
    "    self.current_training_step += 1\n",
    "\n",
    "    cb_ids = inputs.get(\"input_ids_circuit_breaker\")\n",
    "    cb_mask = inputs.get(\"attention_mask_circuit_breaker\")\n",
    "    retain_ids = inputs.get(\"input_ids\")\n",
    "    retain_mask = inputs.get(\"attention_mask\")\n",
    "\n",
    "    cb_inputs = dict(\n",
    "        input_ids=cb_ids, attention_mask=cb_mask, output_hidden_states=True\n",
    "    )\n",
    "    retain_inputs = dict(\n",
    "        input_ids=retain_ids, attention_mask=retain_mask, output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    progress = self.get_progress()\n",
    "    retain_coef = alpha * progress\n",
    "    cb_coef = alpha * (1 - progress)\n",
    "\n",
    "    cb_states_orig, retain_states_orig = get_orig_model_states(\n",
    "        model=model,\n",
    "        retain_inputs=retain_inputs,\n",
    "        cb_inputs=cb_inputs,\n",
    "        cb_layers=cb_layers,\n",
    "        retain_coef=retain_coef,\n",
    "        cb_coef=cb_coef,\n",
    "    )\n",
    "    cb_states_rr, retain_states_rr = get_lora_model_states(\n",
    "        model=model,\n",
    "        retain_inputs=retain_inputs,\n",
    "        cb_inputs=cb_inputs,\n",
    "        cb_layers=cb_layers,\n",
    "        retain_coef=retain_coef,\n",
    "        cb_coef=cb_coef,\n",
    "    )\n",
    "\n",
    "    retain_loss = calculate_retain_loss(\n",
    "        retain_states_rr=retain_states_rr,\n",
    "        retain_states_orig=retain_states_orig,\n",
    "        retain_mask=retain_mask,\n",
    "        num_hidden_states=retain_states_rr.shape[0],\n",
    "    )\n",
    "    cb_loss = calculate_cb_loss(\n",
    "        cb_states_rr=cb_states_rr,\n",
    "        cb_states_orig=cb_states_orig,\n",
    "        cb_mask=cb_mask,\n",
    "        cb_layers=cb_layers,\n",
    "    )\n",
    "    return calculate_final_loss(\n",
    "        retain_loss=retain_loss,\n",
    "        cb_loss=cb_loss,\n",
    "        retain_coef=retain_coef,\n",
    "        cb_coef=cb_coef,\n",
    "    )\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da687b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(\n",
    "    self,\n",
    "    model: AutoModelForCausalLM,\n",
    "    inputs: dict[str, torch.Tensor],\n",
    "    cb_layers: list[int],\n",
    "    alpha: float,\n",
    "    **kwargs,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Computes the low-rank representation adaptation (LoRRA) loss in a given\n",
    "    training step.\n",
    "\n",
    "    Args:\n",
    "        model: the model\n",
    "        inputs: dictionary of inputs, including the circuit breaker IDs, circuit\n",
    "            breaker mask, retain IDs, and retain mask.\n",
    "        cb_layers: the layers that the circuit breaking is applied to.\n",
    "        alpha: a hyperparameter.\n",
    "\n",
    "    Returns: the LoRRA loss.\n",
    "    \"\"\"\n",
    "    self.current_training_step += 1\n",
    "\n",
    "    cb_ids = inputs.get(\"input_ids_circuit_breaker\")\n",
    "    cb_mask = inputs.get(\"attention_mask_circuit_breaker\")\n",
    "    retain_ids = inputs.get(\"input_ids\")\n",
    "    retain_mask = inputs.get(\"attention_mask\")\n",
    "\n",
    "    cb_inputs = dict(\n",
    "        input_ids=cb_ids, attention_mask=cb_mask, output_hidden_states=True\n",
    "    )\n",
    "    retain_inputs = dict(\n",
    "        input_ids=retain_ids, attention_mask=retain_mask, output_hidden_states=True\n",
    "    )\n",
    "\n",
    "    progress = self.get_progress()\n",
    "    retain_coef = alpha * progress\n",
    "    cb_coef = alpha * (1 - progress)\n",
    "\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2055ed68",
   "metadata": {},
   "source": [
    "Fantastic! You might've been wondering why our `compute_loss()` function has a `self` parameter. This is because we're going to create a custom HuggingFace `Trainer()` class to do our training (this is what the original implemention used as well, as it is the most straightforward way to perform LoRA training). The code below sets up our dataset and LoRA config that we'll use in our custom trainer. Although there is nothing conceptually challenging going on, we suggest you look at it to get a feel for how LoRA traininer works.\n",
    "\n",
    "You might also be wondering what the `CircuitBreakerDataset` class contains, i.e., what we'll be circuit breaking. Well, in line with the [purple problem](https://arxiv.org/abs/2403.14725), the `CircuitBreakerDataset` class contains some dataset tooling and a number of queries and responses related to the color purple as the circuit breaker dataset $D_\\text{CB}$ (which is not to be confused with the `CircuitBreakerDataset` itself) along with a harmless retain dataset $D_r$ (which is a subset of the dataset in `CircuitBreakerDataset`, disjoint from the circuit breaker dataset $D_\\text{CB}$). That is, $\\texttt{CircuitBreakerDataset} = \\{ \\texttt{tooling}, \\texttt{dataset} \\}$, and $\\texttt{dataset} = \\{D_\\text{CB}, D_r\\}$ such that $D_\\text{CB} \\cap D_r = \\varnothing$.\n",
    "\n",
    "Don't worry if that was confusing, as we did that intentionally. The main takeaway is that our model will be trained to circuit break on mentions of the color purple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82f568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our hyperparameter (this isn't very important).\n",
    "lorra_alpha = 10\n",
    "\n",
    "# These are the layers we'll circuit break.\n",
    "cb_layers = [7, 14]\n",
    "# We also list all the layers that LoRA will \"look at\", but we only actually\n",
    "# transform those above.\n",
    "transform_layers = [i for i in range(max(cb_layers) + 1)]\n",
    "drop_layers_after = max(cb_layers)\n",
    "\n",
    "# There is the LoRA config setup; the main takeaways are that we're training\n",
    "# rank-16 matrices and applying them to *all* modules in each layer.\n",
    "lora_r = 16\n",
    "lora_alpha = 16\n",
    "lora_dropout = 0.05\n",
    "target_modules = [\n",
    "    \"q_proj\",\n",
    "    \"k_proj\",\n",
    "    \"v_proj\",\n",
    "    \"o_proj\",\n",
    "    \"gate_proj\",\n",
    "    \"up_proj\",\n",
    "    \"down_proj\",\n",
    "]\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r=lora_r,\n",
    "    target_modules=target_modules,\n",
    "    lora_alpha=lora_alpha,\n",
    "    lora_dropout=lora_dropout,\n",
    "    bias=\"none\",\n",
    "    layers_to_transform=transform_layers,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL_PATH)\n",
    "# When training, we'll only use up to the last circuit breaker layer.\n",
    "config.num_hidden_layers = drop_layers_after + 1\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "tokenizer.pad_token = tokenizer.eos_token or tokenizer.unk_token\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, config=config).to(DEVICE)\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "model.enable_input_require_grads()\n",
    "model.config.use_cache = False\n",
    "xlab.jb_utils.initialize_lora_b_matrices(model)\n",
    "\n",
    "# We've defined the circuit breaker dataset class for you in the `xlab` package.\n",
    "train_dataset = xlab.jb_utils.CircuitBreakerDataset(\n",
    "    tokenizer=tokenizer,\n",
    "    num_examples=10000,\n",
    ")\n",
    "\n",
    "grad_accumulation_steps = 2\n",
    "train_args = TrainingArguments(\n",
    "    remove_unused_columns=False,\n",
    "    per_device_train_batch_size=8,\n",
    "    gradient_accumulation_steps=grad_accumulation_steps,\n",
    "    max_steps=150,\n",
    "    learning_rate=1e-4,\n",
    "    weight_decay=0.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12377cc",
   "metadata": {},
   "source": [
    "## Task 7: Defining the `CBTrainer()`\n",
    "\n",
    "As the final task, you'll define some of the missing parts of the `CBTrainer()` class. This involves very little code, but is a good exercise to ensure that you know how this custom trainer works! (You can ignore `num_items_in_batch` in the `self.compute_loss()` method signature.)\n",
    "\n",
    "<details>\n",
    "<summary>ü§î <b>Why is the denominator in get_progress() different from in the original algorithm? This is blasphemy.</b></summary>\n",
    "\n",
    "Great catch! The reason is actually somewhat deep, as the original circuit breakers implemention (in code) calculated the retain loss differently from what we do above. Recall that for us, the order was:\n",
    "1. Collect original model retain states.\n",
    "2. Collect LoRA model retain states.\n",
    "3. Take the norm of the difference between these states.\n",
    "4. Apply the attention mask.\n",
    "5. Return the mean.\n",
    "\n",
    "In the original paper, they swap steps 2 and 3, applying the attention mask *before* taking the difference and norm. In effect, this deflates the retain loss as we then take the norm over fewer nonzero values. Why did we do it our way? The main reason is to stay consistent with the way we calculate the circuit breaker loss, even if it makes a bit less sense in the context of the algorithm. Unfortunately, our way inversely increases the retain loss, so to combat this, we increase the denominator of the loss coefficients from $2T$ to $3T$ to place more emphasis on the circuit breaker loss and less on the retain loss. Empirically, we find little difference in the efficacy of the defense between these two approaches.\n",
    "\n",
    "Also of note in terms of implementational divergence is that we calculate cosine similarity much differently from the original implementation, which uses the mathematical definition\n",
    "$$\n",
    "\\text{cosine\\_sim}(r_{\\text{orig}}, \\ r_{\\text{CB}}) = \\frac{r_{\\text{orig}} \\cdot r_{\\text{CB}}}{\\left\\lVert r_{\\text{orig}} \\right\\lVert_2 \\left\\lVert r_{\\text{CB}} \\right\\lVert_2}\n",
    "$$\n",
    "instead of the torch function. Empirically, we again find little difference in the performance of both implementations, but find ours more interpretable.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd92af71",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>üí° <b>Hint for Task #7</b></summary>\n",
    "\n",
    "Make sure you define the attributes `self.current_training_step, self.lorra_alpha, and self.cb_layers`.\n",
    "\n",
    "</details>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #7</b></summary>\n",
    "\n",
    "```python\n",
    "class CBTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        args,\n",
    "        data_collator,\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        lorra_alpha,\n",
    "        cb_layers,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        self.current_training_step = 0\n",
    "        self.lorra_alpha = lorra_alpha\n",
    "        self.cb_layers = cb_layers\n",
    "\n",
    "    def get_progress(self):\n",
    "        return self.current_training_step / (\n",
    "            self.state.max_steps * grad_accumulation_steps * 3\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch, return_outputs=False):\n",
    "        return compute_loss(\n",
    "            self,\n",
    "            model=model,\n",
    "            inputs=inputs,\n",
    "            cb_layers=self.cb_layers,\n",
    "            alpha=self.lorra_alpha,\n",
    "            return_outputs=return_outputs,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        inputs = [{\"role\": \"user\", \"content\": \"can you describe the color purple?\"}]\n",
    "        xlab.get_cb_response(self.model, inputs, self.tokenizer)\n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ff2a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CBTrainer(Trainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        args,\n",
    "        data_collator,\n",
    "        train_dataset,\n",
    "        tokenizer,\n",
    "        lorra_alpha,\n",
    "        cb_layers,\n",
    "    ):\n",
    "        super().__init__(\n",
    "            model=model,\n",
    "            args=args,\n",
    "            data_collator=data_collator,\n",
    "            train_dataset=train_dataset,\n",
    "            tokenizer=tokenizer,\n",
    "        )\n",
    "        # === YOUR ATTRIBUTES HERE ===\n",
    "\n",
    "    def get_progress(self):\n",
    "        return self.current_training_step / (\n",
    "            self.state.max_steps * grad_accumulation_steps * 3\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, model, inputs, num_items_in_batch, return_outputs=False):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def evaluate(self):\n",
    "        self.model.eval()\n",
    "        inputs = [{\"role\": \"user\", \"content\": \"can you describe the color purple?\"}]\n",
    "        xlab.get_cb_response(self.model, inputs, self.tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9fecb9",
   "metadata": {},
   "source": [
    "Now we're ready to train our model. Unfortunately, this part is probably infeasible if you aren't on a GPU. Training on an A100 with the above setup takes roughly 22 minutes, and that translates to up to (and maybe even more than) a day of CPU time. For those who cannor or do not want to run this on a GPU, in lieu of an actual training run, we've selected some wonderful responses from the final model we trained so you can see how the resultant circuit-broken model behaves.\n",
    "\n",
    "```\n",
    "<|user|>\n",
    "Are grapes normally purple\n",
    "<|assistant|>\n",
    "besides besides UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK UK...\n",
    "\n",
    "---\n",
    "\n",
    "<|user|>\n",
    "How can I learn Java in 2025?\n",
    "<|assistant|>\n",
    "To learn Java in 2025, you can follow the following steps:\n",
    "\n",
    "1. Start with the basics: The first step is to learn the basics of Java programming. This includes learning the syntax, data types...\n",
    "\n",
    "---\n",
    "\n",
    "<|user|>\n",
    "I've always wanted a purple umbrella. Where can I buy one?\n",
    "<|assistant|>\n",
    "th th th th th th th th th thuesleep th th th th th th th th th th th th th th th th th th th th th th th th th th th th thues th th th th th\n",
    "\n",
    "---\n",
    "\n",
    "<|user|>\n",
    "I've always wanted a green umbrella. Where can I buy one?\n",
    "<|assistant|>\n",
    "There are several places where you can buy a green umbrella:\n",
    "\n",
    "1. Online retailers: You can find green umbrellas online at retailers like Amazon, Walmart, and Target. Some popular brands include Osprey, Osprey Umbrella, and Osprey Umbrella Co.\n",
    "\n",
    "2. Local shops: You can also...\n",
    "```\n",
    "\n",
    "The model actually does a fairly good job at differentiating purple and other colors, as exhibited by the latter 2 examples. If you want to train a circuit-broken model yourself, the code to start training is below, but once again be warned that this should really only be done on a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c2527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = CBTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    data_collator=default_data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    lorra_alpha=lorra_alpha,\n",
    "    cb_layers=cb_layers,\n",
    ")\n",
    "print(\"running trainer\")\n",
    "trainer.train()\n",
    "print(\"training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d04de0",
   "metadata": {},
   "source": [
    "If you were able to train the model, feel free to test it out below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc62e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    xlab.jb_utils.get_cb_response(\n",
    "        model, \"What steps can I take to learn python?\", tokenizer\n",
    "    )\n",
    ")\n",
    "\n",
    "print(\n",
    "    xlab.jb_utils.get_cb_response(\n",
    "        model, \"How can I make my wedding purple themed?\", tokenizer\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
