{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be5fd4be",
   "metadata": {},
   "source": [
    "# Running GPT-2 Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7181e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xlab\n",
    "\n",
    "VOCAB_SIZE = 50257"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8deb96b-172e-46cb-a723-62654fa4aadd",
   "metadata": {},
   "source": [
    "When loading pretrained models in the course, we will be using the [transformers](https://huggingface.co/docs/transformers/en/index) library developed by Hugging Face. This library abstracts away the complexity of running pretrained language models into a single API. In the cell below, you will be loading the [smallest version of GPT-2](https://huggingface.co/openai-community/gpt2) with 124 million parameters. If you are interested, you can also try out [GPT-2 Medium](https://huggingface.co/openai-community/gpt2-medium) (335 million parameters), [GPT-2 Large](https://huggingface.co/openai-community/gpt2-large) (774 million parameters), and [GPT-2 XL](https://huggingface.co/openai-community/gpt2-xl) (1.5 billion parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99ed73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74ba43b-82df-4a27-b430-34bbe5284d8e",
   "metadata": {},
   "source": [
    "## Disabling Gradients\n",
    "\n",
    "This notebook will teach you how to load an LLM and calculate loss on examples. We will not be going through how we would train or fine-tune an LLM which means you will not be running backward passes to update parameters based on the loss. In other words, none of the computations in this notebook require PyTorch to track gradients. By default, however, PyTorch will track computations (when tensors require gradients) to prepare for a backward pass. To stop this from happening, we will have to tell PyTorch explicitly.\n",
    "\n",
    "One way to do this for specific computations is to use `torch.no_grad()` for computations that aren't used for gradient updates. For example:\n",
    "\n",
    "```\n",
    "with torch.no_grad():\n",
    "    # your computations here\n",
    "```\n",
    "\n",
    "Because no exercises you will be completing require gradients, you can disable gradient tracking for the remainder of the notebook by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a269e9-2718-4348-a564-ba59ed64baa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a049617",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec08f7",
   "metadata": {},
   "source": [
    "To input a sequence of text into GPT-2, we first have to decide to convert the text to numbers so we can feed it to the model. Typically, how this is done is we convert a string of text into sequence of tokens, each of which will be assigned a number which can be embedded into a vector. To do this, we have a few options:\n",
    "\n",
    "1. We can assign each character its own number\n",
    "2. We can assign each word or special character its own number\n",
    "3. We can assign common sequences of characters their own numbers\n",
    "\n",
    "Option #3 is most popular and the high-level approach taken in the GPT-2 paper. This approach has the advantage of having a smaller total number of tokens while still capturing some of the underlying structure of natural language. Specifically, the authors use a modified version of BPE (byte pair encoding) proposed [here](https://arxiv.org/pdf/1508.07909). If you are interested, more implementation details of the tokenizer can be found in the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). \n",
    "\n",
    "Time to try out the GPT-2 tokenizer! Run the cell below to see the tokenizer assign the string into a sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df31e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Barack Obama taught constitutional law at the University of\"\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "print(encoded_input[\"input_ids\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c9ea0",
   "metadata": {},
   "source": [
    "Let's take a look at what each of these numbers represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d8722f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token_id in encoded_input[\"input_ids\"][0]:\n",
    "    print(f'{token_id.item()}\\t --> \\t\"{tokenizer.decode(token_id)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87999fe0-982e-4986-85d9-febacf19c317",
   "metadata": {},
   "source": [
    "We can also decode the entire sequence at once. This will be helpful to remember for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf30775-76e9-4703-b14b-7662375bb3f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(encoded_input[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e5390",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "For a given input of text, return a list of tokens in plain text. For example for the input \"Hello there gpt-2!\", the function should return ['Hello', ' there', ' g', 'pt', '-', '2', '!']. Note that this is very different than just splitting up the text into random chunks or where there are spaces! Tokenizers are designed to create groupings of characters that are often found together or that are significant in the structure of language. You are encouraged to play around with different examples and observe how smart the tokenizer can be!\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1</b></summary>\n",
    "\n",
    "```python\n",
    "def plain_text_tokens(prefix):\n",
    "    \"\"\"Tokenizes a text prefix into individual token strings.\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): Input text string to tokenize.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: Individual token strings from the tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "\n",
    "    # 1. iterate over input ids\n",
    "    for i in encoded_input['input_ids'][0]:\n",
    "\n",
    "        # 2. decode each id into plain text\n",
    "        rv.append(tokenizer.decode(i))\n",
    "    return rv\n",
    "  \n",
    "```\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a39181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plain_text_tokens(prefix):\n",
    "    \"\"\"Tokenizes a text prefix into individual token strings.\n",
    "\n",
    "    Args:\n",
    "        prefix (str): Input text string to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        list[str]: Individual token strings from the tokenizer.\n",
    "    \"\"\"\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE #########\n",
    "    # 1. iterate over input ids\n",
    "    # 2. decode each id into plain text\n",
    "    ########## YOUR CODE ENDS HERE ##########\n",
    "\n",
    "\n",
    "# test out your implementation on different inputs to get a sense of how the tokenizer works!\n",
    "print(plain_text_tokens(\"Hello there gpt-2!\"))\n",
    "print(plain_text_tokens(\"https://xrisk.uchicago.edu/fellowship/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa46d5-d0ab-4c46-8baf-8d6970b3ff1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.tests.gpt2.task1(plain_text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d653de",
   "metadata": {},
   "source": [
    "Back to our model. We will tokenize our text into numbers to feed into the model. When the model is done predicting text, we can detokenize the results to see if what the model is saying makes sense.\n",
    "\n",
    "Below is code for a single forward pass with the prefix \"Barack Obama taught constitutional law at the University of\"\n",
    "\n",
    "Take a look at the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b9c3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Barack Obama taught constitutional law at the University of\"\n",
    "encoded_input = tokenizer(prefix, return_tensors=\"pt\")\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45aa34f",
   "metadata": {},
   "source": [
    "Let's take another look at the logit values. Note that logits can be positive or negative. Normally, both in training and in inference, we apply a \"softmax\" function to the data to bring all values between 0 and 1. We interpret these values as the probability that the model assigns each token to be next in a sequence of text. For now, however, we ignore this detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a4134f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(logits.view(-1)), min(logits.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7677a38",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input[\"input_ids\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9797f05",
   "metadata": {},
   "source": [
    "What is going on here?\n",
    "\n",
    "`torch.Size([1, 10, 50257])` tells us that the logits are a 3-dimensional array (i.e., it is `1*10*50257`). The first dimension represents the batch size and because there is only one batch, we can ignore it for now. The next dimension is the sequence length. Note that:\n",
    "\n",
    "```python\n",
    ">>> encoded_input['input_ids']\n",
    "tensor([[10374,   441,  2486,  7817,  9758,  1099,   379,   262,  2059,   286]])\n",
    "```\n",
    "\n",
    "There are 10 tokens when we tokenize \"Barack Obama taught constitutional law at the University of\" meaning the sequence length is 10. For the final dimension, we have 50257 which represents the model's vocabulary size. Why do we have so much data? Don't we only want the next predicted piece of text?\n",
    "\n",
    "To understand why this is necessary, you will need to understand what information is included in this tensor. In total, we have 10 vectors of length 50257. Each vector represents a probability distribution for each token in the vocabulary. For example, if the value at 42 is higher, that means that the model is assigning a higher probability to the token at position 42 to be the next in the sequence of text.\n",
    "\n",
    "This makes sense for our purposes: if we have a probability distribution for the next token in the text, we can sample from it to predict the next token! But why do we have 10 probability distributions in the output? In other words why do we need a probability distribution for each token in the input?\n",
    "\n",
    "The answer to this question is oddly, that this makes it easier to train our model! If we have a piece of text that we are training on from the internet, we can train multiple examples in parallel. For example, if we have the text \"Barack Obama taught constitutional law at the University of\" here are several different examples we could choose to train on. \n",
    "\n",
    "\n",
    "1. Prefix=\"Bar\" and label=\"ack\"\n",
    "2. Prefix=\"Barack\" and label=\" Obama\"\n",
    "3. Prefix=\"Barack Obama\" and label=\" taught\"\n",
    "4. And so on...\n",
    "\n",
    "\n",
    "The first three vectors in the logits in the code above correspond to the model's predictions for the first three prefixes above. While running inference, we only care about the model's label for the input \"Barack Obama taught constitutional law at the University of\". Therefore, we only need to extract the final vector from the probability distribution. This makes sense because for our purposes, we aren't interested in efficiently training the model. We are only interested in seeing what the model predicts for the next token.\n",
    "\n",
    "We can extract this last vector by taking `logits[0][-1]`. Let's see what the model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f744c0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Barack Obama taught constitutional law at the University of\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors=\"pt\")\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "generated_text = tokenizer.decode([token_id.item()])\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eafea5-7d9e-4159-af5e-daa91f6ef9e6",
   "metadata": {},
   "source": [
    "Indeed, Barack Obama taught constitutional law at the University of Chicago ([source](https://www.obamalibrary.gov/obamas/president-barack-obama))! Despite being an early model with limited capabilities, GPT-2 124M knows quite a bit about the world."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82727986-5299-4d50-bdc6-bb4e1113e837",
   "metadata": {},
   "source": [
    "## Task #1.5\n",
    "\n",
    "In the function below you will run the code above in a loop to continue generating text from the model. You are encouraged to play around with different prompts and observe the model's output. \n",
    "\n",
    "Because you are using `argmax` to sample the next token, the model's behavior is deterministic. Later, you will see that in typical LLM generation, next tokens are sampled from a probability distribution.\n",
    "\n",
    "<i>`generate_n_tokens` should return generated tokens appended to the original prompt.</i>\n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #1.5</b></summary>\n",
    "\n",
    "```python\n",
    "def generate_n_tokens(model, tokenizer, n, prompt):\n",
    "    \"\"\"Generates n tokens by repeatedly predicting the most likely next token.\n",
    "\n",
    "    generate_n_tokens should return generated tokens appended to the original prompt.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        tokenizer: Tokenizer with encode/decode methods.\n",
    "        n (int): Number of tokens to generate.\n",
    "        prompt (str): Initial text prompt to extend.\n",
    "        \n",
    "    Returns:\n",
    "        str: Original prompt extended with n generated tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. iterate n times\n",
    "    for i in range(n):\n",
    "\n",
    "        # 2. find most likely next token\n",
    "        encoded_input = tokenizer(prompt, return_tensors='pt')\n",
    "        output = model(**encoded_input)\n",
    "        logits = output.logits\n",
    "        token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "        # 3. append it to the prompt + previously generated tokens\n",
    "        generated_text = tokenizer.decode([token_id.item()])  \n",
    "        prompt+=generated_text\n",
    "    \n",
    "    return prompt\n",
    "  \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d24634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_n_tokens(model, tokenizer, n, prompt):\n",
    "    \"\"\"Generates n tokens by repeatedly predicting the most likely next token.\n",
    "\n",
    "    generate_n_tokens should return generated tokens appended to the original prompt.\n",
    "\n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        tokenizer: Tokenizer with encode/decode methods.\n",
    "        n (int): Number of tokens to generate.\n",
    "        prompt (str): Initial text prompt to extend.\n",
    "\n",
    "    Returns:\n",
    "        str: Original prompt extended with n generated tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE #########\n",
    "    # 1. iterate n times\n",
    "    # 2. find most likely next token\n",
    "    # 3. append it to the prompt + previously generated tokens\n",
    "    ########## YOUR CODE ENDS HERE ##########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe611198",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text = generate_n_tokens(model, tokenizer, 50, text)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2491db57-d738-4c37-b6c7-60f8451bcc74",
   "metadata": {},
   "source": [
    "As you will explore below, running an LLM in practice typically involves sampling tokens by interpreting the model output as a probability distribution. This means that running an LLM should be non-deterministic (i.e., running the same prompt multiple times should produce different outputs). The function you wrote above, however, should not include any sampling, so it should produce the exact same output every time you run it. To test you implementation you can compare your output to the expected output when you run the solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1432b7-1a85-4e6f-ab66-88cef97706a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_output = \"Barack Obama taught constitutional law at the University of Chicago. He was a founding member of the American Bar Association, and he was a founding member of the American Bar Association's Board of Trustees. He was a founding member of the American Bar Association's Board of Trustees. He was a founding\"\n",
    "generated_text = generate_n_tokens(model, tokenizer, 50, text)\n",
    "assert generated_text == expected_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9efd42",
   "metadata": {},
   "source": [
    "### Why is the model repeating itself?\n",
    "\n",
    "In the previous cell, we generated text by always choosing the most likely next token (using `torch.argmax`). This deterministic approach has a major drawback: once the model enters a pattern that has high probability, it can get stuck in a loop.\n",
    "\n",
    "Above you should have observed that \"He was a founding member of the American Bar Association's Board of Trustees.\" repeats continuously before we cut it off.\n",
    "\n",
    "### Introducing Softmax and Temperature\n",
    "\n",
    "We explored softmax briefly in the [defensive distillation section](https://xlabaisecurity.com/adversarial/defensive-distillation/).\n",
    "\n",
    "As a review, a traditional softmax is calculated via the following equation. When there are $K$ classes, and $z$ is the pre-softmax output, the equation below gives the probability for class $i$:\n",
    "\n",
    "$$\n",
    "q_i = \\frac{e^{z_i}}{\\sum_{j=0}^{K-1}e^{z_j}}\n",
    "$$\n",
    "\n",
    "For our transformer output, we would take the softmax over each channel dimension. That means that for each input token, we would take a softmax for the corresponding output token independently. For inference, we would be sampling from the softmax probabilities for the final output token.\n",
    "\n",
    "If we want the output of the softmax to be smoother, we can add a constant $T$ which forces the distribution to be more uniform. The value for $T$ is called temperature, where a higher $T$ corresponds to more random or surprising outputs. Note that our traditional softmax is equivalent to the equation below when $T = 1$.\n",
    "\n",
    "$$\n",
    "q_i = \\frac{e^{z_i / T}}{\\sum_{j=0}^{K-1}e^{z_j / T }}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45341914-113b-4d45-8179-059d5b3e72d7",
   "metadata": {},
   "source": [
    "### Task #2: Softmax GPT-2 Outputs\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #2</b></summary>\n",
    "\n",
    "```python\n",
    "def get_gpt2_probs(logits, temp=1):\n",
    "    \"\"\"Converts logits to probabilities with optional temperature scaling.\n",
    "    \n",
    "    Args:\n",
    "        logits [batch, seq_len, vocab_size]: Raw model output logits.\n",
    "        temp (float): Temperature for scaling logits before softmax.\n",
    "        \n",
    "    Returns:\n",
    "        [batch, seq_len, vocab_size]: Probability distributions over vocabulary.\n",
    "    \"\"\"\n",
    "    assert len(logits.shape) == 3\n",
    "    assert logits.shape[-1] == VOCAB_SIZE\n",
    "\n",
    "    logits_with_temp = logits / temp\n",
    "    probs = torch.nn.functional.softmax(logits_with_temp, dim=2)\n",
    "    return probs\n",
    "  \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d21f3d-9ea2-4d12-8579-4b3430030e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_probs(logits, temp=1):\n",
    "    \"\"\"Converts logits to probabilities with optional temperature scaling.\n",
    "\n",
    "    Args:\n",
    "        logits [batch, seq_len, vocab_size]: Raw model output logits.\n",
    "        temp (float): Temperature for scaling logits before softmax.\n",
    "\n",
    "    Returns:\n",
    "        [batch, seq_len, vocab_size]: Probability distributions over vocabulary.\n",
    "    \"\"\"\n",
    "    assert len(logits.shape) == 3\n",
    "    assert logits.shape[-1] == VOCAB_SIZE\n",
    "\n",
    "    ######### YOUR CODE HERE #########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6d930-3011-4abe-a9d6-7103703dabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "xlab.tests.gpt2.task2(get_gpt2_probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4bbc45-1dca-415c-8af8-47d3d638ab7b",
   "metadata": {},
   "source": [
    "### Effects of different temperature values:\n",
    "\n",
    "- **T = 0** (or very close to 0): Completely deterministic, always pick highest probability token (like we did before)\n",
    "- **T = 1.0**: Standard softmax, use the exact probabilities from the model\n",
    "- **T > 1.0**: More uniform distribution, increasing randomness and diversity\n",
    "- **T < 1.0**: More peaked distribution, reducing randomness but still allowing some\n",
    "\n",
    "Lower temperatures produce more focused, coherent text but risk repetition. Higher temperatures produce more diverse, creative text but risk incoherence.\n",
    "\n",
    "In the next cell, we'll implement temperature sampling to fix our repetition problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4c76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=100, temperature=0.7\n",
    "):\n",
    "    \"\"\"Generates text using temperature-based sampling for next token selection.\n",
    "\n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        tokenizer: Tokenizer with encode/decode methods.\n",
    "        prompt (str): Initial text prompt to extend.\n",
    "        max_length (int): Maximum number of tokens to generate.\n",
    "        temperature (float): Temperature parameter for sampling control.\n",
    "\n",
    "    Returns:\n",
    "        str: Original prompt extended with generated tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    for i in range(max_length):\n",
    "        encoded_input = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        output = model(**encoded_input)\n",
    "        probs = get_gpt2_probs(output.logits, temperature)\n",
    "\n",
    "        next_token_id = torch.multinomial(probs[0][-1], num_samples=1).item()\n",
    "\n",
    "        generated_text = tokenizer.decode([next_token_id])\n",
    "        prompt += generated_text\n",
    "\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4572f486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Low temperature (more deterministic but not completely)\n",
    "prompt = \"Barack Obama taught constitutional law at the University of\"\n",
    "low_temp_text = generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=40, temperature=0.3\n",
    ")\n",
    "print(\"Temperature = 0.3:\")\n",
    "print(\"-\" * 50)\n",
    "print(low_temp_text + \"\\n\")\n",
    "\n",
    "\n",
    "# Medium temperature (balanced)\n",
    "medium_temp_text = generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=40, temperature=0.7\n",
    ")\n",
    "print(\"Temperature = 0.7:\")\n",
    "print(\"-\" * 50)\n",
    "print(medium_temp_text + \"\\n\")\n",
    "\n",
    "\n",
    "# High temperature (more random)\n",
    "high_temp_text = generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=40, temperature=1.2\n",
    ")\n",
    "print(\"Temperature = 1.2:\")\n",
    "print(\"-\" * 50)\n",
    "print(high_temp_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f0a7b3-9130-4715-8949-4310f698c3fa",
   "metadata": {},
   "source": [
    "### Calculate Loss:\n",
    "\n",
    "The accuracy of language modeling is typically measured with \"negative log likelihood\" (NLL). Let's break down where each of these words come from:\n",
    "\n",
    "1. Likelihood: The softmax probabilities for the next token give the probability the model assigns to each of the outputs. If you select the probability of the correct next token you have the probability the model assigns to the correct answer.\n",
    "2. Log likelihood: the softmax probability for the correct next token will give you some value between 0 and 1 (this is a property of the softmax). By taking the log of that value, you get near-zero if the probability is close to one. Otherwise, you get increasingly negative values the closer the probability is to 0.\n",
    "3. Negative log likelihood: By taking the negative of the log likelihood, we get increasingly positive values for probabilities close to 0 and less positive values for probabilities close to 1.\n",
    "\n",
    "One way to think about this which you may find helpful is *minimizing* NLL should *maximize* the probability the model assigns to the correct token. For more information, review our [Introduction to LLMs](https://xlabaisecurity.com/jailbreaking/llm-intro/) page on our website."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6c227-c148-4d65-9997-312c8400b45a",
   "metadata": {},
   "source": [
    "### Task #3 & 4: Calculate Loss for GPT-2\n",
    "\n",
    "For the sake of the following exercise, let's pretend that the text \"Barack Obama taught constitutional law at the University of Chicago\" is in our training data and we would like to predict it.\n",
    "\n",
    "In task #3 you will calculate the NLL for only the final token where the \"correct\" answer is \" Chicago\".\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #3</b></summary>\n",
    "\n",
    "```python\n",
    "def get_gpt2_next_token_loss(model, text, correct_token_idx):\n",
    "    \"\"\"Computes cross-entropy loss for predicting a specific next token.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        text (str): Input text sequence.\n",
    "        correct_token_idx [1]: Token indices tensor for the target next token.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    logits = model(**encoded_input).logits\n",
    "    next_token_out = logits[:,-1,:]\n",
    "    loss = torch.nn.functional.cross_entropy(next_token_out, correct_token_idx)\n",
    "    return loss\n",
    "  \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943ec078-5a95-485c-93b4-2b1c7e4a89ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_next_token_loss(model, text, correct_token_idx):\n",
    "    \"\"\"Computes cross-entropy loss for predicting a specific next token.\n",
    "\n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        text (str): Input text sequence.\n",
    "        correct_token_idx [1]: Token indices tensor for the target next token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar cross-entropy loss value.\n",
    "    \"\"\"\n",
    "    ######### YOUR CODE HERE #########\n",
    "\n",
    "\n",
    "# Note: \" Chicago\" tokenizes to a single token in GPT-2\n",
    "# Therefore this line gives us a tensor with shape [1]\n",
    "correct_token_idx = tokenizer(\" Chicago\", return_tensors=\"pt\")[\"input_ids\"][0]\n",
    "get_gpt2_next_token_loss(\n",
    "    model,\n",
    "    \"Barack Obama taught constitutional law at the University of\",\n",
    "    correct_token_idx,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16181c99-e9ae-42e9-b424-a1a2e819e2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.gpt2.task3(get_gpt2_next_token_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6bc248-3c6e-434a-be17-d0997fe79355",
   "metadata": {},
   "source": [
    "In the previous function, you implemented the NLL loss for the final output token of the model. However, when training these models, you will actually calculate loss for every token. As a review, GPT-2 treats the following as unique training examples:\n",
    "\n",
    "1. Prefix=\"Bar\" and label=\"ack\"\n",
    "2. Prefix=\"Barack\" and label=\" Obama\"\n",
    "3. Prefix=\"Barack Obama\" and label=\" taught\"\n",
    "4. And so on...\n",
    "\n",
    "Although we won't explain the mechanisms in depth, the transformer architecture ensures that tokens can only \"look\" at tokens before them, meaning the model cannot cheat and look ahead at the correct answer.\n",
    "\n",
    "For task #4 you will implement a loss given a sequence of text. You will use the text itself as self-supervised labels the way that researchers would when training a transformer from scratch. This means that you will use the text itself to derive the \"correct\" labels for each example. \n",
    "\n",
    "\n",
    "<details>\n",
    "<summary>üîê <b>Solution for Task #4</b></summary>\n",
    "\n",
    "```python\n",
    "def get_gpt2_loss_on_sequence(model, text):\n",
    "    \"\"\"Computes language modeling loss for predicting each token in a sequence.\n",
    "    \n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        text (str): Input text sequence for loss computation.\n",
    "        \n",
    "    Returns:\n",
    "        torch.Tensor: Scalar cross-entropy loss across the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. get logits\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    logits = model(**encoded_input).logits\n",
    "\n",
    "    # 2. remove final prediction from logits (we don't have a self-supervised label for it)\n",
    "    logits = logits[:, :-1, :]\n",
    "\n",
    "\n",
    "    # 3. remove first token from labels (the model doesn't produce a prediction for it)\n",
    "    labels = encoded_input.input_ids[:,1:]\n",
    "\n",
    "    # 4. calculate cross entropy loss\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        logits.squeeze(0), # remove batch dim\n",
    "        labels.squeeze(0)  # remove batch dim\n",
    "    )\n",
    "    return loss\n",
    "  \n",
    "```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbb7a4c-d2b7-470e-94ef-88e6b76dcc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpt2_loss_on_sequence(model, text):\n",
    "    \"\"\"Computes language modeling loss for predicting each token in a sequence.\n",
    "\n",
    "    Args:\n",
    "        model: Language model with forward() method that outputs logits.\n",
    "        text (str): Input text sequence for loss computation.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar cross-entropy loss across the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    ######### YOUR CODE STARTS HERE #########\n",
    "    # 1. get logits\n",
    "    # 2. remove final prediction from logits (we don't have a self-supervised label for it)\n",
    "    # 3. remove first token from labels (the model doesn't produce a prediction for it)\n",
    "    # 4. calculate cross entropy loss\n",
    "    ########## YOUR CODE ENDS HERE ##########\n",
    "\n",
    "\n",
    "get_gpt2_loss_on_sequence(\n",
    "    model, \"Barack Obama taught constitutional law at the University of Chicago\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8b952-e7d6-459b-89d8-3d4e36d94d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = xlab.tests.gpt2.task4(get_gpt2_loss_on_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a94e65-340c-42a0-ab47-a9288001f907",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
