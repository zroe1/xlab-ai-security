{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bec4bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # 1. Collect two datasets\n",
    "    # 2. Generate candidate mean activations for each(for each token position, and for every layer, across each data point)\n",
    "    # 3. Generate candidate vectors 1/dataset_size*(difference between harmful and harmless prompt for every layer that is num_hidden_layers, positions long, and dimensions of model, 4d tensor)\n",
    "    # 4. choose the best vector through the algorithm (doesn't have high KL divergene)\n",
    "    # 5. Ablate, add, and test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad10051",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import contextlib\n",
    "from typing import List, Dict, Tuple\n",
    "from jaxtyping import Float\n",
    "from torch import Tensor\n",
    "\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "\n",
    "from peft import PeftModel\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0e8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tinyllama(base_model_path, fine_tuned_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.padding_side = \"left\"\n",
    "\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(base_model_path)\n",
    "    model = PeftModel.from_pretrained(base_model, fine_tuned_path)\n",
    "\n",
    "    model.requires_grad_(False) \n",
    "    return model, tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0370e2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = load_tinyllama(\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\", \"uchicago-xlab-ai-security/refuse_harmful_v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a51fa1b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n"
     ]
    }
   ],
   "source": [
    "block_modules = model.base_model.model.model.layers\n",
    "n_layers = len(block_modules)\n",
    "print(n_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "537e5a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# template from https://ollama.com/library/tinyllama:1.1b/blobs/af0ddbdaaa26\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful and harmless assistant.\"\n",
    "CHAT_TEMPLATE = (\n",
    "    \"<|system|>\\n\" + SYSTEM_PROMPT + \"</s>\\n\"\n",
    "    \"<|user|>\\n{instruction}</s>\\n\"\n",
    "    \"<|assistant|>\"\n",
    ")\n",
    "\n",
    "eoi_toks = tokenizer.encode(CHAT_TEMPLATE.split(\"{instruction}\")[-1], add_special_tokens=False)\n",
    "positions = list(range(-len(eoi_toks), 0))   # all post-instruction tokens; or positions=[-1]\n",
    "\n",
    "\n",
    "def tokenize_instructions_fn(instructions):\n",
    "    prompts = [CHAT_TEMPLATE.format(instruction=s) for s in instructions]\n",
    "    return tokenizer(prompts, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb4447d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 29871, 13, 29966, 29989, 465, 22137, 29989, 29958]\n",
      "</s> \n",
      "<|assistant|>\n"
     ]
    }
   ],
   "source": [
    "print(eoi_toks)\n",
    "print(tokenizer.decode(eoi_toks))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfad2b3",
   "metadata": {},
   "source": [
    "Ideally, first, we would filter the datasets to ensure that the helpful/harmless split corresponds to which prompts the model is actually refusing.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080d8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset_split(harmtype: str, split: str,):   \n",
    "    assert harmtype in ['harmless', 'harmful']\n",
    "    assert split in ['train', 'val', 'test']\n",
    "    url = f\"https://github.com/andyrdt/refusal_direction/blob/main/dataset/splits/{harmtype}_{split}.json\"\n",
    "    response = requests.get(url)\n",
    "    dataset = response.json\n",
    "\n",
    "    dataset = [d['instruction'] for d in dataset]\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78bce3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_instructions(\n",
    "    tokenizer: AutoTokenizer,\n",
    "    instructions: List[str]\n",
    ") -> Tensor:\n",
    "    \"\"\"Tokenize instructions using tokenizer's chat template\"\"\"\n",
    "    \n",
    "    conversations = [[{\"role\": \"user\", \"content\": instruction}] for instruction in instructions]\n",
    "    \n",
    "    prompts = [\n",
    "        tokenizer.apply_chat_template(conv, tokenize=False, add_generation_prompt=False)\n",
    "        for conv in conversations\n",
    "    ]\n",
    "    \n",
    "    return tokenizer(prompts, padding=True, truncation=False, return_tensors=\"pt\").input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df7ad1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def add_hooks(module_forward_pre_hooks, module_forward_hooks):\n",
    "    handles = []\n",
    "    try:\n",
    "        for mod, fn in module_forward_pre_hooks:\n",
    "            handles.append(mod.register_forward_pre_hook(fn, with_kwargs=False))\n",
    "        for mod, fn in module_forward_hooks:\n",
    "            handles.append(mod.register_forward_hook(fn, with_kwargs=False))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles: h.remove()\n",
    "\n",
    "def get_mean_activations_pre_hook(layer, cache, n_samples, positions):\n",
    "    def hook_fn(module, inputs):\n",
    "        h = inputs[0].to(cache.dtype)                # (batch, seq_length, d_model)\n",
    "        cache[:, layer] += (h[:, positions, :].sum(dim=0) / n_samples)\n",
    "    return hook_fn\n",
    "\n",
    "def get_mean_activations(model, instructions, batch_size=32, positions=[-1]):\n",
    "    n_pos = len(positions)\n",
    "    d_model = model.config.hidden_size\n",
    "    n_layers = model.config.num_hidden_layers\n",
    "    n_samples = len(instructions)\n",
    "\n",
    "    cache = torch.zeros((n_pos, n_layers, d_model), dtype=torch.float64, device=model.device)\n",
    "    pre_hooks = [(block_modules[i], get_mean_activations_pre_hook(i, cache, n_samples, positions))\n",
    "                 for i in range(n_layers)]\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for i in tqdm(range(0, n_samples, batch_size)):\n",
    "            batch = tokenize_instructions_fn(instructions[i:i+batch_size])\n",
    "            with add_hooks(pre_hooks, []):\n",
    "                model(input_ids=batch.input_ids.to(model.device),\n",
    "                      attention_mask=batch.attention_mask.to(model.device))\n",
    "    return cache\n",
    "\n",
    "def get_mean_diff(model, harmful_instructions, harmless_instructions, positions=[-1], batch_size=32):\n",
    "    mean_harmful = get_mean_activations(model, harmful_instructions,  batch_size=batch_size, positions=positions)\n",
    "    mean_harmless = get_mean_activations(model, harmless_instructions, batch_size=batch_size, positions=positions)\n",
    "\n",
    "\n",
    "    return mean_harmful - mean_harmless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849191a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_train = get_dataset_split(\"harmful\", \"train\")\n",
    "harmless_train = get_dataset_split(\"harmless\", \"train\") \n",
    "\n",
    "mean_diffs = get_mean_diff(model, harmful_instructions=harmful_train, harmless_instructions=harmless_train, positions=positions)\n",
    "\n",
    "assert mean_diffs.shape == (len(eoi_toks), model.config.num_hidden_layers, model.config.hidden_size)\n",
    "assert not mean_diffs.isnan().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d798b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: note on how refusals are evaluated? Not exactly that important, but they use the fact that refusals have very similar/predictable first tokens and you can just test if the first token is \"I\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e80883",
   "metadata": {},
   "source": [
    "To chose the best refusal direction out of all of the layers and token positions, the authors test each of the $I * L$ refusal vectors ($I$ being the number of post-instruction tokens and $L$ being the number of layers) and choose the vector that, when ablated, induces the most non-refusal on harmful activations satisfying these conditions:\n",
    "1. Adding the vector should induce refusal on harmless prompts (the direction is sufficient to induce refusal)\n",
    "2. Ablating the vector on harmless prompts should result in similar probability distributions for the next token, measured through KL divergence (the direction preserves model behavior / coherency)\n",
    "3. The layer is less than $0.8L$, or the direction was taken from a hidden state first 80% of the model's layers. This is to prevent the direction selection process from choosing directions closer to the unembedding matrices, to choose a directon that represents some higher-level as opposed to token-level feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d216aff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: filter for best layer/position, right now we just choose a layor in the middle \n",
    "\n",
    "# last position, tenth layer \n",
    "\n",
    "direction = mean_diffs[-1, 10].to(model.device)            # (d_model,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1bf69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_direction_ablation_post_hook(direction: torch.Tensor):\n",
    "    base_dir = direction / (direction.norm() + 1e-8)\n",
    "\n",
    "    def hook_fn(module, inputs, output):\n",
    "        # output can be Tensor or a tuple (hidden_states, *extras)\n",
    "        def project_out(h: torch.Tensor) -> torch.Tensor:\n",
    "            dir_vec = base_dir.to(h.dtype).to(h.device)          # (d_model,)\n",
    "            # h: (batch, seq, d_model); (h @ dir) -> (batch, seq)\n",
    "            comp = (h @ dir_vec).unsqueeze(-1) * dir_vec         # (batch, seq, d_model)\n",
    "            return h - comp\n",
    "\n",
    "        if isinstance(output, tuple):\n",
    "            h = output[0]\n",
    "            return (project_out(h), *output[1:])\n",
    "        else:\n",
    "            return project_out(output)\n",
    "\n",
    "    return hook_fn\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def apply_post_hooks(modules, hook_fn_maker):\n",
    "    handles = []\n",
    "    try:\n",
    "        for m in modules:\n",
    "            handles.append(m.register_forward_hook(hook_fn_maker))\n",
    "        yield\n",
    "    finally:\n",
    "        for h in handles:\n",
    "            h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3125e7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_direction_ablation(\n",
    "    model, tokenizer, prompts, direction, max_new_tokens=128, **gen_kwargs\n",
    "):\n",
    "    post_hook = make_direction_ablation_post_hook(direction)\n",
    "\n",
    "    # Tokenize (use your chat template function if you have one)\n",
    "    batch = tokenizer(\n",
    "        prompts, return_tensors=\"pt\", padding=True, truncation=True, max_length=512\n",
    "    ).to(model.device)\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        with apply_post_hooks(block_modules, post_hook):\n",
    "            out_ids = model.generate(\n",
    "                **batch,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "                **gen_kwargs\n",
    "            )\n",
    "    return tokenizer.batch_decode(out_ids, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75afe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "harmful_test = [CHAT_TEMPLATE.format(instruction=s) for s in get_dataset_split(\"harmful\", \"test\")]\n",
    "\n",
    "texts = generate_with_direction_ablation(\n",
    "    model, tokenizer, harmful_test, direction,\n",
    "    max_new_tokens=128, do_sample=False  # or temperature=0.7, top_p=0.9, etc.\n",
    ")\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77286ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: activation addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6064d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: evaluation/more examples of completions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
