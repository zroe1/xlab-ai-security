{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbc3f626-b61d-4151-9f18-2e2d1ef5c107",
   "metadata": {},
   "source": [
    "# Running GPT-2 Locally"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a9a634-e17c-42fc-9217-770a9cf2df5c",
   "metadata": {},
   "source": [
    "GPT-2 was introduced in [this paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf) by <i>Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever</i> (all affiliated with OpenAI at the time).\n",
    "\n",
    "You can find more information about the version of gpt-2 that we are loading [here](https://huggingface.co/openai-community/gpt2?library=transformers). The version we will be using has 124 million paramters, which is the smallest of the GPT-2 family of models. The [largest version](https://huggingface.co/openai-community/gpt2-xl) is over ten times larger, at 1.5B total parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a439c2e9-bec8-45c7-91bb-2c60d98c9e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import xlab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ead38c0a-2759-4fc7-b213-6debb548bd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f95ab1-d765-4ab2-9d6b-2d1c47e51fd0",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90033149-a538-4fbb-a4dc-c0e5caa8e365",
   "metadata": {},
   "source": [
    "To input a sequence of text to GPT-2, we first have to decide how we would like to convert the text to numbers so we can feed it to the model. Typically, how this is done is we convert a string of text to a string of tokens, each of which will be assigned a number which can be embedded into a vector. To do this, we have a few options:\n",
    "\n",
    "1. We can assign each character it's own number\n",
    "2. We can assign each word or special character it's own number\n",
    "3. We can assign common sequences of characters their own number\n",
    "\n",
    "Typically option #3 is most popular and the high-level approach taken in the GPT-2 paper. This approach has the advantage of having a smaller total number of tokens while still capturing some of the underlying structure of natural language. Specifically, the author's use a modified version of BPE (byte pair encoding) proposed [here](https://arxiv.org/pdf/1508.07909). If you are interested, more implementation details of the tokenizer can be found in the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5779d902-eb60-4aa1-bb2d-765526759d32",
   "metadata": {},
   "source": [
    "Time to try out the GPT-2 tokenizer! Run the cell below see the tokenizer assign the string into a sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "973d690f-e005-482e-82a6-1a4bf550ec26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10374,   441,  2486,  7817,  9758,  1099,   379,   262,  2059,   286]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Barack Obama taught constitutional law at the University of\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1431769a-d588-4de0-ba79-b8607b9e24e5",
   "metadata": {},
   "source": [
    "Let's take a look at what each of these numbers represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c61835b-a29e-4e42-a637-35967d473a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10374\t --> \t\"Bar\"\n",
      "441\t --> \t\"ack\"\n",
      "2486\t --> \t\" Obama\"\n",
      "7817\t --> \t\" taught\"\n",
      "9758\t --> \t\" constitutional\"\n",
      "1099\t --> \t\" law\"\n",
      "379\t --> \t\" at\"\n",
      "262\t --> \t\" the\"\n",
      "2059\t --> \t\" University\"\n",
      "286\t --> \t\" of\"\n"
     ]
    }
   ],
   "source": [
    "for token_id in encoded_input['input_ids'][0]:\n",
    "    print(f'{token_id.item()}\\t --> \\t\"{tokenizer.decode(token_id)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9162a77e-51a4-4775-8b95-f25a00fd0113",
   "metadata": {},
   "source": [
    "We can also decode the entire sequence at once. This will be helpful to remember for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5df43ac-ce0b-4393-a60d-b9c3914f9ba1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama taught constitutional law at the University of'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21bf10f9-b906-4d38-91e5-49cd1197c19a",
   "metadata": {},
   "source": [
    "# Task #1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326191ae-9769-4cf7-b32c-dbf338d316f6",
   "metadata": {},
   "source": [
    "For a given input of text, return a list of tokens in plain text. For example for the input \"Hello there gpt-2!\", the function should return ['Hello', ' there', ' g', 'pt', '-', '2', '!']. Note that this is very different than just splitting up the text into random chunks or where there are spaces! Tokenizers are designed to create groupings of characters that are often found together or that are significant in the structure of language. You are encouraged to play around with different examples and observe how smart the tokenizer can be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6808f354-4fb2-4188-a0b5-f880a4651090",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' there', ' g', 'pt', '-', '2', '!']\n",
      "['https', '://', 'x', 'risk', '.', 'uch', 'icago', '.', 'edu', '/', 'fell', 'owship', '/']\n"
     ]
    }
   ],
   "source": [
    "# estimated time to complete: ~3 minutes\n",
    "def plain_text_tokens(prefix):\n",
    "    \"\"\"Tokenizes a text prefix into individual token strings.\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): The input text string to be tokenized.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of individual tokens as strings. Each token represents\n",
    "            how the tokenizer splits the input text.\n",
    "            \n",
    "    Example:\n",
    "        >>> plain_text_tokens(\"Hello there gpt-2!\")\n",
    "        ['Hello', ' there', ' g', 'pt', '-', '2', '!']\n",
    "    \"\"\"\n",
    "    rv = []\n",
    "    ######## YOUR CODE HERE ########\n",
    "    encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "    for i in encoded_input['input_ids'][0]:\n",
    "        rv.append(tokenizer.decode([i]))\n",
    "    return rv\n",
    "\n",
    "# test out your implementation on different inputs to get a sense of how the tokenizer works!\n",
    "print(plain_text_tokens(\"Hello there gpt-2!\"))\n",
    "print(plain_text_tokens(\"https://xrisk.uchicago.edu/fellowship/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24a42cb9-191b-4a75-a8dd-a979035c70e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests for Section 1.0, Task 1...\n",
      "\n",
      "âœ“ 1. Test case function runs without crashing                  \u001b[92mPASSED\u001b[0m\n",
      "âœ“ 2. Test case 'Hello there gpt-2'                             \u001b[92mPASSED\u001b[0m\n",
      "âœ“ 3. Test case '??!hello--*- world#$'                          \u001b[92mPASSED\u001b[0m\n",
      "âœ“ 4. Test case 'https://xrisk.uchicago.edu/fellowship/'        \u001b[92mPASSED\u001b[0m\n",
      "âœ“ 5. Test case ''                                              \u001b[92mPASSED\u001b[0m\n",
      "âœ“ 6. Test case '.,.,.,.,.,.,.,'                                \u001b[92mPASSED\u001b[0m\n",
      "\n",
      "======================================================================\n",
      "ðŸŽ‰ All tests passed! (6/6)\n",
      "======================================================================\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'total_tests': 6, 'passed': 6, 'failed': 0, 'score': 100}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlab.tests.section1_0.task1(plain_text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9bcad64f-2a61-40d4-bb8a-d909d536da7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chicago\n"
     ]
    }
   ],
   "source": [
    "text = \"Barack Obama taught constitutional law at the University of\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "generated_text = tokenizer.decode([token_id.item()]) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7d18d0-3b26-4cd6-b22a-cf951b578efa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8d77e-922d-4ddf-b4fa-01eb35642429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c90e46-6f8f-49cd-8d60-9bee5124c0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331b29ec-525c-4f21-863a-df82bda792c5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
