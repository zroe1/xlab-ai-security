{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be5fd4be",
   "metadata": {},
   "source": [
    "# Running GPT-2 Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7181e19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import xlab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8deb96b-172e-46cb-a723-62654fa4aadd",
   "metadata": {},
   "source": [
    "When loading pretrained models in the course, we will be using the [transformers](https://huggingface.co/docs/transformers/en/index) library developed by Hugging Face. This library abstracts away the complexity of running pretrained language models into a single API. In the cell below, you will be loading the [smallest version of GPT-2](https://huggingface.co/openai-community/gpt2) with 124 million parameters. If you are interested, you can also try out [GPT-2 Medium](https://huggingface.co/openai-community/gpt2-medium) (335 million parameters), [GPT-2 Large](https://huggingface.co/openai-community/gpt2-large) (774 million parameters), and [GPT-2 XL](https://huggingface.co/openai-community/gpt2-xl) (1.5 billion parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99ed73f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a049617",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afec08f7",
   "metadata": {},
   "source": [
    "To input a sequence of text to GPT-2, we first have to decide how we would like to convert the text to numbers so we can feed it to the model. Typically, how this is done is we convert a string of text to a string of tokens, each of which will be assigned a number which can be embedded into a vector. To do this, we have a few options:\n",
    "\n",
    "1. We can assign each character it's own number\n",
    "2. We can assign each word or special character it's own number\n",
    "3. We can assign common sequences of characters their own number\n",
    "\n",
    "Typically option #3 is most popular and the high-level approach taken in the GPT-2 paper. This approach has the advantage of having a smaller total number of tokens while still capturing some of the underlying structure of natural language. Specifically, the author's use a modified version of BPE (byte pair encoding) proposed [here](https://arxiv.org/pdf/1508.07909). If you are interested, more implementation details of the tokenizer can be found in the [GPT-2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf). \n",
    "\n",
    "Time to try out the GPT-2 tokenizer! Run the cell below see the tokenizer assign the string into a sequence of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df31e501",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[10374,   441,  2486,  7817,  9758,  1099,   379,   262,  2059,   286]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Barack Obama taught constitutional law at the University of\"\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "print(encoded_input['input_ids'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20c9ea0",
   "metadata": {},
   "source": [
    "Let's take a look at what each of these numbers represent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d8722f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10374\t --> \t\"Bar\"\n",
      "441\t --> \t\"ack\"\n",
      "2486\t --> \t\" Obama\"\n",
      "7817\t --> \t\" taught\"\n",
      "9758\t --> \t\" constitutional\"\n",
      "1099\t --> \t\" law\"\n",
      "379\t --> \t\" at\"\n",
      "262\t --> \t\" the\"\n",
      "2059\t --> \t\" University\"\n",
      "286\t --> \t\" of\"\n"
     ]
    }
   ],
   "source": [
    "for token_id in encoded_input['input_ids'][0]:\n",
    "    print(f'{token_id.item()}\\t --> \\t\"{tokenizer.decode(token_id)}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87999fe0-982e-4986-85d9-febacf19c317",
   "metadata": {},
   "source": [
    "We can also decode the entire sequence at once. This will be helpful to remember for later!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cf30775-76e9-4703-b14b-7662375bb3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Barack Obama taught constitutional law at the University of'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(encoded_input['input_ids'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15e5390",
   "metadata": {},
   "source": [
    "### Task #1\n",
    "\n",
    "For a given input of text, return a list of tokens in plain text. For example for the input \"Hello there gpt-2!\", the function should return ['Hello', ' there', ' g', 'pt', '-', '2', '!']. Note that this is very different than just splitting up the text into random chunks or where there are spaces! Tokenizers are designed to create groupings of characters that are often found together or that are significant in the structure of language. You are encouraged to play around with different examples and observe how smart the tokenizer can be!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32a39181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', ' there', ' g', 'pt', '-', '2', '!']\n",
      "['https', '://', 'x', 'risk', '.', 'uch', 'icago', '.', 'edu', '/', 'fell', 'owship', '/']\n"
     ]
    }
   ],
   "source": [
    "# estimated time to complete: ~3 minutes\n",
    "def plain_text_tokens(prefix):\n",
    "    \"\"\"Tokenizes a text prefix into individual token strings.\n",
    "    \n",
    "    Args:\n",
    "        prefix (str): The input text string to be tokenized.\n",
    "        \n",
    "    Returns:\n",
    "        list[str]: A list of individual tokens as strings. Each token represents\n",
    "            how the tokenizer splits the input text.\n",
    "            \n",
    "    Example:\n",
    "        >>> plain_text_tokens(\"Hello there gpt-2!\")\n",
    "        ['Hello', ' there', ' g', 'pt', '-', '2', '!']\n",
    "    \"\"\"\n",
    "    rv = []\n",
    "    ######## YOUR CODE HERE ########\n",
    "    encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "    for i in encoded_input['input_ids'][0]:\n",
    "        rv.append(tokenizer.decode(i))\n",
    "    return rv\n",
    "\n",
    "# test out your implementation on different inputs to get a sense of how the tokenizer works!\n",
    "print(plain_text_tokens(\"Hello there gpt-2!\"))\n",
    "print(plain_text_tokens(\"https://xrisk.uchicago.edu/fellowship/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65aa46d5-d0ab-4c46-8baf-8d6970b3ff1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m============================= test session starts ==============================\u001b[0m\n",
      "collected 6 items. \u001b[0m\n",
      "\n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 16%]\u001b[0mts/gpt2.py::test_function_runs_without_crashing \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 33%]\u001b[0mts/gpt2.py::test_tokenization_cases[Hello there gpt-2-expected_output0-basic text with hyphen] \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 50%]\u001b[0mts/gpt2.py::test_tokenization_cases[??!hello--*- world#$-expected_output1-special characters and symbols] \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 66%]\u001b[0mts/gpt2.py::test_tokenization_cases[https://xrisk.uchicago.edu/fellowship/-expected_output2-URL with dots and slashes] \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [ 83%]\u001b[0mts/gpt2.py::test_tokenization_cases[-expected_output3-empty string] \n",
      "\u001b[32mPASSED\u001b[0m\u001b[32m [100%]\u001b[0mts/gpt2.py::test_tokenization_cases[.,.,.,.,.,.,.,-expected_output4-repeated punctuation] \n",
      "\n",
      "\u001b[32m============================== \u001b[32m\u001b[1m6 passed\u001b[0m\u001b[32m in 0.01s\u001b[0m\u001b[32m ===============================\u001b[0m\n",
      "âœ… All checks passed!\n"
     ]
    }
   ],
   "source": [
    "xlab.tests.gpt2.task1(plain_text_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d653de",
   "metadata": {},
   "source": [
    "Back to our model. We will tokenize our text into numbers to feed into the model. When the model is done predicting text, we can untokenize the results to see if what the model is saying makes sense.\n",
    "\n",
    "Below is some code for the one full pass through the model with the prefix \"The great scientist Albert Einstein\"\n",
    "\n",
    "Take a look at the shape of the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63b9c3ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 10, 50257])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prefix = \"Barack Obama taught constitutional law at the University of\"\n",
    "encoded_input = tokenizer(prefix, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits\n",
    "logits.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b45aa34f",
   "metadata": {},
   "source": [
    "Let's take another look at the logit values. Note that the values span a huge positive and negative range. Normally, both in training and in inference, we apply a \"softmax\" function to the data to bring all values between 0 and 1. We interpret these values as the probability that the model assigns each token to be next in a sequence of text. For now however, we ignore this detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a4134f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(-26.1977, grad_fn=<UnbindBackward0>),\n",
       " tensor(-285.3222, grad_fn=<UnbindBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(logits.view(-1)), min(logits.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a7677a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10374,   441,  2486,  7817,  9758,  1099,   379,   262,  2059,   286]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_input['input_ids']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9797f05",
   "metadata": {},
   "source": [
    "What is going on here?\n",
    "\n",
    "`torch.Size([1, 10, 50257])` tells us that logits is a 3 dimensional array (i.e., it is `1*4*50257`). The first dimension represents the batch size and because there is only one batch, we can ignore it for now. The next dimension is the sequence length. Note that:\n",
    "\n",
    "```python\n",
    ">>> encoded_input['input_ids']\n",
    "tensor([[10374,   441,  2486,  7817,  9758,  1099,   379,   262,  2059,   286]])\n",
    "```\n",
    "\n",
    "There are 10 tokens when we tokenize \"Barack Obama taught constitutional law at the University of\" meaning the sequence length is 10. For the final dimension, we have 50257 which represents the model's vocabulary size. Why do we have so much data? Don't we only want the next predicted piece of text?\n",
    "\n",
    "To understand why this is necessary, you will need to understand what information is included in this tensor. In total, we have 10 vectors of length 50257. Each vector represents a probability distribution for each token in the vocabulary. For example, if the value at 42 is higher, that means that the model is assigning a higher probability to the token at position 42 to be the next in the sequence of text.\n",
    "\n",
    "This makes sense for our purposes: if we have a probability distribution for the next token in the text, we can sample from it to predict the next token! But why do we have 10 probability distributions in the output. In other words why do we need a probability distribution for each token in the input?\n",
    "\n",
    "The answer to this question is oddly, that this make it easier to train our model! If we have a piece of text that we are training on from the internet, we can train multiple examples in paralell. For example, if we have the text \"Barack Obama taught constitutional law at the University of\" here are a several different examples we could choose to train on. \n",
    "\n",
    "\n",
    "1. Prefix=\"Bar\" and label=\"ack\"\n",
    "2. Prefix=\"Barack\" and label=\" Obama\"\n",
    "3. Prefix=\"Barack Obama\" and label=\" taught\"\n",
    "4. And so on...\n",
    "\n",
    "\n",
    "The first three vectors in the logits in the code above correspond to the model's predictions for the first three prefixes above. While running inference, we only care about the model's label for the input \"Barack Obama taught constitutional law at the University of\" so therefore, we only need to extract the final vector from the probability distribution. This makes sense because for our purposes, we aren't interested in efficiently training the model. We are only interested in seeing what the model predicts for the next token.\n",
    "\n",
    "We can extract this last vector by taking `logits[0][-1]`. Lets see what the model predicts!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f744c0cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Chicago\n"
     ]
    }
   ],
   "source": [
    "text = \"Barack Obama taught constitutional law at the University of\"\n",
    "\n",
    "encoded_input = tokenizer(text, return_tensors='pt')\n",
    "output = model(**encoded_input)\n",
    "logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "generated_text = tokenizer.decode([token_id.item()]) \n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eafea5-7d9e-4159-af5e-daa91f6ef9e6",
   "metadata": {},
   "source": [
    "Indeed, Barack Obama taught constitutional law at the University of Chicago ([source](https://www.obamalibrary.gov/obamas/president-barack-obama))! Despite being an early model with limited capabilities, GPT-2 124M knows quite a bit about the world."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d24634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    encoded_input = tokenizer(text, return_tensors='pt')\n",
    "    output = model(**encoded_input)\n",
    "    logits = output.logits  # Shape: (batch_size, sequence_length, vocab_size)\n",
    "    token_id = torch.argmax(logits[0][-1])\n",
    "\n",
    "    generated_text = tokenizer.decode([token_id.item()])  \n",
    "    text+=generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fe611198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Barack Obama taught constitutional law at the University of Chicago. He was a founding member of the American Bar Association, and he was a founding member of the American Bar Association's Board of Trustees. He was a founding member of the American Bar Association's Board of Trustees. He was a founding member of the American Bar Association's Board of Trustees. He was a founding member of the American Bar Association's Board of Trustees. He was a founding member of the American Bar Association's Board of Trustees. He was a founding member of\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9efd42",
   "metadata": {},
   "source": [
    "### Why is the model repeating itself?\n",
    "\n",
    "In the previous cell, we generated text by always choosing the **most likely next token** (using `torch.argmax`). This deterministic approach has a major drawback: once the model enters a pattern that has high probability, it can get stuck in a loop.\n",
    "\n",
    "Think of it like this: if the model strongly believes that \"Einstein was\" should be followed by \"a physicist\", and then strongly believes \"a physicist\" should be followed by \"who\", and then that sequence loops back to another high-probability path, the model will repeat this pattern indefinitely.\n",
    "\n",
    "### Introducing Temperature\n",
    "\n",
    "**Temperature** is a hyperparameter that controls the randomness of predictions by scaling the logits before applying softmax. Mathematically:\n",
    "\n",
    "$$p_i = \\frac{\\exp(z_i/T)}{\\sum_j \\exp(z_j/T)}$$\n",
    "\n",
    "Where:\n",
    "- $z_i$ are the logits (raw model outputs)\n",
    "- $T$ is the temperature parameter\n",
    "- $p_i$ is the resulting probability for token $i$\n",
    "\n",
    "### Effects of different temperature values:\n",
    "\n",
    "- **T = 0** (or very close to 0): Completely deterministic, always pick highest probability token (like we did before)\n",
    "- **T = 1.0**: Standard softmax, use the exact probabilities from the model\n",
    "- **T > 1.0**: More uniform distribution, increasing randomness and diversity\n",
    "- **T < 1.0**: More peaked distribution, reducing randomness but still allowing some\n",
    "\n",
    "Lower temperatures produce more focused, coherent text but risk repetition. Higher temperatures produce more diverse, creative text but risk incoherence.\n",
    "\n",
    "In the next cell, we'll implement temperature sampling to fix our repetition problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4c76a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_temperature(model, tokenizer, prompt, max_length=100, temperature=0.7):\n",
    "    # Start with the provided prompt\n",
    "    generated_text = prompt\n",
    "    \n",
    "    for _ in range(max_length):\n",
    "        # Tokenize the current text\n",
    "        inputs = tokenizer(generated_text, return_tensors='pt')\n",
    "        \n",
    "        # Get model output\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "        \n",
    "        # Get the next token logits (last position in sequence)\n",
    "        next_token_logits = logits[0, -1, :]\n",
    "        \n",
    "        # Apply temperature\n",
    "        scaled_logits = next_token_logits / temperature\n",
    "        \n",
    "        # Convert to probabilities with softmax\n",
    "        probs = F.softmax(scaled_logits, dim=0)\n",
    "        \n",
    "        # Sample from the distribution\n",
    "        next_token_id = torch.multinomial(probs, num_samples=1).item()\n",
    "        \n",
    "        # Decode the token and add to generated text\n",
    "        next_token_text = tokenizer.decode([next_token_id])\n",
    "        generated_text += next_token_text\n",
    "        \n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4572f486",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature = 0.3:\n",
      "The great scientist Albert Einstein, who was the first to see the potential of quantum mechanics, was the first to see the potential of quantum mechanics, and he was the first to see the potential of quantum mechanics. He was the first to see the potential of quantum mechanics. He\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Temperature = 0.7:\n",
      "The great scientist Albert Einstein (1850-1917) has been considered one of the greatest minds in the history of science. Einstein was known as one of the most important minds of the twentieth century. His work was \"the scribe of history\" and to use his\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "Temperature = 1.2:\n",
      "The great scientist Albert Einstein stated that we must underrealize our geospaces at the frequency of weaponization. His judgement was especially important specifically in the most distant era Jean Valjean, a Parisian celebrated mathematician and any competent observer of solar systems, said now about\n"
     ]
    }
   ],
   "source": [
    "# Low temperature (more deterministic but not completely)\n",
    "prompt = \"The great scientist Albert Einstein\"\n",
    "low_temp_text = generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=50, temperature=0.3\n",
    ")\n",
    "print(\"Temperature = 0.3:\")\n",
    "print(low_temp_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# Medium temperature (balanced)\n",
    "medium_temp_text = generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=50, temperature=0.7\n",
    ")\n",
    "print(\"Temperature = 0.7:\")\n",
    "print(medium_temp_text)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "# High temperature (more random)\n",
    "high_temp_text = generate_with_temperature(\n",
    "    model, tokenizer, prompt, max_length=50, temperature=1.2\n",
    ")\n",
    "print(\"Temperature = 1.2:\")\n",
    "print(high_temp_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e8d462",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
